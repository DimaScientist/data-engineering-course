{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Dataframes (Scala)\n",
    "**Сергей Гришаев**  \n",
    "serg.grishaev@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Сравнение RDD API и DataFrame API \n",
    "+ Базовые функции\n",
    "+ Очистка данных\n",
    "+ Агрегаты\n",
    "+ Кеширование \n",
    "+ Репартиционирование\n",
    "+ Встроенные функции\n",
    "+ Пользовательские функции\n",
    "+ Соединения\n",
    "+ Оконные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение RDD API и DataFrame API \n",
    "\n",
    "### Типы данных\n",
    "**RDD**: низкоуревная распределенная коллекция данных любого типа  \n",
    "**DF**: таблица со схемой, состоящей из колонок разных типов, описанных в `org.apache.spark.sql.types`  \n",
    "\n",
    "### Обработка данных\n",
    "**RDD**: сериализуемые функции  \n",
    "**DF**: кодогенерация SQL > Java код  \n",
    "\n",
    "### Функции и алгоритмы\n",
    "**RDD**: нет ограничений  \n",
    "**DF**: ограничен SQL операторами, функциями `org.apache.spark.sql.functions` и пользовательскими функциями  \n",
    "\n",
    "### Источники данных\n",
    "**RDD**: каждый источник имеет свое API  \n",
    "**DF**: единое API для всех источников \n",
    "\n",
    "### Производительность\n",
    "**RDD**: напрямую зависит от качества кода\n",
    "**DF**: встроенные механизмы оптимизации SQL запроса\n",
    "\n",
    "\n",
    "### Потоковая обработка данных\n",
    "**RDD**: устаревший DStreams  \n",
    "**DF**: активно развивающийся Structured Streaming\n",
    "\n",
    "\n",
    "### Выводы:\n",
    "+ На текущий момент RDD является низкоуровневым API, которое постепенно уходит \"под капот\" Apache Spark\n",
    "+ DF API представляет собой библиотеку для обработки данных с использованием SQL примитивов\n",
    "\n",
    "## Базовые функции\n",
    "\n",
    "Создать dataframe можно на основе:\n",
    "+ локальной коллекции\n",
    "+ файлов\n",
    "+ базы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cityList = Vector(Moscow, Paris, Madrid, London, New York)\n",
       "df = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "val cityList: Vector[String] = Vector(\"Moscow\", \"Paris\", \"Madrid\", \"London\", \"New York\")\n",
    "\n",
    "// метод toDF изначально отсутствует у Vector[T], он добавляется через import spark.implicits._\n",
    "val df: DataFrame = cityList.toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У любого DF есть схема:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть содержимое DF можно с помощью метода `show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|  Moscow|\n",
      "|   Paris|\n",
      "|  Madrid|\n",
      "|  London|\n",
      "|New York|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно вывести содержимое в вертикальной ориентации - это удобно при большое количестве столбцов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " value | Moscow   \n",
      "-RECORD 1---------\n",
      " value | Paris    \n",
      "-RECORD 2---------\n",
      " value | Madrid   \n",
      "-RECORD 3---------\n",
      " value | London   \n",
      "-RECORD 4---------\n",
      " value | New York \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(numRows = 20, truncate = 500, vertical=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет количества элементов в DF с помощью `count()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отфильтровать данные можно с помощью метода `filter`. В отличие от RDD, он принимает SQL выражение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = value\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "value"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: org.apache.spark.sql.Column = 'value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = value\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "value"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: org.apache.spark.sql.Column = $\"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = value\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "value"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: org.apache.spark.sql.Column = col(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$eq$eq$eq: (x: Int)Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ===(x: Int) = { \n",
    "    x + 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "===(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "condition = (value = Moscow)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(value = Moscow)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val condition: org.apache.spark.sql.Column = 'value === \"Moscow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Требует наличия import spark.implicits._\n",
    "\n",
    "df.filter( 'value.===(\"Moscow\") ).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|  Moscow|\n",
      "|   Paris|\n",
      "|  Madrid|\n",
      "|  London|\n",
      "|New York|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter( condition ).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Требует наличия import spark.implicits._\n",
    "\n",
    "df.filter($\"value\" === \"Moscow\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "condition = (value = value)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(value = value)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val condition = col(\"value\") === 'value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// sugar free & type safe\n",
    "// Три знака равно здесь используются, тк на самом деле это метод,\n",
    "// применяемый к колонке org.apache.spark.sql.Column\n",
    "\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "df.filter(col(\"value\") === \"Moscow\" ).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|Moscow|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// легко ошибиться и получить ошибку в рантайме\n",
    "\n",
    "df.filter(\"value = 'Moscow'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// промежуточный вариант между col и обычной строкой\n",
    "// expr также может использоваться для вызова SQL builtin функций, \n",
    "// отсутствующих в org.apache.spark.sql.functions\n",
    "\n",
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "df.filter(expr(\"value = 'Moscow'\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавить новую колонку можно с помощью метода `withColumn`. Необходимо помнить, что данный метод, как и другие, является трансформацией и не изменяет оригинальный DF, а создает новый."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|   value|upperCity|\n",
      "+--------+---------+\n",
      "|  Moscow|   MOSCOW|\n",
      "|   Paris|    PARIS|\n",
      "|  Madrid|   MADRID|\n",
      "|  London|   LONDON|\n",
      "|New York| NEW YORK|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.upper\n",
    "df.withColumn(\"upperCity\", upper('value)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичный результат получить, используя метод `select`. Данный метод может быть использован не только для выбора определенных колонок, но и для создания новых."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|   value|upperCity|\n",
      "+--------+---------+\n",
      "|  Moscow|   MOSCOW|\n",
      "|   Paris|    PARIS|\n",
      "|  Madrid|   MADRID|\n",
      "|  London|   LONDON|\n",
      "|New York| NEW YORK|\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "withUpper = [value: string, upperCity: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string, upperCity: string]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withUpper = df.select('value, upper('value).alias(\"upperCity\"))\n",
    "withUpper.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если передать `col(\"*\")` в `select`, то вы получите DF со всеми колонками. Это полезно, когда вы не знаете список всех колонок (например вы получили его через API), но вам нужно их все выбрать и добавить новую колонку. Это можно сделать следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------+---+\n",
      "|   value|upperCity|lowerCity|length|bar|\n",
      "+--------+---------+---------+------+---+\n",
      "|  Moscow|   MOSCOW|   moscow|     7|foo|\n",
      "|   Paris|    PARIS|    paris|     6|foo|\n",
      "|  Madrid|   MADRID|   madrid|     7|foo|\n",
      "|  London|   LONDON|   london|     7|foo|\n",
      "|New York| NEW YORK| new york|     9|foo|\n",
      "+--------+---------+---------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// методы name, as и alias часто являются взаимозаменяемыми\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "withUpper.select(\n",
    "    col(\"*\"), \n",
    "    lower($\"value\").name(\"lowerCity\"), \n",
    "    (length('value) + 1).as(\"length\"),\n",
    "    lit(\"foo\").alias(\"bar\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При необходимости в `select` можно передать список колонок, используя обычные строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|   value|upperCity|\n",
      "+--------+---------+\n",
      "|  Moscow|   MOSCOW|\n",
      "|   Paris|    PARIS|\n",
      "|  Madrid|   MADRID|\n",
      "|  London|   LONDON|\n",
      "|New York| NEW YORK|\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "withUpper.select(\"value\", \"upperCity\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалить колонку из DF можно с помощью метода `drop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   value|\n",
      "+--------+\n",
      "|  Moscow|\n",
      "|   Paris|\n",
      "|  Madrid|\n",
      "|  London|\n",
      "|New York|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// drop не будет выдавать ошибку, если будет указана несуществующая колонка\n",
    "\n",
    "withUpper.drop(\"upperCity\", \"abraKadabra\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ методы `filter` и `select` принимают в качестве аргументов колонки [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column). Это может быть либо ссылка на существующую колонку, либо функцию из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "+ любые трансформации возвращают новый DF, не меняя существующий\n",
    "+ тип [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column) играет важную роль в DF API - на его основе создаются ссылки на существующие колонки, а также функции, принимающие [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column) и возвращающие [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column). По этой причине обычное сравнение `==` не будет работать в DF API, тк `filter` принимает [org.apache.spark.sql.Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column), а не `Boolean`\n",
    "+ Класс DataFrame в последних версиях Spark представляет собой `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]`, поэтому его описание следует искать в [org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)\n",
    "\n",
    "## Очистка данных\n",
    "\n",
    "Одной из задач обработки данных является их очистка. DF API содержит класс функций \"not available\", описанный в пакете [org.apache.spark.sql.DataFrameNaFunctions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameNaFunctions). В данном пакете есть три функции:\n",
    "+ `na.drop`\n",
    "+ `na.fill`\n",
    "+ `na.replace`\n",
    "\n",
    "Для демонстрации работы данных функций создадим новый датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{ \"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664}\n",
      "{ \"name\":\"Madrid\", \"country\":\"Spain\" }\n",
      "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}\n",
      "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}\n",
      "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }\n",
      "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
      "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
      "{ \"name\":\"New York, \"country\":\"USA\",|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "-RECORD 0-------------------------------------------------\n",
      " col | { \"name\":\"Moscow\", \"country\":\"Rossiya\", \"contin... \n",
      "-RECORD 1-------------------------------------------------\n",
      " col | { \"name\":\"Madrid\", \"country\":\"Spain\" }             \n",
      "-RECORD 2-------------------------------------------------\n",
      " col | { \"name\":\"Paris\", \"country\":\"France\", \"continen... \n",
      "-RECORD 3-------------------------------------------------\n",
      " col | { \"name\":\"Berlin\", \"country\":\"Germany\", \"contin... \n",
      "-RECORD 4-------------------------------------------------\n",
      " col | { \"name\":\"Barselona\", \"country\":\"Spain\", \"conti... \n",
      "-RECORD 5-------------------------------------------------\n",
      " col | { \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent... \n",
      "-RECORD 6-------------------------------------------------\n",
      " col | { \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent... \n",
      "-RECORD 7-------------------------------------------------\n",
      " col | { \"name\":\"New York, \"country\":\"USA\",               \n",
      "\n",
      "root\n",
      " |-- value: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      "\n",
      "+--------------------+---------+-------+---------+----------+\n",
      "|     _corrupt_record|continent|country|     name|population|\n",
      "+--------------------+---------+-------+---------+----------+\n",
      "|                null|   Europe|Rossiya|   Moscow|  12380664|\n",
      "|                null|     null|  Spain|   Madrid|      null|\n",
      "|                null|   Europe| France|    Paris|   2196936|\n",
      "|                null|   Europe|Germany|   Berlin|   3490105|\n",
      "|                null|   Europe|  Spain|Barselona|      null|\n",
      "|                null|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|                null|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|{ \"name\":\"New Yor...|     null|   null|     null|      null|\n",
      "+--------------------+---------+-------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testData = \n",
       "raw = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{ \"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664}\n",
       "{ \"name\":\"Madrid\", \"country\":\"Spain\" }\n",
       "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}\n",
       "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}\n",
       "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }\n",
       "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
       "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
       "{ \"name\":\"New York, \"country\":\"USA\",\n",
       "jsonStrings:...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "val testData =\n",
    "\"\"\"{ \"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664}\n",
    "{ \"name\":\"Madrid\", \"country\":\"Spain\" }\n",
    "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936}\n",
    "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105}\n",
    "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 }\n",
    "{ \"name\":\"New York, \"country\":\"USA\",\"\"\"\n",
    "\n",
    "// Создаем DF из одной строки и добавляем данные в виде новой колонки\n",
    "val raw = spark.range(0,1).select(lit(testData).alias(\"value\"))\n",
    "raw.show(1, false)\n",
    "\n",
    "// Создаем новую колонку, разибая наши данные по \\n\n",
    "val jsonStrings: Column = split(col(\"value\"), \"\\n\").alias(\"value\")\n",
    "\n",
    "raw.select(explode(jsonStrings)).show(10, 50, true)\n",
    "raw.select(jsonStrings).printSchema\n",
    "\n",
    "// Используем функцию explode для того, чтобы развернуть массив мехом наружу и используем темную магию \n",
    "// для превращения DataFrame в Dataset[String]\n",
    "val splited: Dataset[String] = raw.select(explode(jsonStrings)).as[String]\n",
    "\n",
    "// splited.show(numRows = 10, truncate = false)\n",
    "\n",
    "\n",
    "// // Создаем новый датафре... датасет, в котором наши JSON строки будут распарсены\n",
    "val df: Dataset[Row] = spark.read.json(splited)\n",
    "df.printSchema\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для очистки датасета:\n",
    "+ удалим строку с навалидным JSON, сохраним ее в отдельное место\n",
    "+ удалим дубликаты\n",
    "+ заполним `null`ы в колонках\n",
    "+ исправим `Rossiya` на `Russia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corruptData = Array([{ \"name\":\"New York, \"country\":\"USA\",])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array([{ \"name\":\"New York, \"country\":\"USA\",])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corruptData = df.select(col(\"_corrupt_record\")).na.drop(\"all\").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|Undefined|  Spain|   Madrid|         0|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Europe|  Spain|Barselona|         0|\n",
      "|   Europe| Russia|   Moscow|  12380664|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fillData = Map(continent -> Undefined, population -> 0)\n",
       "replaceData = Map(Rossiya -> Russia)\n",
       "cleanData = [continent: string, country: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, country: string ... 2 more fields]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fillData: Map[String, Any] = Map(\"continent\" -> \"Undefined\", \"population\" -> 0)\n",
    "val replaceData: Map[Any, Any] = Map(\"Rossiya\" -> \"Russia\", 100000 -> 0)\n",
    "\n",
    "val cleanData = \n",
    "    df\n",
    "    .drop(col(\"_corrupt_record\"))\n",
    "    .na.drop(\"all\")\n",
    "    .na.fill(fillData)\n",
    "    .na.replace(\"country\", replaceData)\n",
    "    .dropDuplicates\n",
    "\n",
    "\n",
    "cleanData.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API обладает удобным API для очистки данных, позволяющим разработчику сконцентрироваться разработчику на бизнес логике, а не на написании функций для обработки всех возможных исключительных ситуаций\n",
    "+ метод `spark.read.json` позволяет читать не только файлы, но и `Dataset[String]`, содержащие JSON строки.\n",
    "\n",
    "## Агрегаты\n",
    "Посчитаем суммарное население и количество городов с разбивкой по континентам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "// from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|continent|count|\n",
      "+---------+-----+\n",
      "|   Europe|    4|\n",
      "|   Africa|    1|\n",
      "|Undefined|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggCount = [continent: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, count: bigint]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggCount = cleanData.groupBy('continent).count\n",
    "aggCount.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|continent|sum(population)|\n",
      "+---------+---------------+\n",
      "|   Europe|       18067705|\n",
      "|   Africa|       11922948|\n",
      "|Undefined|              0|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggSum = [continent: string, sum(population): bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, sum(population): bigint]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggSum = cleanData.groupBy('continent).sum(\"population\")\n",
    "aggSum.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы совместить несколько агрегатов в одном DF, мы можем использовать метод `agg()`. Данный метод позволяет использовать любые `Aggregate functions` из пакета [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------+\n",
      "|continent|count|  sumPop|\n",
      "+---------+-----+--------+\n",
      "|   Europe|    4|18067705|\n",
      "|   Africa|    1|11922948|\n",
      "|Undefined|    1|       0|\n",
      "+---------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "agg = [continent: string, count: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, count: bigint ... 1 more field]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val agg = cleanData.groupBy('continent).agg(count(\"*\").alias(\"count\"), sum(\"population\").alias(\"sumPop\"))\n",
    "agg.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью агрегатов мы можем выполнять такие действия, как, например, `collect_list` и `collect_set`. Стоит отметить, что колонки в Spark могут иметь не только скалярные типы, но и структуры, словари и массивы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- countries: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "-RECORD 0-------------------------------------\n",
      " continent | Europe                           \n",
      " countries | [France, Germany, Spain, Russia] \n",
      "-RECORD 1-------------------------------------\n",
      " continent | Africa                           \n",
      " countries | [Egypt]                          \n",
      "-RECORD 2-------------------------------------\n",
      " continent | Undefined                        \n",
      " countries | [Spain]                          \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[continent: string, countries: array<string>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "aggList = [continent: string, countries: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val aggList = cleanData.groupBy('continent).agg(collect_list(\"country\").alias(\"countries\"))\n",
    "aggList.printSchema\n",
    "aggList.show(numRows = 10, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя методы `struct` и `to_json`, мы можем превратить произвольный набор колонок в JSON строку. Этот методы часто используется перед отправкой данных в Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- s: struct (nullable = false)\n",
      " |    |-- continent: string (nullable = false)\n",
      " |    |-- countries: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+------------------------------------------+\n",
      "|s                                         |\n",
      "+------------------------------------------+\n",
      "|[Europe, [France, Germany, Spain, Russia]]|\n",
      "|[Africa, [Egypt]]                         |\n",
      "|[Undefined, [Spain]]                      |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "withStruct = [s: struct<continent: string, countries: array<string>>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[s: struct<continent: string, countries: array<string>>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withStruct = aggList.select(struct('continent, 'countries).alias(\"s\"))\n",
    "withStruct.printSchema\n",
    "\n",
    "withStruct.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+\n",
      "|s                                                                       |\n",
      "+------------------------------------------------------------------------+\n",
      "|{\"continent\":\"Europe\",\"countries\":[\"France\",\"Germany\",\"Spain\",\"Russia\"]}|\n",
      "|{\"continent\":\"Africa\",\"countries\":[\"Egypt\"]}                            |\n",
      "|{\"continent\":\"Undefined\",\"countries\":[\"Spain\"]}                         |\n",
      "+------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withStruct.withColumn(\"s\", to_json('s)).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо превратить все колонки DF в JSON String, можно воспользоваться функций `toJSON`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+\n",
      "|value                                                                   |\n",
      "+------------------------------------------------------------------------+\n",
      "|{\"continent\":\"Europe\",\"countries\":[\"France\",\"Germany\",\"Spain\",\"Russia\"]}|\n",
      "|{\"continent\":\"Africa\",\"countries\":[\"Egypt\"]}                            |\n",
      "|{\"continent\":\"Undefined\",\"countries\":[\"Spain\"]}                         |\n",
      "+------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jString = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jString: Dataset[String] = aggList.toJSON\n",
    "jString.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нам необходимо создать колонки из значений текущих колонок, мы можем воспользоваться функцией `pivot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+---------+\n",
      "|country|  Africa|  Europe|Undefined|\n",
      "+-------+--------+--------+---------+\n",
      "| Russia|    null|12380664|     null|\n",
      "|Germany|    null| 3490105|     null|\n",
      "| France|    null| 2196936|     null|\n",
      "|  Spain|    null|       0|        0|\n",
      "|  Egypt|11922948|    null|     null|\n",
      "+-------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanData.groupBy(col(\"country\")).pivot(\"continent\").agg(sum(\"population\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ DF API позволяет строить большое количество агрегатов. При этом необходимо помнить, что операции `groupBy`, `cube`, `rollup` возвращают [org.apache.spark.sql.RelationalGroupedDataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset), к которому затем необходимо применить одну из функций агрегации - `count`, `sum`, `agg` и т. п.\n",
    "+ При вычислении агрегатов необходимо помнить, что эта операция требует перемешивания данных между воркерами, что, в случае перекошенных данных, может привести к OOM на воркере.\n",
    "\n",
    "## Кеширование\n",
    "По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения. Для демонстрации возьмем датасет [Airport Codes](https://datahub.io/core/airport-codes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | 40.07080078125, -74.93360137939453 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем несколько агрегатов. Несмотря на то, что `onlyRu` является общим для всех действий, он пересчитывается при вызове каждого действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " ident        | RU-0006               \n",
      " type         | closed                \n",
      " name         | Arabatuk Air Base     \n",
      " elevation_ft | 2280                  \n",
      " continent    | EU                    \n",
      " iso_country  | RU                    \n",
      " iso_region   | RU-CHI                \n",
      " municipality | Daurija               \n",
      " gps_code     | null                  \n",
      " iata_code    | null                  \n",
      " local_code   | ZA2N                  \n",
      " coordinates  | 50.223801, 117.098999 \n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+-----+\n",
      "|     municipality|count|\n",
      "+-----------------+-----+\n",
      "|            Chita|    3|\n",
      "|     Nizhneudinsk|    2|\n",
      "|         Ulan Ude|    2|\n",
      "|    Nizhneangarsk|    2|\n",
      "|          Irkutsk|    2|\n",
      "|           Borzya|    2|\n",
      "|  Severo-Eniseysk|    1|\n",
      "|          Chistyy|    1|\n",
      "|     Karachayevsk|    1|\n",
      "|         Barguzin|    1|\n",
      "|Usolye-Sibirskoye|    1|\n",
      "|           Amazar|    1|\n",
      "|            Baley|    1|\n",
      "|     Snezhnogorsk|    1|\n",
      "|       Akkem Lake|    1|\n",
      "|            Kyren|    1|\n",
      "|       Ust-Ilimsk|    1|\n",
      "|      Olovyannaya|    1|\n",
      "|    Kazachinskaya|    1|\n",
      "|     Novokuznetsk|    1|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "onlyRuAndHigh = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val onlyRuAndHigh = airports.filter('iso_country === \"RU\" and 'elevation_ft > 1000)\n",
    "onlyRuAndHigh.show(numRows = 1, truncate = 100, vertical = true)\n",
    "\n",
    "onlyRuAndHigh.count\n",
    "onlyRuAndHigh.collect\n",
    "onlyRuAndHigh.groupBy('municipality).count.orderBy('count.desc).na.drop(\"any\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой проблемы следует использовать методы `cache`, либо `persist`. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что `persist` позволяет выбрать, куда сохранить данные, а `cache` использует значение по умолчанию. В текущей версии Spark это [StorageLevel.MEMORY_ONLY](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence). Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить `unpersist` для очистки памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " ident        | RU-0006               \n",
      " type         | closed                \n",
      " name         | Arabatuk Air Base     \n",
      " elevation_ft | 2280                  \n",
      " continent    | EU                    \n",
      " iso_country  | RU                    \n",
      " iso_region   | RU-CHI                \n",
      " municipality | Daurija               \n",
      " gps_code     | null                  \n",
      " iata_code    | null                  \n",
      " local_code   | ZA2N                  \n",
      " coordinates  | 50.223801, 117.098999 \n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+-----+\n",
      "|     municipality|count|\n",
      "+-----------------+-----+\n",
      "|            Chita|    3|\n",
      "|     Nizhneudinsk|    2|\n",
      "|         Ulan Ude|    2|\n",
      "|    Nizhneangarsk|    2|\n",
      "|          Irkutsk|    2|\n",
      "|           Borzya|    2|\n",
      "|  Severo-Eniseysk|    1|\n",
      "|          Chistyy|    1|\n",
      "|     Karachayevsk|    1|\n",
      "|         Barguzin|    1|\n",
      "|Usolye-Sibirskoye|    1|\n",
      "|           Amazar|    1|\n",
      "|            Baley|    1|\n",
      "|     Snezhnogorsk|    1|\n",
      "|       Akkem Lake|    1|\n",
      "|            Kyren|    1|\n",
      "|       Ust-Ilimsk|    1|\n",
      "|      Olovyannaya|    1|\n",
      "|    Kazachinskaya|    1|\n",
      "|     Novokuznetsk|    1|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyRuAndHigh.cache\n",
    "onlyRuAndHigh.count\n",
    "// при вычислении count данные будут помещены в cache\n",
    "onlyRuAndHigh.show(numRows = 1, truncate = 100, vertical = true)\n",
    "onlyRuAndHigh.collect\n",
    "onlyRuAndHigh.groupBy('municipality).count.orderBy('count.desc).na.drop(\"any\").show\n",
    "\n",
    "onlyRuAndHigh.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Использование `cache` и `persist` позволяет существенно сократить время обработки данных, однако следует помнить и об увеличении потребляемой памяти на воркерах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Репартиционирование\n",
    "RDD и DF являются представляют собой классы, описывающие распределенные коллекции данных. Они (коллекции) разбиты на крупные блоки, которые называются партициями. В графе вычисления, который называется в Spark DAG (Direct Acyclic Graph), есть три основных компонента - `job`, `stage`, `task`.\n",
    "\n",
    "`job` представляет собой весь граф целиком, от момента создания DF, до применения `action` к нему. Состоит из одной или более `stage`. Когда возникает необходимость сделать `shuffle` данных, Spark создает новый `stage`. Каждый `stage` состоит из большого количества `task`. `task` это базовая операция над данными. Одновременно Spark выполняет N `task`, которые обрабатывают N партиций, где N - это суммарное число доступных потоков на всех воркерах.\n",
    "\n",
    "Исходя из этого, важно обеспечивать:\n",
    "+ достаточное количество партиций для распределения нагрузки по всем воркерам\n",
    "+ равномерное распределение данных между партициями\n",
    "\n",
    "Создадим датасет с перекосом данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|itemPerPartition|\n",
      "+----------------+\n",
      "|0               |\n",
      "|900             |\n",
      "|0               |\n",
      "|100             |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "|0               |\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "skewColumn = CASE WHEN (id < 900) THEN 0 ELSE 1 END\n",
       "skewDf = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "printItemPerPartition: [T](ds: org.apache.spark.sql.Dataset[T])Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val skewColumn = when(col(\"id\") < 900, lit(0)).otherwise(lit(1))\n",
    "\n",
    "val skewDf = spark.range(0,1000).repartition(10, skewColumn)\n",
    "\n",
    "def printItemPerPartition[T](ds: Dataset[T]): Unit = {\n",
    "    ds.mapPartitions { x => Iterator(x.length) }\n",
    "    .withColumnRenamed(\"value\", \"itemPerPartition\")\n",
    "    .show(50, false)\n",
    "}\n",
    "\n",
    "printItemPerPartition[java.lang.Long](skewDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Любые операции с таким датасетом будут работать медленно, т.к.\n",
    "+ если суммарное количество потоков на всех воркерах больше 10, то в один момент времени работать будут максимум 10, остальные будут простаивать\n",
    "+ из 10 партицийи только в 2 есть данные и это означает, что только 2 потока будут обрабатывать данные, при этом из-за перекоса данных между ними (900 vs 100) первый станет bottleneck'ом\n",
    "\n",
    "Обычно перекошенные датасеты возникают после вычисления агрегатов, оконных функций и соединений, но также могут возникать и при чтении источников.\n",
    "\n",
    "Для устранения проблемы перекоса данных, следует использовать метод `repartition`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|itemPerPartition|\n",
      "+----------------+\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "|50              |\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "repartitionedDf = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// здесь мы передаем только новое количество партиций и Spark выполнит RoundRobinPartitioning\n",
    "val repartitionedDf = skewDf.repartition(20)\n",
    "\n",
    "printItemPerPartition[java.lang.Long](repartitionedDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|itemPerPartition|\n",
      "+----------------+\n",
      "|37              |\n",
      "|61              |\n",
      "|48              |\n",
      "|59              |\n",
      "|47              |\n",
      "|54              |\n",
      "|45              |\n",
      "|58              |\n",
      "|55              |\n",
      "|55              |\n",
      "|56              |\n",
      "|46              |\n",
      "|45              |\n",
      "|46              |\n",
      "|49              |\n",
      "|64              |\n",
      "|44              |\n",
      "|39              |\n",
      "|40              |\n",
      "|52              |\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "repartitionedDf = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// здесь мы добавляем к числу партиций колонку, по которой необходимо сделать репартиционирование,\n",
    "// поэтому Spark выполнит HashPartitioning\n",
    "val repartitionedDf = skewDf.repartition(20, col(\"id\"))\n",
    "\n",
    "printItemPerPartition[java.lang.Long](repartitionedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://pngimage.net/wp-content/uploads/2018/06/соленья-png-4.png\">\n",
    "\n",
    "### Соленья\n",
    "Часто при вычислении агрегатов приходится работать с перекошенными данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[type: string, count: bigint]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.printSchema\n",
    "\n",
    "airports.groupBy('type).count.orderBy('count.desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|          type|count|\n",
      "+--------------+-----+\n",
      "| small_airport|33998|\n",
      "|      heliport|11316|\n",
      "|medium_airport| 4531|\n",
      "|        closed| 3618|\n",
      "| seaplane_base| 1014|\n",
      "| large_airport|  613|\n",
      "|   balloonport|   23|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.groupBy('type).count.orderBy('count.desc).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку при вычислении агрегата происходит неявный `HashPartitioning` по ключу (ключам) агрегата, то при выполнении определенных условий происходит нехватка памяти на воркере, которую нельзя исправить, не изменив подход к построению агрегата.\n",
    "\n",
    "Один из вариантов устранение - соление ключей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | 40.07080078125, -74.93360137939453 \n",
      " salt         | 3                                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "saltModTen = CAST(pmod(round((rand(6531147292355459874) * 100), 0), 10) AS INT)\n",
       "salted = [ident: string, type: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 11 more fields]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val saltModTen = pmod(round((rand() * 100), 0), lit(10)).cast(\"int\")\n",
    "\n",
    "val salted = airports.withColumn(\"salt\", saltModTen)\n",
    "salted.show(numRows = 1, truncate = 200, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам существенно снизить объем данных в каждой партиции (30к vs 3к):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-----+\n",
      "|type          |salt|count|\n",
      "+--------------+----+-----+\n",
      "|small_airport |9   |3455 |\n",
      "|small_airport |3   |3452 |\n",
      "|small_airport |8   |3452 |\n",
      "|small_airport |4   |3426 |\n",
      "|small_airport |2   |3404 |\n",
      "|small_airport |5   |3398 |\n",
      "|small_airport |0   |3391 |\n",
      "|small_airport |1   |3369 |\n",
      "|small_airport |6   |3337 |\n",
      "|small_airport |7   |3314 |\n",
      "|heliport      |2   |1214 |\n",
      "|heliport      |8   |1185 |\n",
      "|heliport      |3   |1142 |\n",
      "|heliport      |0   |1135 |\n",
      "|heliport      |9   |1130 |\n",
      "|heliport      |4   |1109 |\n",
      "|heliport      |6   |1106 |\n",
      "|heliport      |7   |1104 |\n",
      "|heliport      |1   |1097 |\n",
      "|heliport      |5   |1094 |\n",
      "|medium_airport|0   |494  |\n",
      "|medium_airport|5   |486  |\n",
      "|medium_airport|3   |467  |\n",
      "|medium_airport|2   |466  |\n",
      "|medium_airport|8   |458  |\n",
      "|medium_airport|1   |450  |\n",
      "|medium_airport|6   |437  |\n",
      "|medium_airport|7   |430  |\n",
      "|medium_airport|9   |425  |\n",
      "|medium_airport|4   |418  |\n",
      "|closed        |9   |399  |\n",
      "|closed        |1   |392  |\n",
      "|closed        |5   |379  |\n",
      "|closed        |6   |379  |\n",
      "|closed        |7   |359  |\n",
      "|closed        |3   |353  |\n",
      "|closed        |0   |350  |\n",
      "|closed        |8   |348  |\n",
      "|closed        |4   |335  |\n",
      "|closed        |2   |324  |\n",
      "|seaplane_base |4   |117  |\n",
      "|seaplane_base |8   |114  |\n",
      "|seaplane_base |3   |111  |\n",
      "|seaplane_base |7   |110  |\n",
      "|seaplane_base |6   |108  |\n",
      "|seaplane_base |0   |99   |\n",
      "|seaplane_base |9   |93   |\n",
      "|seaplane_base |1   |91   |\n",
      "|seaplane_base |5   |86   |\n",
      "|seaplane_base |2   |85   |\n",
      "|large_airport |2   |73   |\n",
      "|large_airport |9   |65   |\n",
      "|large_airport |7   |63   |\n",
      "|large_airport |5   |62   |\n",
      "|large_airport |3   |61   |\n",
      "|large_airport |4   |61   |\n",
      "|large_airport |8   |61   |\n",
      "|large_airport |6   |60   |\n",
      "|large_airport |1   |59   |\n",
      "|large_airport |0   |48   |\n",
      "|balloonport   |7   |4    |\n",
      "|balloonport   |8   |3    |\n",
      "|balloonport   |1   |3    |\n",
      "|balloonport   |9   |3    |\n",
      "|balloonport   |4   |2    |\n",
      "|balloonport   |0   |2    |\n",
      "|balloonport   |3   |2    |\n",
      "|balloonport   |6   |2    |\n",
      "|balloonport   |2   |1    |\n",
      "|balloonport   |5   |1    |\n",
      "+--------------+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "firstStep = [type: string, salt: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, salt: int ... 1 more field]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstStep = salted.groupBy('type, 'salt).count()\n",
    "\n",
    "firstStep.orderBy('count.desc).show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторым шагом мы делаем еще один агрегат, суммируя предыдущие значения `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|type          |count|\n",
      "+--------------+-----+\n",
      "|small_airport |33998|\n",
      "|heliport      |11316|\n",
      "|medium_airport|4531 |\n",
      "|closed        |3618 |\n",
      "|seaplane_base |1014 |\n",
      "|large_airport |613  |\n",
      "|balloonport   |23   |\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "secondStep = [type: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, count: bigint]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val secondStep = firstStep.groupBy('type).agg(sum(\"count\").alias(\"count\"))\n",
    "\n",
    "secondStep.orderBy('count.desc).show(200, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то, что мы сделали две группировки вместо одной, распределение данных по воркерам было более равномерным, что позволило избежать OOM на воркерах.\n",
    "\n",
    "### Выводы:\n",
    "+ Партиционирование - важный аспект распределенных вычислений, от которого напрямую зависит стабильность и скорость вычислений\n",
    "+ В Spark всегда работает правило 1 TASK = 1 THREAD = 1 PARTITION\n",
    "+ Репартиционирование и соление данных позволяет решить проблему перекоса данных и вычислений\n",
    "+ Важно помнить, что репартиционирование использует дисковую и сетевую подсистемы - обмен данными происходит **по сети**, а результат записывается **на диск**, что может стать узким местом при выполнении репартиционирования\n",
    "\n",
    "## Встроенные функции\n",
    "Помимо базовых SQL операторов, в Spark существует большой набор встроенных функций:\n",
    "+ API методы из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n",
    "+ [SQL built-in functions](https://spark.apache.org/docs/latest/api/sql/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|pmod|\n",
      "+---+----+\n",
      "|  0|   0|\n",
      "|  1|   1|\n",
      "|  2|   0|\n",
      "|  3|   1|\n",
      "|  4|   0|\n",
      "|  5|   1|\n",
      "|  6|   0|\n",
      "|  7|   1|\n",
      "|  8|   0|\n",
      "|  9|   1|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n",
       "newCol = pmod(id, 2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pmod(id, 2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.range(0,10)\n",
    "\n",
    "// используем org.apache.spark.sql.functions\n",
    "val newCol: Column = pmod(col(\"id\"), lit(2))\n",
    "df.withColumn(\"pmod\", newCol).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|pmod|\n",
      "+---+----+\n",
      "|  0|   0|\n",
      "|  1|   1|\n",
      "|  2|   0|\n",
      "|  3|   1|\n",
      "|  4|   0|\n",
      "|  5|   1|\n",
      "|  6|   0|\n",
      "|  7|   1|\n",
      "|  8|   0|\n",
      "|  9|   1|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newCol = pmod(id, 2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pmod(id, 2)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "\n",
    "// используем SQL built-in functions\n",
    "val newCol: Column = expr(\"\"\"pmod(id, 2)\"\"\")\n",
    "df.withColumn(\"pmod\", newCol).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "+ Spark обладает широким набором функций для работы с колонками разных типов, включая простые типы - строки, числа, и т. д., а также словари, массивы и структуры\n",
    "+ Встроенные функции принимают колонки `org.apache.spark.sql.Column` и возвращают `org.apache.spark.sql.Column` в большинстве случаев\n",
    "+ Встроенные функции доступны в двух местах - org.apache.spark.sql.functions и SQL built-in functions\n",
    "+ Встроенные функции можно (и нужно) использовать вместе - на вход во встроенные функции могут подаваться результаты встроенной функции, тк все они возвращают `sql.Column` \n",
    "\n",
    "### Пользовательские функции\n",
    "\n",
    "В том случае, если функционала встроенных функций не хватает, можно написать пользовательскую функцию - UDF. Пользовательская функция может принимать до 16 аргументов. Соответствие Spark и Scala типов описано [здесь](https://spark.apache.org/docs/latest/sql-reference.html#data-types)\n",
    "\n",
    "Необходимо помнить, что `null` в Spark превращается в `null` внутри UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|id |idPlusOne|\n",
      "+---+---------+\n",
      "|0  |1        |\n",
      "|1  |2        |\n",
      "|2  |3        |\n",
      "|3  |4        |\n",
      "|4  |5        |\n",
      "|5  |6        |\n",
      "|6  |7        |\n",
      "|7  |8        |\n",
      "|8  |9        |\n",
      "|9  |10       |\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n",
       "plusOne = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,LongType,Some(List(LongType)))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{udf, col}\n",
    "\n",
    "val df = spark.range(0,10)\n",
    "\n",
    "val plusOne = udf { (value: Long) => value + 1 }\n",
    "\n",
    "df.withColumn(\"idPlusOne\", plusOne(col(\"id\"))).show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plusOne2: (x: Int)Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plusOne2(x: Int): Int = { x + 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_udf_2 = UserDefinedFunction(<function1>,IntegerType,Some(List(IntegerType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,IntegerType,Some(List(IntegerType)))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val my_udf_2 = udf { plusOne2 _ }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|UDF(id)|\n",
      "+-------+\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "|      6|\n",
      "|      7|\n",
      "|      8|\n",
      "|      9|\n",
      "|     10|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).select(my_udf_2('id)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пользовательская функция может возвращать:\n",
    "+ простой тип - `String`, `Long`, `Float`, `Boolean` и т.д.\n",
    "+ массив - любые коллекции, наследующие `Seq[T]` - `List[T]`, `Vector[T]` и т. д.\n",
    "+ словарь - `Map[A,B]`\n",
    "+ инстанс `case class`'а\n",
    "+ Option[T]\n",
    "\n",
    "Реализуем функцию, которая возвращает имя хоста, на котором работает воркер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Option[T]\n",
    "Try[T]\n",
    "List[T]\n",
    "Vector[T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|id |hostname      |\n",
      "+---+--------------+\n",
      "|0  |spark-node-3  |\n",
      "|1  |spark-node-3  |\n",
      "|2  |spark-node-3  |\n",
      "|3  |spark-node-3  |\n",
      "|4  |spark-node-3  |\n",
      "|5  |spark-master-6|\n",
      "|6  |spark-master-6|\n",
      "|7  |spark-master-6|\n",
      "|8  |spark-master-6|\n",
      "|9  |spark-master-6|\n",
      "+---+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hostname = UserDefinedFunction(<function0>,StringType,Some(List()))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function0>,StringType,Some(List()))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.net.InetAddress\n",
    "\n",
    "val hostname = udf { () => InetAddress.getLocalHost().getHostName() }\n",
    "\n",
    "df.withColumn(\"hostname\", hostname()).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем использовать монады `Try[T]` и `Option[T]` и для написания пользовательской функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- divideTwoBy: long (nullable = true)\n",
      "\n",
      "+---+-----------+\n",
      "|id |divideTwoBy|\n",
      "+---+-----------+\n",
      "|0  |null       |\n",
      "|1  |2          |\n",
      "|2  |1          |\n",
      "|3  |0          |\n",
      "|4  |0          |\n",
      "|5  |0          |\n",
      "|6  |0          |\n",
      "|7  |0          |\n",
      "|8  |0          |\n",
      "|9  |0          |\n",
      "+---+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n",
       "divideTwoBy = UserDefinedFunction(<function1>,LongType,Some(List(LongType)))\n",
       "result = [id: bigint, divideTwoBy: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, divideTwoBy: bigint]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Try\n",
    "import org.apache.spark.sql.functions.{udf, col}\n",
    "\n",
    "val df = spark.range(0,10)\n",
    "\n",
    "val divideTwoBy = udf { (inputValue: Long) => Try { 2L / inputValue }.toOption }\n",
    "\n",
    "val result = df.withColumn(\"divideTwoBy\", divideTwoBy(col(\"id\")))\n",
    "result.printSchema\n",
    "result.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(StructField(id,LongType,false))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructField(id,LongType,false)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.types.LongType$"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema(\"id\").dataType.getClass.getCanonicalName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:53: error: not found: value LongType\n",
       "       LongType // java.lang.Long\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LongType // java.lang.Long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "+ Пользовательские функции позволяют реализовать произвольный алгоритм и использовать его в DF API\n",
    "+ Пользовательские функции работают медленнее встроенных, поскольку при использовании встроенных функций Spark использует ряд оптимизаций, например векторизацию вычислений на уровне CPU\n",
    "\n",
    "## Соединения\n",
    "\n",
    "Join'ы позволяют соединять два DF в один по заданным условиям.\n",
    "\n",
    "По типу условия join'ы делятся на:\n",
    "+ equ-join - соединение по равенству одного или более ключей\n",
    "+ non-equ join - соединение по условию, отличному от равенства одного или более ключей\n",
    "\n",
    "По методу соединения join'ы бывают:\n",
    "![Joins](http://kirillpavlov.com/images/join-types.png)\n",
    "[Источник](http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/)\n",
    "\n",
    "Добавим новую колонку к датасету `airports`, в которой будет процент заданного типа аэропорта ко всем типам аэропорта по каждой стране. Первым шагом посчитаем число аэропортов каждого типа по стране:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------------+\n",
      "|type          |iso_country|cnt_country_type|\n",
      "+--------------+-----------+----------------+\n",
      "|large_airport |GB         |27              |\n",
      "|small_airport |MP         |1               |\n",
      "|heliport      |CH         |19              |\n",
      "|closed        |LT         |4               |\n",
      "|medium_airport|SS         |3               |\n",
      "+--------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggTypeCountry = [type: string, iso_country: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, iso_country: string ... 1 more field]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{count, round, lit}\n",
    "\n",
    "val aggTypeCountry = airports.groupBy('type, 'iso_country).agg(count(\"*\").alias(\"cnt_country_type\"))\n",
    "\n",
    "aggTypeCountry.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем количество аэропортов по каждой стране:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|iso_country|cnt_country|\n",
      "+-----------+-----------+\n",
      "|DZ         |61         |\n",
      "|LT         |57         |\n",
      "|MM         |75         |\n",
      "|CI         |26         |\n",
      "|TC         |8          |\n",
      "+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggCountry = [iso_country: string, cnt_country: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, cnt_country: bigint]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggCountry = airports.groupBy('iso_country).agg(count(\"*\").alias(\"cnt_country\"))\n",
    "aggCountry.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим получившиеся датасеты и получим процентное распределение типов аэропорта по стране"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------+\n",
      "|iso_country|type          |percent|\n",
      "+-----------+--------------+-------+\n",
      "|GB         |large_airport |2.97   |\n",
      "|MP         |small_airport |9.09   |\n",
      "|CH         |heliport      |21.84  |\n",
      "|LT         |closed        |7.02   |\n",
      "|SS         |medium_airport|6.52   |\n",
      "+-----------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "percent = [iso_country: string, type: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, type: string ... 1 more field]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val percent = \n",
    "    aggTypeCountry\n",
    "        .join(aggCountry, Seq(\"iso_country\"), \"inner\")\n",
    "        .select('iso_country, 'type, (round(lit(100) * 'cnt_country_type / 'cnt_country, 2).alias(\"percent\")))\n",
    "percent.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим полученный датасет с изначальным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------+-------+\n",
      "|ident  |iso_country|type          |percent|\n",
      "+-------+-----------+--------------+-------+\n",
      "|EGGW   |GB         |large_airport |2.97   |\n",
      "|EGHH   |GB         |large_airport |2.97   |\n",
      "|EGPH   |GB         |large_airport |2.97   |\n",
      "|EGSS   |GB         |large_airport |2.97   |\n",
      "|EYKR   |LT         |closed        |7.02   |\n",
      "|LT-0013|LT         |closed        |7.02   |\n",
      "|LSHC   |CH         |heliport      |21.84  |\n",
      "|LSHG   |CH         |heliport      |21.84  |\n",
      "|LSHI   |CH         |heliport      |21.84  |\n",
      "|NZAA   |NZ         |large_airport |1.42   |\n",
      "|LOXZ   |AT         |medium_airport|4.83   |\n",
      "|EBBE   |BE         |medium_airport|5.52   |\n",
      "|EBCV   |BE         |medium_airport|5.52   |\n",
      "|EEKA   |EE         |medium_airport|13.51  |\n",
      "|EEKE   |EE         |medium_airport|13.51  |\n",
      "|HECW   |EG         |medium_airport|36.23  |\n",
      "|HEPS   |EG         |medium_airport|36.23  |\n",
      "|HU-0014|HU         |small_airport |80.73  |\n",
      "|HU-0015|HU         |small_airport |80.73  |\n",
      "|HU-0016|HU         |small_airport |80.73  |\n",
      "+-------+-----------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [iso_country: string, type: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, type: string ... 11 more fields]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = airports.join(percent, Seq(\"iso_country\", \"type\"), \"left\")\n",
    "result.select('ident, 'iso_country, 'type, 'percent).sample(0.2).show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во всех наших джойнах присутствует массив `Seq[String]`. Это синтаксических сахар, позволяющий не переименовывать колонки датасетов, а просто указать, что соединение будет делаться по колонкам с именами, входящим в массив.\n",
    "\n",
    "В общем случае условие джойна должно быть выражено в виде колонки `sql.Column`, например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinCondition = ((left_a = right_a) AND (left_b = right_b))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((left_a = right_a) AND (left_b = right_b))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "val joinCondition: Column = col(\"left_a\") === col(\"right_a\") and col(\"left_b\") === col(\"right_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом в данном выражении допускается использование встроенных функций, пользовательских функций и операторов сравнения. Однако следует помнить, что мы выполняем джойн двух распределенных датасетов и если условие соединения будет плохо составлено, то Spark выполнит `cross join`, производительность которого будет \"крайне мала\" &copy;\n",
    "\n",
    "### Выводы:\n",
    "+ Spark поддерживает большое число типов соединений\n",
    "+ Условием соединения может быть `Seq[String]`, либо `sql.Column`\n",
    "+ При использовании сложных условий соединения следует избегать тех, которые приведут к `cross join`\n",
    "\n",
    "## Оконные функции\n",
    "Оконные функции позволяют делать функции над \"окнами\" (кто бы мог подумать) данных\n",
    "\n",
    "Окно создается из класса [org.apache.spark.sql.expressions.Window](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.Window) с указанием полей, определяющих границы окон и полей, определяющих порядок сортировки внутри окна:\n",
    "\n",
    "```val window = Window.partitionBy(\"a\", \"b\").orderBy(\"a\")```\n",
    "\n",
    "Применяя окна, можно использовать такие полезные функции из [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$), как ```lag()``` и ```lead()```, а также эффективно работать с данными time-series данными.\n",
    "\n",
    "Выполним задачу с вычисление процента отношения типов аэропортов, используя оконные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------+-------+\n",
      "|ident  |iso_country|type          |percent|\n",
      "+-------+-----------+--------------+-------+\n",
      "|LT-0013|LT         |closed        |7.02   |\n",
      "|DAAG   |DZ         |large_airport |1.64   |\n",
      "|DAAD   |DZ         |medium_airport|59.02  |\n",
      "|DAAJ   |DZ         |medium_airport|59.02  |\n",
      "|DAAP   |DZ         |medium_airport|59.02  |\n",
      "|DAAV   |DZ         |medium_airport|59.02  |\n",
      "|DAAZ   |DZ         |medium_airport|59.02  |\n",
      "|DABB   |DZ         |medium_airport|59.02  |\n",
      "|DAON   |DZ         |medium_airport|59.02  |\n",
      "|DAUA   |DZ         |medium_airport|59.02  |\n",
      "|DAUB   |DZ         |medium_airport|59.02  |\n",
      "|DAUI   |DZ         |medium_airport|59.02  |\n",
      "|DAUO   |DZ         |medium_airport|59.02  |\n",
      "|DAUZ   |DZ         |medium_airport|59.02  |\n",
      "|VYHH   |MM         |medium_airport|26.67  |\n",
      "|VYLK   |MM         |medium_airport|26.67  |\n",
      "|VYLS   |MM         |medium_airport|26.67  |\n",
      "|VYMS   |MM         |medium_airport|26.67  |\n",
      "|VYSW   |MM         |medium_airport|26.67  |\n",
      "|DAAF   |DZ         |small_airport |36.07  |\n",
      "+-------+-----------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "windowCountry = org.apache.spark.sql.expressions.WindowSpec@5f24e18c\n",
       "windowTypeCountry = org.apache.spark.sql.expressions.WindowSpec@496eefc0\n",
       "result = [ident: string, type: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 13 more fields]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val windowCountry = Window.partitionBy(\"iso_country\")\n",
    "val windowTypeCountry = Window.partitionBy(\"type\", \"iso_country\")\n",
    "\n",
    "val result = airports\n",
    "                .withColumn(\"cnt_country\", count(\"*\").over(windowCountry))\n",
    "                .withColumn(\"cnt_country_type\", count(\"*\").over(windowTypeCountry))\n",
    "                .withColumn(\"percent\", round(lit(100) * 'cnt_country_type / 'cnt_country, 2))\n",
    "                            \n",
    "result.select('ident, 'iso_country, 'type, 'percent).sample(0.2).show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Оконные функции позволяют применять функции, применительно к окнам данных\n",
    "+ Окно определяется списком колонок и сортировкой\n",
    "+ Применение оконных функций приводит к `shuffle`\n",
    "\n",
    "После завершения работы не забывайте останавливать `SparkSession`, чтобы освободить ресурсы кластера!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
