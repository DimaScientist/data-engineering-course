{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Введение в Spark: RDD API\n",
    "**Сергей Гришаев**  \n",
    "serg.grishaev@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Общие сведения\n",
    "+ Область применения\n",
    "+ Архитектура приложений\n",
    "+ Базовые функции RDD API\n",
    "+ Рair RDD функции\n",
    "+ Работа с данными\n",
    "\n",
    "## Общие сведения\n",
    "\n",
    "**Apache Spark** - это:\n",
    "+ Платформа для построения распределнных приложений обработки данных\n",
    "+ Эволюция Hadoop MapReduce\n",
    "+ Библиотека под Python/Scala\n",
    "+ Один из самых популярных проектов в области обработки больших данных\n",
    "\n",
    "\n",
    "## Область применения\n",
    "- Распределенная обработка больших данных\n",
    "- Построение ETL пайплайнов\n",
    "- Работа со структурированными данными (SQL)\n",
    "- Разработка стриминг приложений\n",
    "\n",
    "## Архитектура приложения\n",
    "\n",
    "+ **Driver** (aka Master):\n",
    "  - предоставляет API через SparkSession и SparkContext\n",
    "  - выполняет ваш код - python файл или скомпилированный .jar\n",
    "  - контролирует выполнение задачи\n",
    "\n",
    "+ **Workers** (aka Executors or Slaves):\n",
    "  - обрабатывают данные\n",
    "  - каждый Worker работает со своим сегментом данных - **Partition**\n",
    "  - не выполняются ваш код напрямую\n",
    "  - получают задачи от Driver\n",
    "+ **Cluster Manager** (YARN/Mesos):\n",
    "  - отвечает за аллокацию контейнеров, выполняющих код драйвера и воркеров, на кластере\n",
    "  - квотирует ресурсы между пользователями\n",
    "  - контролирует состояние контейнеров\n",
    " \n",
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-06-05%2013.07.01.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset\n",
    "\n",
    "**RDD** aka Resilient Distributed Dataset - самая базовая и самая низкоуровневая структура в Spark, доступная разработчику. Представляет собой типизированную неизменяемую неупорядоченную партиционированную коллекцию данных, распределенную по узлам кластера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Vector has 5 elements, the first one is Moscow}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cities = Vector(Moscow, Paris, Madrid, London, New York)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector(Moscow, Paris, Madrid, London, New York)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cities: Vector[String] = Vector(\"Moscow\", \"Paris\", \"Madrid\", \"London\", \"New York\")\n",
    "println(s\"The Vector has ${cities.length} elements, the first one is ${cities.head}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cities = Vector(Moscow, Paris, Madrid, London, New York, Cape Town)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector(Moscow, Paris, Madrid, London, New York, Cape Town)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cities = Vector(\"Moscow\", \"Paris\", \"Madrid\", \"London\", \"New York\", \"Cape Town\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cities2 = Vector(Moscow, Paris, Madrid, London, New York)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector(Moscow, Paris, Madrid, London, New York)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var cities2: Vector[String] = Vector(\"Moscow\", \"Paris\", \"Madrid\", \"London\", \"New York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cities2 = Vector(Moscow, Paris, Madrid, London, New York, Test city)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector(Moscow, Paris, Madrid, London, New York, Test city)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities2 = Vector(\"Moscow\", \"Paris\", \"Madrid\", \"London\", \"New York\", \"Test city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cities2 = Vector()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities2 = Vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newCities = Vector(Moscow, Paris, Madrid, London, New York, Madrid)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector(Moscow, Paris, Madrid, London, New York, Madrid)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newCities = cities :+ \"Madrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vector(Moscow, Paris, Madrid, London, New York)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vector(Moscow, Paris, Madrid, London, New York, Madrid)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newCities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toUpper: (x: String)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def toUpper(x: String): String = { \n",
    "    x.toUpperCase\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toUpper2 = > String = <function1>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "> String = <function1>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val toUpper2 = (x: String) => {\n",
    "    x.toUpperCase\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FOO"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toUpper(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FOO"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toUpper2(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vector(PARIS, NEW YORK, MOSCOW, MADRID, LONDON)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities\n",
    "    .sorted\n",
    "    .reverse\n",
    "    .map(x => x.toUpperCase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vector(PARIS, NEW YORK, MOSCOW, MADRID, LONDON)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities\n",
    "    .sorted\n",
    "    .reverse\n",
    "    .map(toUpper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cities = Vector()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cities = Vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Paris"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Paris"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.apply(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@7d7eac85"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@50060ba7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@7d7eac85"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RDD has 6 elements, the first one is Moscow\n",
      "The RDD has 6 elements, the first one is Moscow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd = ParallelCollectionRDD[0] at parallelize at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at <console>:31"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val rdd: RDD[String] = sc.parallelize(cities)\n",
    "println(s\"The RDD has ${rdd.count} elements, the first one is ${rdd.first}\")\n",
    "println(f\"The RDD has ${rdd.count} elements, the first one is ${rdd.first}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD может быть создана из:\n",
    "- локальной коллекции на драйвере\n",
    "- файла (локального или на распределенной файловой системе, например HDFS)\n",
    "- базы данных\n",
    "\n",
    "### Операции с RDD\n",
    "1. Трансформации (e.g. map, filter)\n",
    "2. Действия (e.g. reduce, collect, count, foreach)\n",
    "\n",
    "**Трансформации** (Transormations):\n",
    "- всегда превращают один RDD в новый RDD\n",
    "- всегда являются ленивыми - создают граф вычислений, но не запускают их\n",
    "- иногда (часто) неявно требуют перемешивания данных между воркерами - **Shuffle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New RDD: MOSCOW, PARIS, MADRID\n",
      "Old RDD: Moscow, Paris, Madrid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "upperRdd = MapPartitionsRDD[2] at map at <console>:33\n",
       "upperVec = Array(MOSCOW, PARIS, MADRID)\n",
       "oldVec = Array(Moscow, Paris, Madrid)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(Moscow, Paris, Madrid)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Трансформация map: не запускает вычислений, не изменяет изначальный RDD\n",
    "val upperRdd: RDD[String] = rdd.map { city => city.toUpperCase }\n",
    "\n",
    "\n",
    "// Метод take возвращает N первых элементов RDD\n",
    "val upperVec: Array[String] = upperRdd.take(3)\n",
    "val oldVec: Array[String] = rdd.take(3)\n",
    "\n",
    "// метод mkString позволяет сделать из любой локальной коллекции строку\n",
    "println(s\"\"\"New RDD: ${upperVec.mkString(\", \")}\"\"\")\n",
    "println(s\"\"\"Old RDD: ${oldVec.mkString(\", \")}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oldVec = Array(foo)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(foo)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val oldVec = Array(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oldVec = Array(foo2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(foo2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldVec = Array(\"foo2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldVec(0) = \"foo123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo123"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldVec(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2019.15.06.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Действия** (Actions):\n",
    "- выполняют действие над RDD\n",
    "- запускают вычисления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RDD contains 40 letters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count = 40\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Действие reduce применяет функцию f к промежуточному результату \n",
    "// от предыдущей итерации со следующим элементом коллекции\n",
    "val count = rdd.map( x => x.length ).reduce { (x,y) => x + y }\n",
    "println(s\"The RDD contains ${count} letters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.rdd.ParallelCollectionRDD"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getClass.getCanonicalName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.getClass.getCanonicalName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2019.22.10.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры операций с RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фильтрация RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following city names starts with M: MOSCOW, MADRID\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "startsWithM = MapPartitionsRDD[5] at filter at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[5] at filter at <console>:30"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val startsWithM = upperRdd.filter( x => x.startsWith(\"M\") )\n",
    "println(s\"\"\"The following city names starts with M: ${startsWithM.take(5).mkString(\", \")}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2019.33.22.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет количества элементов в RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RDD contains 2 elements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countM = 2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val countM: Long = startsWithM.count()\n",
    "println(s\"The RDD contains $countM elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2019.45.03.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передача ВСЕХ элементов RDD на драйвер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array contains MOSCOW: true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "localArray = Array(MOSCOW, MADRID)\n",
       "containsMoscow = true\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val localArray: Array[String] = startsWithM.collect()\n",
    "val containsMoscow: Boolean = localArray.contains(\"MOSCOW\")\n",
    "println(s\"The array contains MOSCOW: $containsMoscow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "public scala.Option org.apache.spark.rdd.MapPartitionsRDD.partitioner()\n",
       "public boolean org.apache.spark.rdd.MapPartitionsRDD.isBarrier_()\n",
       "private boolean org.apache.spark.rdd.MapPartitionsRDD.isBarrier_$lzycompute()\n",
       "public scala.Enumeration$Value org.apache.spark.rdd.MapPartitionsRDD.getOutputDeterministicLevel()\n",
       "public org.apache.spark.Partition[] org.apache.spark.rdd.MapPartitionsRDD.getPartitions()\n",
       "public void org.apache.spark.rdd.MapPartitionsRDD.clearDependencies()\n",
       "public static boolean org.apache.spark.rdd.MapPartitionsRDD.$lessinit$greater$default$4()\n",
       "public static boolean org.apache.spark.rdd.MapPartitionsRDD.$lessinit$greater$default$5()\n",
       "public static boolean org.apache.spark.rdd.MapPartitionsRDD.$lessinit$greater$default$3()\n",
       "public void org.apache.spark.rdd.Ma...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "startsWithM.getClass.getDeclaredMethods.mkString(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передача N элементов по сети на драйвер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two elements of the RDD are: MOSCOW\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "twoElements = Array(MOSCOW)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(MOSCOW)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val twoElements: Array[String] = startsWithM.take(1)\n",
    "println(s\"\"\"Two elements of the RDD are: ${twoElements.mkString(\", \")}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сортировка и выборка из N первых элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two elements of the RDD are: MADRID, MOSCOW\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "twoElementsSorted = Array(MADRID, MOSCOW)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(MADRID, MOSCOW)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val twoElementsSorted: Array[String] = startsWithM.takeOrdered(2)\n",
    "println(s\"\"\"Two elements of the RDD are: ${twoElementsSorted.mkString(\", \")}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2020.40.09.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спрямление вложенных коллекций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(M, o, s, c, o, w)\n",
      "Vector(P, a, r, i, s)\n",
      "####\n",
      "m\n",
      "o\n",
      "s\n",
      "c\n",
      "o\n",
      "w\n",
      "p\n",
      "a\n",
      "r\n",
      "i\n",
      "s\n",
      "m\n",
      "a\n",
      "d\n",
      "r\n",
      "i\n",
      "d\n",
      "l\n",
      "o\n",
      "n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mappedRdd = MapPartitionsRDD[9] at map at <console>:31\n",
       "flatMappedRdd = MapPartitionsRDD[10] at flatMap at <console>:35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at flatMap at <console>:35"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mappedRdd: RDD[Vector[Char]] = rdd.map(x => x.toVector)\n",
    "mappedRdd.take(2).foreach(println)\n",
    "println(\"####\")\n",
    "\n",
    "val flatMappedRdd = rdd.flatMap( x => x.toLowerCase )\n",
    "flatMappedRdd.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(M, o, s, c, o, w)\n",
      "Vector(P, a, r, i, s)\n"
     ]
    }
   ],
   "source": [
    "mappedRdd.take(2).foreach(x => println(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(M, o, s, c, o, w)\n",
      "Vector(P, a, r, i, s)\n"
     ]
    }
   ],
   "source": [
    "mappedRdd.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(M, o, s, c, o, w)\n",
      "Vector(P, a, r, i, s)\n"
     ]
    }
   ],
   "source": [
    "mappedRdd.take(2).foreach( println(_) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(m, o, s, c, o, w, p, a, r, i, s, m, a, d, r, i, d, l, o, n, d, o, n, n, e, w,  , y, o, r, k, c, a, p, e,  , t, o, w, n)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(_.toLowerCase).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letters in the RDD are: a c d e i k l m n o p r s t w y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "uniqueLetters = a c d e i k l m n o p r s t w y\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "a c d e i k l m n o p r s t w y"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val uniqueLetters: String = \n",
    "    flatMappedRdd\n",
    "        .distinct\n",
    "        .filter(x => x != ' ')\n",
    "        .collect\n",
    "        .sorted\n",
    "        .mkString(\" \")\n",
    "\n",
    "println(s\"Letters in the RDD are: ${uniqueLetters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2020.42.42.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- RDD - это неизменяемый распределенный набор данных\n",
    "- Трансформации (map, filter, flatMap) создают новый RDD из существующего и не изменяют существующий\n",
    "- Любые трансформации являются ленивыми и не запускают вычислений\n",
    "- Действия (count, reduce, collect, take) запускают вычисления\n",
    "\n",
    "### Полезные ссылки:\n",
    "- [RDD API Reference](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)\n",
    "- [RDD Programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [Scala 2.11.12 API](https://www.scala-lang.org/files/archive/api/2.11.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PairRDD функции\n",
    "\n",
    "Во всех вышестоящих экспериментах мы создавали RDD, состоящие из элементов базовых типов - числовых, срок и символов. На самом деле RDD не имеют как таковых органичений на тип элементов. Ими могут выступать коллекции, кейс \n",
    "классы, кортежи и т.д.\n",
    "\n",
    "**PairRDD** - расширенный класс функций, доступных для RDD, где элементы - это кортеж (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = (1,foo)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1,foo)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: (Int, String) = (1, \"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo._1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo._2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left = 1\n",
       "right = foo\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "foo"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (left: Int, right: String) = foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bar = List(1, 2, 3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(1, 2, 3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bar = List(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first = 1\n",
       "second = 2\n",
       "third = 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val List(first, second, third) = bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(m,1)\n",
      "(o,1)\n",
      "(s,1)\n",
      "(c,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pairRdd = MapPartitionsRDD[17] at map at <console>:28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[17] at map at <console>:28"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairRdd: RDD[(Char, Int)] = rdd.flatMap( x => x.toLowerCase ).map( x => (x, 1))\n",
    "pairRdd.take(4).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(m, o, s, c, o, w, p, a, r, i, s, m, a, d, r, i, d, l, o, n, d, o, n, n, e, w,  , y, o, r, k, c, a, p, e,  , t, o, w, n)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRdd.map (x => x._1).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRdd.map { case (_, v) => v }.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iter = non-empty iterator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "non-empty iterator"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iter: Iterator[Any] = foo.productIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "foo\n"
     ]
    }
   ],
   "source": [
    "while (iter.hasNext) { \n",
    "    val next = iter.next\n",
    "    println(next)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter.toList.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iterator[T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "countByKey подсчитывает количество кортежей по каждому ключу и возвращает локальный Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "letterCount = Map(e -> 2, s -> 2, n -> 4, y -> 1, t -> 1, a -> 3, m -> 2, i -> 2,   -> 2, l -> 1, p -> 2, c -> 2, r -> 3, w -> 3, k -> 1, o -> 6, d -> 3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(e -> 2, s -> 2, n -> 4, y -> 1, t -> 1, a -> 3, m -> 2, i -> 2,   -> 2, l -> 1, p -> 2, c -> 2, r -> 3, w -> 3, k -> 1, o -> 6, d -> 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val letterCount: scala.collection.Map[Char,Long] = pairRdd.countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letterCount('e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.util.NoSuchElementException\n",
       "Message: key not found: b\n",
       "StackTrace:   at scala.collection.MapLike$class.default(MapLike.scala:228)\n",
       "  at scala.collection.AbstractMap.default(Map.scala:59)\n",
       "  at scala.collection.MapLike$class.apply(MapLike.scala:141)\n",
       "  at scala.collection.AbstractMap.apply(Map.scala:59)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letterCount('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bar = None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bar: Option[Long] = letterCount.get('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = Some(2)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Some(2)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: Option[Long] = letterCount.get('e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: scala.MatchError\n",
       "Message: None (of class scala.None$)\n",
       "StackTrace: It would fail on the following input: None\n",
       "       val x = bar match {\n",
       "               ^\n",
       "scala.MatchError: None (of class scala.None$)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = bar match { \n",
    "    case Some(v) => v\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y = 2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val y = bar match { \n",
    "    case Some(v) => v\n",
    "    case None => -1 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: Long = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = Some(1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Some(1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: Option[Long] = Some(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(2)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.map(x => x + 1).filter( x => x == 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List(1,2,3).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = Some(List())\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Some(List())"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: Option[List[Int]] = Option(List())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "elem = None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val elem: Option[Int] = foo.flatMap(x => x.headOption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: Option[Long] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:26: error: an expression of type Null is ineligible for implicit conversion\n",
       "       val test1: Int = null\n",
       "                        ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test1: Int = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: java.lang.Integer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.isInstanceOf[Int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:26: error: an expression of type Null is ineligible for implicit conversion\n",
       "       val foo: Int = null\n",
       "                      ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: Int = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo: Integer = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val foo: java.lang.Integer = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.isInstanceOf[Int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res302: Integer = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " List(2, 3) : hooray!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myList = List(1, 2, 3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(1, 2, 3)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myList = letterCount.values.toList.distinct.sorted.take(3)\n",
    "myList match { \n",
    "    case 1 :: x => println(s\" $x : hooray!\")\n",
    "    case _ => println(\"hello!\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(1 -> foo, 2 -> bar)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Map( (1, \"foo\"), (2, \"bar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,foo)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 -> \"foo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Map( 1 -> \"foo\", 2 -> \"bar\").contains(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduceByKey работает аналогично обычному reduce, но промежуточный итог накапливается по каждому ключу независимо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d,3) (p,2) (t,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "letterCount = ShuffledRDD[27] at reduceByKey at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[27] at reduceByKey at <console>:32"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val letterCount: RDD[(Char, Int)] = pairRdd.reduceByKey { (x,y) => x + y }\n",
    "println(letterCount.take(3).mkString(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://raw.githubusercontent.com/tenkeiu8/spark-examples/master/images/photo_2021-09-19%2020.57.17.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join позволяет соединить два RDD по ключу. Поддерживаются join, leftOuterJoin и fullOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter d is my favourite and it appears in the RDD 3 times\n",
      "The letter p is not my favourite!\n",
      "The letter t is not my favourite!\n",
      "The letter   is not my favourite!\n",
      "The letter n is not my favourite!\n",
      "The letter r is not my favourite!\n",
      "The letter l is not my favourite!\n",
      "The letter w is not my favourite!\n",
      "The letter s is not my favourite!\n",
      "The letter e is not my favourite!\n",
      "The letter a is my favourite and it appears in the RDD 3 times\n",
      "The letter k is not my favourite!\n",
      "The letter y is not my favourite!\n",
      "The letter i is not my favourite!\n",
      "The letter o is my favourite and it appears in the RDD 6 times\n",
      "The letter m is not my favourite!\n",
      "The letter c is not my favourite!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "favouriteLetters = Vector(a, d, o)\n",
       "favLetRdd = MapPartitionsRDD[29] at map at <console>:31\n",
       "joined = MapPartitionsRDD[32] at leftOuterJoin at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[32] at leftOuterJoin at <console>:34"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val favouriteLetters: Vector[Char] = Vector('a', 'd', 'o')\n",
    "val favLetRdd = sc.parallelize(favouriteLetters).map(x => (x,1))\n",
    "\n",
    "\n",
    "val joined: RDD[(Char, (Int, Option[Int]))] = letterCount.leftOuterJoin(favLetRdd)\n",
    "joined.collect.foreach { j => \n",
    "    val (letter, (leftCount, rightCount)) = j\n",
    "    rightCount match {\n",
    "        case Some(v) => println(s\"The letter $letter is my favourite and it appears in the RDD $leftCount times\")\n",
    "        case None => println(s\"The letter $letter is not my favourite!\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Apple\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Apple(size: Int, color: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = Apple(1,red)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Apple(1,red)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo: Apple = Apple(1, \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "red"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size = 1\n",
       "color = red\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "red"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Apple(size, color) = foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bar = List(Apple(1,red), Apple(1,red))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(Apple(1,red), Apple(1,red))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bar: List[Apple] = List(foo, foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List(1, red)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple(color=\"red\", size=1).productIterator.toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple(color=\"red\", size=1) == Apple(color=\"red\", size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Foo\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Foo(i: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo = Foo@f76d2b1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Foo@f76d2b1"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo = new Foo(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Foo2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object Foo2 { \n",
    "    val i: Int = 2\n",
    "    \n",
    "    def f(i: Int) {\n",
    "        println(i)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "Foo2.f(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined trait Fruit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trait Fruit { \n",
    "    val color: String\n",
    "    \n",
    "    def getColorUpperCase(): String = { \n",
    "        this.color.toUpperCase\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Apple\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Apple(size: Int, color: String) extends Fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Banana\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Banana(origin: String, color: String) extends Fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myList = List(Apple(1,green), Banana(Spain,yellow))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(Apple(1,green), Banana(Spain,yellow))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myList: List[Fruit] = List(Apple(1, \"green\"), Banana(\"Spain\", \"yellow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:34: error: not found: value Fruit\n",
       "           case Fruit(c) => s\"Unknown fruit, color: ${c}\"\n",
       "                ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList.map { \n",
    "    case Apple(s, c) => s\"Apple: $s, $c\"\n",
    "    case Banana(o, c) => s\"Banana: $o, $c\"\n",
    "    case _ => s\"Unknown fruit\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GREEN"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple(1, \"green\").getColorUpperCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined trait Fruit\n",
       "defined trait Round\n",
       "defined trait CanBeRed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trait Fruit\n",
    "trait Round\n",
    "trait CanBeRed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Apple\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Apple(size: Int) extends Fruit with Round with CanBeRed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- PairRDD функции - расширенный список функций, доступный для RDD, элементы которых являются кортежем (K, V)\n",
    "- PairRDD позволяют соединять два RDD по ключу K\n",
    "\n",
    "### Полезные ссылки:\n",
    "- [PairRDD API Reference](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с данными\n",
    "\n",
    "Для изучения структуры и вычислений RDD проведем анализ датасета [Airport Codes](https://datahub.io/core/airport-codes)  \n",
    "\n",
    "Метод `sc.textFile` позволяет прочитать файл на локальной, S3 или HDFS совместимой ФС. С помощью данного метода можно читать как обычные файлы, так и директории с файлами, а также архивы с данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd = /tmp/airport-codes.csv MapPartitionsRDD[36] at textFile at <console>:28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/tmp/airport-codes.csv MapPartitionsRDD[36] at textFile at <console>:28"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.textFile(\"/tmp/airport-codes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем первые 3 строки на экран:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(ident,type,name,elevation_ft,continent,iso_country,iso_region,municipality,gps_code,iata_code,local_code,coordinates, 00A,heliport,Total Rf Heliport,11,NA,US,US-PA,Bensalem,00A,,00A,\"40.07080078125, -74.93360137939453\", 00AA,small_airport,Aero B Ranch Airport,3435,NA,US,US-KS,Leoti,00AA,,00AA,\"38.704022, -101.473911\")"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ident,type,name,elevation_ft,continent,iso_country,iso_region,municipality,gps_code,iata_code,local_code,coordinates\n",
      "00A,heliport,Total Rf Heliport,11,NA,US,US-PA,Bensalem,00A,,00A,\"40.07080078125, -74.93360137939453\"\n",
      "00AA,small_airport,Aero B Ranch Airport,3435,NA,US,US-KS,Leoti,00AA,,00AA,\"38.704022, -101.473911\"\n"
     ]
    }
   ],
   "source": [
    "rdd.take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим `case class` для парсинга данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Airport\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Airport(\n",
    "    ident: String,\n",
    "    `type`: String,\n",
    "    name: String,\n",
    "    elevationFt: String,\n",
    "    continent: String,\n",
    "    isoCountry: String,\n",
    "    isoRegion: String,\n",
    "    municipality: String,\n",
    "    gpsCode: String,\n",
    "    iataCode: String,\n",
    "    localCode: String,\n",
    "    longitude: String,\n",
    "    latitude: String\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберем шапку и ненужные кавычки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firstElem = ident,type,name,elevation_ft,continent,iso_country,iso_region,municipality,gps_code,iata_code,local_code,coordinates\n",
       "noHeader = MapPartitionsRDD[38] at map at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "00A,heliport,Total Rf Heliport,11,NA,US,US-PA,Bensalem,00A,,00A,40.07080078125, -74.93360137939453"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstElem = rdd.first\n",
    "\n",
    "val noHeader = rdd.filter(x => x != firstElem).map(x => x.replaceAll(\"\\\"\", \"\"))\n",
    "noHeader.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая преобразует `RDD[String] => RDD[Airport]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toAirport: (data: String)Airport\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def toAirport(data: String): Airport = {\n",
    "    val airportArr: Array[String] = data.split(\",\").map(_.trim)\n",
    "    val Array(\n",
    "        ident, \n",
    "        aType, \n",
    "        name, \n",
    "        elevationFt, \n",
    "        continent, \n",
    "        isoCountry, \n",
    "        isoRegion, \n",
    "        municipality, \n",
    "        gpsCode, \n",
    "        iataCode, \n",
    "        localCode, \n",
    "        longitude,\n",
    "        latitude) = airportArr\n",
    "    \n",
    "    Airport(\n",
    "        ident = ident,\n",
    "        `type` = aType,\n",
    "        name = name,\n",
    "        elevationFt = elevationFt,\n",
    "        continent = continent,\n",
    "        isoCountry = isoCountry,\n",
    "        isoRegion = isoRegion,\n",
    "        municipality = municipality,\n",
    "        gpsCode = gpsCode,\n",
    "        iataCode = iataCode,\n",
    "        localCode = localCode,\n",
    "        longitude = longitude,\n",
    "        latitude = latitude\n",
    "    )\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним преобразование RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airportRdd = MapPartitionsRDD[39] at map at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[39] at map at <console>:32"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airportRdd: RDD[Airport] = noHeader.map(x => toAirport(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку любые трансформации являются ленивыми, отсутствие ошибок при выполнении предыдущей ячейки еще не означает, что данная функция отрабатывает корректно на всем датасете. Проверим это с помощью операции `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 1 in stage 47.0 failed 4 times, most recent failure: Lost task 1.3 in stage 47.0 (TID 72, spark-node-1.newprolab.com, executor 2): scala.MatchError: [Ljava.lang.String;@1ca02702 (of class [Ljava.lang.String;)\n",
       "\tat $line524.$read$$iw$$iw$$iw$$iw$$iw$$iw.toAirport(<console>:43)\n",
       "\tat $line525.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:32)\n",
       "\tat $line525.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:32)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
       "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1819)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat toAirport(<console>:43)\n",
       "\tat $anonfun$1.apply(<console>:32)\n",
       "\tat $anonfun$1.apply(<console>:32)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
       "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1819)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD.count(RDD.scala:1213)\n",
       "  ... 42 elided\n",
       "Caused by: scala.MatchError: [Ljava.lang.String;@1ca02702 (of class [Ljava.lang.String;)\n",
       "  at toAirport(<console>:43)\n",
       "  at $anonfun$1.apply(<console>:32)\n",
       "  at $anonfun$1.apply(<console>:32)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
       "  at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1819)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportRdd.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что произошло? `count`, как и любой action, запускает вычисление всех элементов в RDD. Если посмотреть стектрейс, мы увидим причину возникновения ошибки: \n",
    "\n",
    "`Caused by: scala.MatchError: [Ljava.lang.String;@42ee14f9 (of class [Ljava.lang.String;)`\n",
    "\n",
    "Это означает, что размер массива, полученного после операции `split`, меньше количества переменных, которые мы указали в данной операции:\n",
    "\n",
    "```scala\n",
    "val Array(\n",
    "        ident, \n",
    "        aType, \n",
    "        name, \n",
    "        elevationFt, \n",
    "        continent, \n",
    "        isoCountry, \n",
    "        isoRegion, \n",
    "        municipality, \n",
    "        gpsCode, \n",
    "        iataCode, \n",
    "        localCode, \n",
    "        longitude,\n",
    "        latitude) = airportArr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим код функции `toAirport`, чтобы решить данную проблему. Для простоты будем считать, что если в строке недостаточно элементов, то эту строку следует выкинуть (сделать `None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toAirportOpt: (data: String)Option[Airport]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def toAirportOpt(data: String): Option[Airport] = {\n",
    "    val airportArr: Array[String] = data.split(\",\", -1)\n",
    "    \n",
    "    airportArr match {\n",
    "        case Array(\n",
    "            ident, \n",
    "            aType, \n",
    "            name, \n",
    "            elevationFt, \n",
    "            continent, \n",
    "            isoCountry, \n",
    "            isoRegion, \n",
    "            municipality, \n",
    "            gpsCode, \n",
    "            iataCode, \n",
    "            localCode, \n",
    "            longitude,\n",
    "            latitude) => {\n",
    "        \n",
    "                Some(\n",
    "                    Airport(\n",
    "                        ident = ident,\n",
    "                        `type` = aType,\n",
    "                        name = name,\n",
    "                        elevationFt = elevationFt,\n",
    "                        continent = continent,\n",
    "                        isoCountry = isoCountry,\n",
    "                        isoRegion = isoRegion,\n",
    "                        municipality = municipality,\n",
    "                        gpsCode = gpsCode,\n",
    "                        iataCode = iataCode,\n",
    "                        localCode = localCode,\n",
    "                        longitude = longitude,\n",
    "                        latitude = latitude\n",
    "                        )\n",
    "                    )\n",
    "        }\n",
    "        case _ => Option.empty[Airport]\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим новую функцию к RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airportOptRdd = MapPartitionsRDD[40] at map at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[40] at map at <console>:32"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airportOptRdd: RDD[Option[Airport]] = noHeader.map(toAirportOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим корректность выполнения функции на первых трех элементах и на всем датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: ClassNotFound with classloader: scala.reflect.internal.util.ScalaClassLoader$URLClassLoader@524f9610\n",
       "StackTrace:   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1409)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
       "  at org.apache.spark.rdd.RDD.take(RDD.scala:1382)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportOptRdd.take(3).foreach(println)\n",
    "airportOptRdd.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airportRdd = MapPartitionsRDD[41] at flatMap at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[41] at flatMap at <console>:32"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airportRdd: RDD[Airport] = noHeader.flatMap(toAirportOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54944"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airportRdd.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим корректную обработку числовых типов в наш код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class AirportTyped\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class AirportTyped(\n",
    "    ident: String,\n",
    "    `type`: String,\n",
    "    name: String,\n",
    "    elevationFt: Int,\n",
    "    continent: String,\n",
    "    isoCountry: String,\n",
    "    isoRegion: String,\n",
    "    municipality: String,\n",
    "    gpsCode: String,\n",
    "    iataCode: String,\n",
    "    localCode: String,\n",
    "    longitude: Float,\n",
    "    latitude: Float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toAirportOptTyped: (data: String)Option[AirportTyped]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def toAirportOptTyped(data: String): Option[AirportTyped] = {\n",
    "    val airportArr: Array[String] = data.split(\",\", -1)\n",
    "    \n",
    "    airportArr match {\n",
    "        case Array(\n",
    "            ident, \n",
    "            aType, \n",
    "            name, \n",
    "            elevationFt, \n",
    "            continent, \n",
    "            isoCountry, \n",
    "            isoRegion, \n",
    "            municipality, \n",
    "            gpsCode, \n",
    "            iataCode, \n",
    "            localCode, \n",
    "            longitude,\n",
    "            latitude) => {\n",
    "        \n",
    "                Some(\n",
    "                    AirportTyped(\n",
    "                        ident = ident,\n",
    "                        `type` = aType,\n",
    "                        name = name,\n",
    "                        elevationFt = elevationFt.toInt,\n",
    "                        continent = continent,\n",
    "                        isoCountry = isoCountry,\n",
    "                        isoRegion = isoRegion,\n",
    "                        municipality = municipality,\n",
    "                        gpsCode = gpsCode,\n",
    "                        iataCode = iataCode,\n",
    "                        localCode = localCode,\n",
    "                        longitude = longitude.toFloat,\n",
    "                        latitude = latitude.toFloat\n",
    "                        )\n",
    "                    )\n",
    "        }\n",
    "        case _ => Option.empty[AirportTyped]\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 50.0 failed 4 times, most recent failure: Lost task 0.3 in stage 50.0 (TID 82, spark-node-1.newprolab.com, executor 2): java.lang.NumberFormatException: For input string: \"\"\n",
       "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:592)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
       "\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)\n",
       "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n",
       "\tat $line555.$read$$iw$$iw$$iw$$iw$$iw$$iw.toAirportOptTyped(<console>:52)\n",
       "\tat $line556.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:32)\n",
       "\tat $line556.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:32)\n",
       "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
       "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
       "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:592)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
       "\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)\n",
       "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n",
       "\tat toAirportOptTyped(<console>:52)\n",
       "\tat $anonfun$1.apply(<console>:32)\n",
       "\tat $anonfun$1.apply(<console>:32)\n",
       "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
       "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
       "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD.count(RDD.scala:1213)\n",
       "  ... 42 elided\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"\"\n",
       "  at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "  at java.lang.Integer.parseInt(Integer.java:592)\n",
       "  at java.lang.Integer.parseInt(Integer.java:615)\n",
       "  at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:273)\n",
       "  at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n",
       "  at toAirportOptTyped(<console>:52)\n",
       "  at $anonfun$1.apply(<console>:32)\n",
       "  at $anonfun$1.apply(<console>:32)\n",
       "  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
       "  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
       "  at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1213)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airportRddTyped: RDD[AirportTyped] = noHeader.flatMap(toAirportOptTyped)\n",
    "airportRddTyped.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас новая ошибка:\n",
    "\n",
    "`java.lang.NumberFormatException: For input string: \"\"`\n",
    "\n",
    "Она возникает, когда мы пытаемся превратить пустую строку в Int с помощью метода `.toInt`\n",
    "Для решения этой задачи мы можем использовать монаду `Try[T]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class AirportSafe\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "toAirportOptSafe: (data: String)Option[AirportSafe]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.util.Try\n",
    "\n",
    "case class AirportSafe(\n",
    "    ident: String,\n",
    "    `type`: String,\n",
    "    name: String,\n",
    "    elevationFt: Option[Int],\n",
    "    continent: String,\n",
    "    isoCountry: String,\n",
    "    isoRegion: String,\n",
    "    municipality: String,\n",
    "    gpsCode: String,\n",
    "    iataCode: String,\n",
    "    localCode: String,\n",
    "    longitude: Option[Float],\n",
    "    latitude: Option[Float]\n",
    ")\n",
    "\n",
    "def toAirportOptSafe(data: String): Option[AirportSafe] = {\n",
    "    val airportArr: Array[String] = data.split(\",\", -1)\n",
    "    \n",
    "    airportArr match {\n",
    "        case Array(\n",
    "            ident, \n",
    "            aType, \n",
    "            name, \n",
    "            elevationFt, \n",
    "            continent, \n",
    "            isoCountry, \n",
    "            isoRegion, \n",
    "            municipality, \n",
    "            gpsCode, \n",
    "            iataCode, \n",
    "            localCode, \n",
    "            longitude,\n",
    "            latitude) => {\n",
    "        \n",
    "                Some(\n",
    "                    AirportSafe(\n",
    "                        ident = ident,\n",
    "                        `type` = aType,\n",
    "                        name = name,\n",
    "                        elevationFt = Try(elevationFt.toInt).toOption,\n",
    "                        continent = continent,\n",
    "                        isoCountry = isoCountry,\n",
    "                        isoRegion = isoRegion,\n",
    "                        municipality = municipality,\n",
    "                        gpsCode = gpsCode,\n",
    "                        iataCode = iataCode,\n",
    "                        localCode = localCode,\n",
    "                        longitude = Try(longitude.toFloat).toOption,\n",
    "                        latitude = Try(latitude.toFloat).toOption\n",
    "                        )\n",
    "                    )\n",
    "        }\n",
    "        case _ => Option.empty[AirportSafe]\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим данную функцию к нашему датасету:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airportSafeRdd = MapPartitionsRDD[43] at map at <console>:33\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[43] at map at <console>:33"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airportSafeRdd: RDD[Option[AirportSafe]] = noHeader.map(toAirportOptSafe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим ее применимость:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "55113"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//airportSafeRdd.take(3).foreach(println)\n",
    "airportSafeRdd.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Option[T]` - это удобная монада, которая позволяет работать с отсутствующими данными, избегая исключительных ситуаций и обработки `null`. Одним из ее преимуществ является то, что ее можно рассматривать как коллекцию, что позволяет применить к `RDD[Option[T]]` метод `flatMap`, который вернет RDD[T], убрав все `None` из нашего датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airportFinal = MapPartitionsRDD[45] at flatMap at <console>:33\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[45] at flatMap at <console>:33"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airportFinal: RDD[AirportSafe] = noHeader.flatMap(toAirportOptSafe)\n",
    "//airportFinal.take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим коллекцию, содержащую максимальную высота аэропорта с разбивкой по странам. Для этого первым шагом получим `PairRDD`: `RDD[(K,V)]`, где `K` - это страна, а `V` - высота"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairAirport = MapPartitionsRDD[47] at map at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array((US,Some(11)), (US,Some(3435)))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairAirport = airportFinal.map(x => (x.isoCountry, x.elevationFt))\n",
    "pairAirport.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку мы не можем напрямую сравнивать два объекта `Option[T]`, то нам необходимо получить `T`. Будем считать, что аэропорты, где атрибут `elevationFt` принимает значение `None`, необходимо поместить в конец нашего списка. Для этого применим функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixedElevation = MapPartitionsRDD[48] at map at <console>:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(US,11)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fixedElevation: RDD[(String, Int)] = pairAirport.map {\n",
    "    case (k, Some(v)) => (k, v)\n",
    "    case (k, None) => (k, Int.MinValue)\n",
    "}\n",
    "fixedElevation.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам необходимо применить функцию reduceByKey и получить нужный результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(IN,22000)\n",
      "(PE,14965)\n",
      "(CN,14472)\n",
      "(BO,14360)\n",
      "(CO,13119)\n",
      "(AR,13000)\n",
      "(CL,12468)\n",
      "(US,12442)\n",
      "(NP,12400)\n",
      "(TJ,11962)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = Array((IN,22000), (PE,14965), (CN,14472), (BO,14360), (CO,13119), (AR,13000), (CL,12468), (US,12442), (NP,12400), (TJ,11962), (FR,11647), (CH,10837), (AF,10490), (LS,10400), (KE,10200), (EC,9649), (AQ,9300), (ID,9288), (MX,9121), (BT,9000), (ET,8490), (PG,8400), (KG,8250), (GT,7933), (TZ,7795), (MW,7759), (ER,7661), (AT,7522), (IR,7385), (PK,7316), (MN,7260), (YE,7216), (SA,6858), (BR,6825), (RU,6695), (CD,6562), (OM,6500), (ZA,6464), (TR,6400), (UG,6200), (RW,6102), (NA,6063), (KZ,6051), (MM,6000), (IT,5938), (AO,5778), (BI,5741), (SO,5720), (AU,5689), (HN,5475), (MA,5459), (ZM,5454), (ZW,5370), (CA,5350), (VE,5269), (AM,5000), (PA,5000), (MG,4997), (VN,4937), (KR,4816), (GE,4778), (CR,4650), (CM,4593), (KP,4547), (DZ,4518), (MZ,4505...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array((IN,22000), (PE,14965), (CN,14472), (BO,14360), (CO,13119), (AR,13000), (CL,12468), (US,12442), (NP,12400), (TJ,11962), (FR,11647), (CH,10837), (AF,10490), (LS,10400), (KE,10200), (EC,9649), (AQ,9300), (ID,9288), (MX,9121), (BT,9000), (ET,8490), (PG,8400), (KG,8250), (GT,7933), (TZ,7795), (MW,7759), (ER,7661), (AT,7522), (IR,7385), (PK,7316), (MN,7260), (YE,7216), (SA,6858), (BR,6825), (RU,6695), (CD,6562), (OM,6500), (ZA,6464), (TR,6400), (UG,6200), (RW,6102), (NA,6063), (KZ,6051), (MM,6000), (IT,5938), (AO,5778), (BI,5741), (SO,5720), (AU,5689), (HN,5475), (MA,5459), (ZM,5454), (ZW,5370), (CA,5350), (VE,5269), (AM,5000), (PA,5000), (MG,4997), (VN,4937), (KR,4816), (GE,4778), (CR,4650), (CM,4593), (KP,4547), (DZ,4518), (MZ,4505..."
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.math.max\n",
    "\n",
    "val result = fixedElevation.reduceByKey { (x, y) => Math.max(x,y) }.collect.sortBy( x => -x._2 )\n",
    "result.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "- RDD API - это низкоуровневый API, который позволяет применять любые функции к распределенным данным\n",
    "- При использовании RDD API обработка всех исключительных ситуаций лежит на плечах разработчика\n",
    "\n",
    "После завершения работы не забывайте останавливать `SparkSession`, чтобы освободить ресурсы кластера!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
