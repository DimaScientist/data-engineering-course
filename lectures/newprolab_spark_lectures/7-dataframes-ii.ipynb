{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Dataframes II\n",
    "**Сергей Гришаев**  \n",
    "serg.grishaev@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Планы выполнения задач\n",
    "+ Оптимизация соединений и группировок\n",
    "+ Управление схемой данных\n",
    "+ Оптимизатор запросов Catalyst\n",
    "\n",
    "## Планы выполнения задач\n",
    "\n",
    "Любой `job` в Spark SQL имеет под собой план выполнения, кототорый генерируется на основе написанно запроса. План запроса содержит операторы, которые затем превращаются в Java код. Поскольку одну и ту же задачу в Spark SQL можно выполнить по-разному, полезно смотреть в планы выполнения, чтобы, например:\n",
    "+ убрать лишние shuffle\n",
    "+ убедиться, чтот тот или иной оператор будет выполнен на уровне источника, а не внутри Spark\n",
    "+ понять, как будет выполнен `join`\n",
    "\n",
    "Планы выполнения доступны в двух видах:\n",
    "+ метод `explain()` у DF\n",
    "+ на вкладке SQL в Spark UI\n",
    "\n",
    "Прочитаем датасет [Airport Codes](https://datahub.io/core/airport-codes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | 40.07080078125, -74.93360137939453 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> true)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем метод `explain`, чтобы посмотреть план запроса. Наиболее интересным является физический план, т.к. он отражает фактически алгоритм обработки данных. В данном случае в плане присутствует единственный оператор `FileScan csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan csv [ident#10,type#11,name#12,elevation_ft#13,continent#14,iso_country#15,iso_region#16,municipality#17,gps_code#18,iata_code#19,local_code#20,coordinates#21] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n"
     ]
    }
   ],
   "source": [
    "airports.explain(extended = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55113"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если остальные планы не нужны, можно показать только физический:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printPhysicalPlan: [_](ds: org.apache.spark.sql.Dataset[_])Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "\n",
    "airports.queryExecution.executedPlan.treeString\n",
    "\n",
    "def printPhysicalPlan[_](ds: Dataset[_]): Unit = {\n",
    "    println(ds.queryExecution.executedPlan.treeString)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также есть возмжность получить эту информацию в виде JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.FileSourceScanExec\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"ident\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":10,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"type\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":11,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":12,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"elevation_ft\",\"dataType\":\"integer\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":13,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"continent\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":14,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"iso_country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":15,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"iso_region\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"municipality\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"gps_code\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"iata_code\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":19,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"local_code\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":20,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"coordinates\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":21,\"jvmId\":\"389ceb62-1324-4f94-bb7f-28ab9bd35cb4\"},\"qualifier\":[]}]],\"requiredSchema\":{\"type\":\"struct\",\"fields\":[{\"name\":\"ident\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"elevation_ft\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"continent\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"iso_country\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"iso_region\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"municipality\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"gps_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"iata_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"local_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"coordinates\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"partitionFilters\":[],\"dataFilters\":[]}]\n"
     ]
    }
   ],
   "source": [
    "println(airports.queryExecution.executedPlan.toJSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним `filter` и проверим план выполнения. Читать план нужно снизу вверх. В плане появился новый оператор `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(1) Project [ident#10, type#11, name#12, elevation_ft#13, continent#14, iso_country#15, iso_region#16, municipality#17, gps_code#18, iata_code#19, local_code#20, coordinates#21]\n",
      "+- *(1) Filter (isnotnull(type#11) && (type#11 = small_airport))\n",
      "   +- *(1) FileScan csv [ident#10,type#11,name#12,elevation_ft#13,continent#14,iso_country#15,iso_region#16,municipality#17,gps_code#18,iata_code#19,local_code#20,coordinates#21] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(type), EqualTo(type,small_airport)], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printPhysicalPlan(airports.filter('type === \"small_airport\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('type = small_airport)\n",
      "+- Relation[ident#10,type#11,name#12,elevation_ft#13,continent#14,iso_country#15,iso_region#16,municipality#17,gps_code#18,iata_code#19,local_code#20,coordinates#21] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ident: string, type: string, name: string, elevation_ft: int, continent: string, iso_country: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string\n",
      "Filter (type#11 = small_airport)\n",
      "+- Relation[ident#10,type#11,name#12,elevation_ft#13,continent#14,iso_country#15,iso_region#16,municipality#17,gps_code#18,iata_code#19,local_code#20,coordinates#21] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(type#11) && (type#11 = small_airport))\n",
      "+- Relation[ident#10,type#11,name#12,elevation_ft#13,continent#14,iso_country#15,iso_region#16,municipality#17,gps_code#18,iata_code#19,local_code#20,coordinates#21] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ident#10, type#11, name#12, elevation_ft#13, continent#14, iso_country#15, iso_region#16, municipality#17, gps_code#18, iata_code#19, local_code#20, coordinates#21]\n",
      "+- *(1) Filter (isnotnull(type#11) && (type#11 = small_airport))\n",
      "   +- *(1) FileScan csv [ident#10,type#11,name#12,elevation_ft#13,continent#14,iso_country#15,iso_region#16,municipality#17,gps_code#18,iata_code#19,local_code#20,coordinates#21] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(type), EqualTo(type,small_airport)], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n"
     ]
    }
   ],
   "source": [
    "airports.filter('type === \"small_airport\").explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним агрегацию и проверим план выполнения. В нем появляется три оператора: 2 `HashAggregate` и `Exchange hashpartitioning`.\n",
    "\n",
    "Первый `HashAggregate` содержит функцию `partial_count(1)`. Это означает, что внутри каждого воркера произойдет подсчет строк по каждому ключу. Затем происходит `shuffle` по ключу агрегата, после которого выполняется еще один `HashAggregate` с функцией `count(1)`. Использование двух `HashAggregate` позволяет сократить количество передаваемых данных по сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(2) HashAggregate(keys=[iso_country#15], functions=[count(1)], output=[iso_country#15, count#116L])\n",
      "+- Exchange hashpartitioning(iso_country#15, 200)\n",
      "   +- *(1) HashAggregate(keys=[iso_country#15], functions=[partial_count(1)], output=[iso_country#15, count#120L])\n",
      "      +- *(1) Project [iso_country#15]\n",
      "         +- *(1) Filter (isnotnull(type#11) && (type#11 = small_airport))\n",
      "            +- *(1) FileScan csv [type#11,iso_country#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(type), EqualTo(type,small_airport)], ReadSchema: struct<type:string,iso_country:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printPhysicalPlan(airports.filter('type === \"small_airport\").groupBy('iso_country).count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([DZ,22], [LT,45], [MM,52], [CI,19], [TC,4], [AZ,21], [FI,61], [SC,13], [UA,102], [ZM,94], [RO,32], [KI,17], [SL,4], [SB,32], [NL,26], [LA,11], [BS,30], [BW,113], [MN,19], [AM,5], [PL,205], [PS,1], [MK,11], [MX,845], [PF,27], [GL,18], [EE,18], [VG,2], [SM,1], [CN,93], [UM,1], [AT,47], [RU,501], [NA,231], [IQ,42], [CG,47], [HR,28], [SV,25], [CZ,197], [NP,41], [SO,24], [PT,106], [PG,506], [GH,5], [CV,4], [BN,1], [LR,14], [TW,10], [BD,6], [PY,64], [CL,338], [TO,3], [ID,395], [FK,32], [LY,46], [SA,40], [AU,1538], [PK,80], [CA,997], [MW,21], [NE,18], [UZ,167], [GB,572], [YE,14], [BR,3102], [KZ,86], [BY,27], [HN,140], [NC,11], [GT,44], [MD,2], [DE,746], [GN,15], [EC,99], [ES,296], [IR,71], [BH,1], [IL,19], [MR,18], [TR,56], [ME,3], [VE,456], [ZA,4..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.filter('type === \"small_airport\").groupBy('iso_country).count.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При необходимости мы можем почитать ~~перед сном~~ сгенерированный ~~теплый ламповый~~ java код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 WholeStageCodegen subtrees.\n",
      "== Subtree 1 / 2 ==\n",
      "*(1) HashAggregate(keys=[iso_country#15], functions=[partial_count(1)], output=[iso_country#15, count#160L])\n",
      "+- *(1) Project [iso_country#15]\n",
      "   +- *(1) Filter (isnotnull(type#11) && (type#11 = small_airport))\n",
      "      +- *(1) FileScan csv [type#11,iso_country#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(type), EqualTo(type,small_airport)], ReadSchema: struct<type:string,iso_country:string>\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean agg_initAgg_0;\n",
      "/* 010 */   private boolean agg_bufIsNull_0;\n",
      "/* 011 */   private long agg_bufValue_0;\n",
      "/* 012 */   private agg_FastHashMap_0 agg_fastHashMap_0;\n",
      "/* 013 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> agg_fastHashMapIter_0;\n",
      "/* 014 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;\n",
      "/* 015 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;\n",
      "/* 016 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;\n",
      "/* 017 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[5];\n",
      "/* 018 */   private scala.collection.Iterator[] scan_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 019 */\n",
      "/* 020 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 021 */     this.references = references;\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 025 */     partitionIndex = index;\n",
      "/* 026 */     this.inputs = inputs;\n",
      "/* 027 */\n",
      "/* 028 */     scan_mutableStateArray_0[0] = inputs[0];\n",
      "/* 029 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);\n",
      "/* 030 */     filter_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 031 */     filter_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 032 */     filter_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 033 */     filter_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 034 */\n",
      "/* 035 */   }\n",
      "/* 036 */\n",
      "/* 037 */   public class agg_FastHashMap_0 {\n",
      "/* 038 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;\n",
      "/* 039 */     private int[] buckets;\n",
      "/* 040 */     private int capacity = 1 << 16;\n",
      "/* 041 */     private double loadFactor = 0.5;\n",
      "/* 042 */     private int numBuckets = (int) (capacity / loadFactor);\n",
      "/* 043 */     private int maxSteps = 2;\n",
      "/* 044 */     private int numRows = 0;\n",
      "/* 045 */     private Object emptyVBase;\n",
      "/* 046 */     private long emptyVOff;\n",
      "/* 047 */     private int emptyVLen;\n",
      "/* 048 */     private boolean isBatchFull = false;\n",
      "/* 049 */\n",
      "/* 050 */     public agg_FastHashMap_0(\n",
      "/* 051 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,\n",
      "/* 052 */       InternalRow emptyAggregationBuffer) {\n",
      "/* 053 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch\n",
      "/* 054 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);\n",
      "/* 055 */\n",
      "/* 056 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));\n",
      "/* 057 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();\n",
      "/* 058 */\n",
      "/* 059 */       emptyVBase = emptyBuffer;\n",
      "/* 060 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;\n",
      "/* 061 */       emptyVLen = emptyBuffer.length;\n",
      "/* 062 */\n",
      "/* 063 */       buckets = new int[numBuckets];\n",
      "/* 064 */       java.util.Arrays.fill(buckets, -1);\n",
      "/* 065 */     }\n",
      "/* 066 */\n",
      "/* 067 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String agg_key_0) {\n",
      "/* 068 */       long h = hash(agg_key_0);\n",
      "/* 069 */       int step = 0;\n",
      "/* 070 */       int idx = (int) h & (numBuckets - 1);\n",
      "/* 071 */       while (step < maxSteps) {\n",
      "/* 072 */         // Return bucket index if it's either an empty slot or already contains the key\n",
      "/* 073 */         if (buckets[idx] == -1) {\n",
      "/* 074 */           if (numRows < capacity && !isBatchFull) {\n",
      "/* 075 */             // creating the unsafe for new entry\n",
      "/* 076 */             org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter\n",
      "/* 077 */             = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(\n",
      "/* 078 */               1, 32);\n",
      "/* 079 */             agg_rowWriter.reset(); //TODO: investigate if reset or zeroout are actually needed\n",
      "/* 080 */             agg_rowWriter.zeroOutNullBytes();\n",
      "/* 081 */             agg_rowWriter.write(0, agg_key_0);\n",
      "/* 082 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result\n",
      "/* 083 */             = agg_rowWriter.getRow();\n",
      "/* 084 */             Object kbase = agg_result.getBaseObject();\n",
      "/* 085 */             long koff = agg_result.getBaseOffset();\n",
      "/* 086 */             int klen = agg_result.getSizeInBytes();\n",
      "/* 087 */\n",
      "/* 088 */             UnsafeRow vRow\n",
      "/* 089 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);\n",
      "/* 090 */             if (vRow == null) {\n",
      "/* 091 */               isBatchFull = true;\n",
      "/* 092 */             } else {\n",
      "/* 093 */               buckets[idx] = numRows++;\n",
      "/* 094 */             }\n",
      "/* 095 */             return vRow;\n",
      "/* 096 */           } else {\n",
      "/* 097 */             // No more space\n",
      "/* 098 */             return null;\n",
      "/* 099 */           }\n",
      "/* 100 */         } else if (equals(idx, agg_key_0)) {\n",
      "/* 101 */           return batch.getValueRow(buckets[idx]);\n",
      "/* 102 */         }\n",
      "/* 103 */         idx = (idx + 1) & (numBuckets - 1);\n",
      "/* 104 */         step++;\n",
      "/* 105 */       }\n",
      "/* 106 */       // Didn't find it\n",
      "/* 107 */       return null;\n",
      "/* 108 */     }\n",
      "/* 109 */\n",
      "/* 110 */     private boolean equals(int idx, UTF8String agg_key_0) {\n",
      "/* 111 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);\n",
      "/* 112 */       return (row.getUTF8String(0).equals(agg_key_0));\n",
      "/* 113 */     }\n",
      "/* 114 */\n",
      "/* 115 */     private long hash(UTF8String agg_key_0) {\n",
      "/* 116 */       long agg_hash_0 = 0;\n",
      "/* 117 */\n",
      "/* 118 */       int agg_result_0 = 0;\n",
      "/* 119 */       byte[] agg_bytes_0 = agg_key_0.getBytes();\n",
      "/* 120 */       for (int i = 0; i < agg_bytes_0.length; i++) {\n",
      "/* 121 */         int agg_hash_1 = agg_bytes_0[i];\n",
      "/* 122 */         agg_result_0 = (agg_result_0 ^ (0x9e3779b9)) + agg_hash_1 + (agg_result_0 << 6) + (agg_result_0 >>> 2);\n",
      "/* 123 */       }\n",
      "/* 124 */\n",
      "/* 125 */       agg_hash_0 = (agg_hash_0 ^ (0x9e3779b9)) + agg_result_0 + (agg_hash_0 << 6) + (agg_hash_0 >>> 2);\n",
      "/* 126 */\n",
      "/* 127 */       return agg_hash_0;\n",
      "/* 128 */     }\n",
      "/* 129 */\n",
      "/* 130 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {\n",
      "/* 131 */       return batch.rowIterator();\n",
      "/* 132 */     }\n",
      "/* 133 */\n",
      "/* 134 */     public void close() {\n",
      "/* 135 */       batch.close();\n",
      "/* 136 */     }\n",
      "/* 137 */\n",
      "/* 138 */   }\n",
      "/* 139 */\n",
      "/* 140 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)\n",
      "/* 141 */   throws java.io.IOException {\n",
      "/* 142 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);\n",
      "/* 143 */\n",
      "/* 144 */     boolean agg_isNull_12 = agg_keyTerm_0.isNullAt(0);\n",
      "/* 145 */     UTF8String agg_value_13 = agg_isNull_12 ?\n",
      "/* 146 */     null : (agg_keyTerm_0.getUTF8String(0));\n",
      "/* 147 */     long agg_value_14 = agg_bufferTerm_0.getLong(0);\n",
      "/* 148 */\n",
      "/* 149 */     filter_mutableStateArray_0[4].reset();\n",
      "/* 150 */\n",
      "/* 151 */     filter_mutableStateArray_0[4].zeroOutNullBytes();\n",
      "/* 152 */\n",
      "/* 153 */     if (agg_isNull_12) {\n",
      "/* 154 */       filter_mutableStateArray_0[4].setNullAt(0);\n",
      "/* 155 */     } else {\n",
      "/* 156 */       filter_mutableStateArray_0[4].write(0, agg_value_13);\n",
      "/* 157 */     }\n",
      "/* 158 */\n",
      "/* 159 */     filter_mutableStateArray_0[4].write(1, agg_value_14);\n",
      "/* 160 */     append((filter_mutableStateArray_0[4].getRow()));\n",
      "/* 161 */\n",
      "/* 162 */   }\n",
      "/* 163 */\n",
      "/* 164 */   private void agg_doConsume_0(UTF8String agg_expr_0_0, boolean agg_exprIsNull_0_0) throws java.io.IOException {\n",
      "/* 165 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;\n",
      "/* 166 */     UnsafeRow agg_fastAggBuffer_0 = null;\n",
      "/* 167 */\n",
      "/* 168 */     if (true) {\n",
      "/* 169 */       if (!agg_exprIsNull_0_0) {\n",
      "/* 170 */         agg_fastAggBuffer_0 = agg_fastHashMap_0.findOrInsert(\n",
      "/* 171 */           agg_expr_0_0);\n",
      "/* 172 */       }\n",
      "/* 173 */     }\n",
      "/* 174 */     // Cannot find the key in fast hash map, try regular hash map.\n",
      "/* 175 */     if (agg_fastAggBuffer_0 == null) {\n",
      "/* 176 */       // generate grouping key\n",
      "/* 177 */       filter_mutableStateArray_0[3].reset();\n",
      "/* 178 */\n",
      "/* 179 */       filter_mutableStateArray_0[3].zeroOutNullBytes();\n",
      "/* 180 */\n",
      "/* 181 */       if (agg_exprIsNull_0_0) {\n",
      "/* 182 */         filter_mutableStateArray_0[3].setNullAt(0);\n",
      "/* 183 */       } else {\n",
      "/* 184 */         filter_mutableStateArray_0[3].write(0, agg_expr_0_0);\n",
      "/* 185 */       }\n",
      "/* 186 */       int agg_value_5 = 48;\n",
      "/* 187 */\n",
      "/* 188 */       if (!agg_exprIsNull_0_0) {\n",
      "/* 189 */         agg_value_5 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_0_0.getBaseObject(), agg_expr_0_0.getBaseOffset(), agg_expr_0_0.numBytes(), agg_value_5);\n",
      "/* 190 */       }\n",
      "/* 191 */       if (true) {\n",
      "/* 192 */         // try to get the buffer from hash map\n",
      "/* 193 */         agg_unsafeRowAggBuffer_0 =\n",
      "/* 194 */         agg_hashMap_0.getAggregationBufferFromUnsafeRow((filter_mutableStateArray_0[3].getRow()), agg_value_5);\n",
      "/* 195 */       }\n",
      "/* 196 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 197 */       // aggregation after processing all input rows.\n",
      "/* 198 */       if (agg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 199 */         if (agg_sorter_0 == null) {\n",
      "/* 200 */           agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 201 */         } else {\n",
      "/* 202 */           agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 203 */         }\n",
      "/* 204 */\n",
      "/* 205 */         // the hash map had be spilled, it should have enough memory now,\n",
      "/* 206 */         // try to allocate buffer again.\n",
      "/* 207 */         agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 208 */           (filter_mutableStateArray_0[3].getRow()), agg_value_5);\n",
      "/* 209 */         if (agg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 210 */           // failed to allocate the first page\n",
      "/* 211 */           throw new OutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 212 */         }\n",
      "/* 213 */       }\n",
      "/* 214 */\n",
      "/* 215 */     }\n",
      "/* 216 */\n",
      "/* 217 */     if (agg_fastAggBuffer_0 != null) {\n",
      "/* 218 */       // common sub-expressions\n",
      "/* 219 */\n",
      "/* 220 */       // evaluate aggregate function\n",
      "/* 221 */       long agg_value_11 = agg_fastAggBuffer_0.getLong(0);\n",
      "/* 222 */\n",
      "/* 223 */       long agg_value_10 = -1L;\n",
      "/* 224 */       agg_value_10 = agg_value_11 + 1L;\n",
      "/* 225 */       // update fast row\n",
      "/* 226 */       agg_fastAggBuffer_0.setLong(0, agg_value_10);\n",
      "/* 227 */     } else {\n",
      "/* 228 */       // common sub-expressions\n",
      "/* 229 */\n",
      "/* 230 */       // evaluate aggregate function\n",
      "/* 231 */       long agg_value_8 = agg_unsafeRowAggBuffer_0.getLong(0);\n",
      "/* 232 */\n",
      "/* 233 */       long agg_value_7 = -1L;\n",
      "/* 234 */       agg_value_7 = agg_value_8 + 1L;\n",
      "/* 235 */       // update unsafe row buffer\n",
      "/* 236 */       agg_unsafeRowAggBuffer_0.setLong(0, agg_value_7);\n",
      "/* 237 */\n",
      "/* 238 */     }\n",
      "/* 239 */\n",
      "/* 240 */   }\n",
      "/* 241 */\n",
      "/* 242 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 243 */     while (scan_mutableStateArray_0[0].hasNext()) {\n",
      "/* 244 */       InternalRow scan_row_0 = (InternalRow) scan_mutableStateArray_0[0].next();\n",
      "/* 245 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numOutputRows */).add(1);\n",
      "/* 246 */       do {\n",
      "/* 247 */         boolean scan_isNull_0 = scan_row_0.isNullAt(0);\n",
      "/* 248 */         UTF8String scan_value_0 = scan_isNull_0 ?\n",
      "/* 249 */         null : (scan_row_0.getUTF8String(0));\n",
      "/* 250 */\n",
      "/* 251 */         if (!(!scan_isNull_0)) continue;\n",
      "/* 252 */\n",
      "/* 253 */         boolean filter_value_2 = false;\n",
      "/* 254 */         filter_value_2 = scan_value_0.equals(((UTF8String) references[8] /* literal */));\n",
      "/* 255 */         if (!filter_value_2) continue;\n",
      "/* 256 */\n",
      "/* 257 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* numOutputRows */).add(1);\n",
      "/* 258 */\n",
      "/* 259 */         boolean scan_isNull_1 = scan_row_0.isNullAt(1);\n",
      "/* 260 */         UTF8String scan_value_1 = scan_isNull_1 ?\n",
      "/* 261 */         null : (scan_row_0.getUTF8String(1));\n",
      "/* 262 */\n",
      "/* 263 */         agg_doConsume_0(scan_value_1, scan_isNull_1);\n",
      "/* 264 */\n",
      "/* 265 */       } while(false);\n",
      "/* 266 */       if (shouldStop()) return;\n",
      "/* 267 */     }\n",
      "/* 268 */\n",
      "/* 269 */     agg_fastHashMapIter_0 = agg_fastHashMap_0.rowIterator();\n",
      "/* 270 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */));\n",
      "/* 271 */\n",
      "/* 272 */   }\n",
      "/* 273 */\n",
      "/* 274 */   protected void processNext() throws java.io.IOException {\n",
      "/* 275 */     if (!agg_initAgg_0) {\n",
      "/* 276 */       agg_initAgg_0 = true;\n",
      "/* 277 */       agg_fastHashMap_0 = new agg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());\n",
      "/* 278 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 279 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 280 */       agg_doAggregateWithKeys_0();\n",
      "/* 281 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 282 */     }\n",
      "/* 283 */\n",
      "/* 284 */     // output the result\n",
      "/* 285 */\n",
      "/* 286 */     while (agg_fastHashMapIter_0.next()) {\n",
      "/* 287 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_fastHashMapIter_0.getKey();\n",
      "/* 288 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_fastHashMapIter_0.getValue();\n",
      "/* 289 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);\n",
      "/* 290 */\n",
      "/* 291 */       if (shouldStop()) return;\n",
      "/* 292 */     }\n",
      "/* 293 */     agg_fastHashMap_0.close();\n",
      "/* 294 */\n",
      "/* 295 */     while (agg_mapIter_0.next()) {\n",
      "/* 296 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();\n",
      "/* 297 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();\n",
      "/* 298 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);\n",
      "/* 299 */\n",
      "/* 300 */       if (shouldStop()) return;\n",
      "/* 301 */     }\n",
      "/* 302 */\n",
      "/* 303 */     agg_mapIter_0.close();\n",
      "/* 304 */     if (agg_sorter_0 == null) {\n",
      "/* 305 */       agg_hashMap_0.free();\n",
      "/* 306 */     }\n",
      "/* 307 */   }\n",
      "/* 308 */\n",
      "/* 309 */ }\n",
      "\n",
      "== Subtree 2 / 2 ==\n",
      "*(2) HashAggregate(keys=[iso_country#15], functions=[count(1)], output=[iso_country#15, count#155L])\n",
      "+- Exchange hashpartitioning(iso_country#15, 200)\n",
      "   +- *(1) HashAggregate(keys=[iso_country#15], functions=[partial_count(1)], output=[iso_country#15, count#160L])\n",
      "      +- *(1) Project [iso_country#15]\n",
      "         +- *(1) Filter (isnotnull(type#11) && (type#11 = small_airport))\n",
      "            +- *(1) FileScan csv [type#11,iso_country#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(type), EqualTo(type,small_airport)], ReadSchema: struct<type:string,iso_country:string>\n",
      "\n",
      "Generated code:\n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=2\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private boolean agg_initAgg_0;\n",
      "/* 010 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;\n",
      "/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;\n",
      "/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;\n",
      "/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 015 */\n",
      "/* 016 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n",
      "/* 017 */     this.references = references;\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 021 */     partitionIndex = index;\n",
      "/* 022 */     this.inputs = inputs;\n",
      "/* 023 */\n",
      "/* 024 */     inputadapter_input_0 = inputs[0];\n",
      "/* 025 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 026 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);\n",
      "/* 027 */\n",
      "/* 028 */   }\n",
      "/* 029 */\n",
      "/* 030 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)\n",
      "/* 031 */   throws java.io.IOException {\n",
      "/* 032 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);\n",
      "/* 033 */\n",
      "/* 034 */     boolean agg_isNull_7 = agg_keyTerm_0.isNullAt(0);\n",
      "/* 035 */     UTF8String agg_value_7 = agg_isNull_7 ?\n",
      "/* 036 */     null : (agg_keyTerm_0.getUTF8String(0));\n",
      "/* 037 */     long agg_value_8 = agg_bufferTerm_0.getLong(0);\n",
      "/* 038 */\n",
      "/* 039 */     agg_mutableStateArray_0[1].reset();\n",
      "/* 040 */\n",
      "/* 041 */     agg_mutableStateArray_0[1].zeroOutNullBytes();\n",
      "/* 042 */\n",
      "/* 043 */     if (agg_isNull_7) {\n",
      "/* 044 */       agg_mutableStateArray_0[1].setNullAt(0);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       agg_mutableStateArray_0[1].write(0, agg_value_7);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     agg_mutableStateArray_0[1].write(1, agg_value_8);\n",
      "/* 050 */     append((agg_mutableStateArray_0[1].getRow()));\n",
      "/* 051 */\n",
      "/* 052 */   }\n",
      "/* 053 */\n",
      "/* 054 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, UTF8String agg_expr_0_0, boolean agg_exprIsNull_0_0, long agg_expr_1_0) throws java.io.IOException {\n",
      "/* 055 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;\n",
      "/* 056 */\n",
      "/* 057 */     // generate grouping key\n",
      "/* 058 */     agg_mutableStateArray_0[0].reset();\n",
      "/* 059 */\n",
      "/* 060 */     agg_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 061 */\n",
      "/* 062 */     if (agg_exprIsNull_0_0) {\n",
      "/* 063 */       agg_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 064 */     } else {\n",
      "/* 065 */       agg_mutableStateArray_0[0].write(0, agg_expr_0_0);\n",
      "/* 066 */     }\n",
      "/* 067 */     int agg_value_2 = 48;\n",
      "/* 068 */\n",
      "/* 069 */     if (!agg_exprIsNull_0_0) {\n",
      "/* 070 */       agg_value_2 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashUnsafeBytes(agg_expr_0_0.getBaseObject(), agg_expr_0_0.getBaseOffset(), agg_expr_0_0.numBytes(), agg_value_2);\n",
      "/* 071 */     }\n",
      "/* 072 */     if (true) {\n",
      "/* 073 */       // try to get the buffer from hash map\n",
      "/* 074 */       agg_unsafeRowAggBuffer_0 =\n",
      "/* 075 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_value_2);\n",
      "/* 076 */     }\n",
      "/* 077 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based\n",
      "/* 078 */     // aggregation after processing all input rows.\n",
      "/* 079 */     if (agg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 080 */       if (agg_sorter_0 == null) {\n",
      "/* 081 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();\n",
      "/* 082 */       } else {\n",
      "/* 083 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());\n",
      "/* 084 */       }\n",
      "/* 085 */\n",
      "/* 086 */       // the hash map had be spilled, it should have enough memory now,\n",
      "/* 087 */       // try to allocate buffer again.\n",
      "/* 088 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(\n",
      "/* 089 */         (agg_mutableStateArray_0[0].getRow()), agg_value_2);\n",
      "/* 090 */       if (agg_unsafeRowAggBuffer_0 == null) {\n",
      "/* 091 */         // failed to allocate the first page\n",
      "/* 092 */         throw new OutOfMemoryError(\"No enough memory for aggregation\");\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */\n",
      "/* 096 */     // common sub-expressions\n",
      "/* 097 */\n",
      "/* 098 */     // evaluate aggregate function\n",
      "/* 099 */     long agg_value_5 = agg_unsafeRowAggBuffer_0.getLong(0);\n",
      "/* 100 */\n",
      "/* 101 */     long agg_value_4 = -1L;\n",
      "/* 102 */     agg_value_4 = agg_value_5 + agg_expr_1_0;\n",
      "/* 103 */     // update unsafe row buffer\n",
      "/* 104 */     agg_unsafeRowAggBuffer_0.setLong(0, agg_value_4);\n",
      "/* 105 */\n",
      "/* 106 */   }\n",
      "/* 107 */\n",
      "/* 108 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {\n",
      "/* 109 */     while (inputadapter_input_0.hasNext() && !stopEarly()) {\n",
      "/* 110 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 111 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 112 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 113 */       null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 114 */       long inputadapter_value_1 = inputadapter_row_0.getLong(1);\n",
      "/* 115 */\n",
      "/* 116 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1);\n",
      "/* 117 */       if (shouldStop()) return;\n",
      "/* 118 */     }\n",
      "/* 119 */\n",
      "/* 120 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */));\n",
      "/* 121 */   }\n",
      "/* 122 */\n",
      "/* 123 */   protected void processNext() throws java.io.IOException {\n",
      "/* 124 */     if (!agg_initAgg_0) {\n",
      "/* 125 */       agg_initAgg_0 = true;\n",
      "/* 126 */\n",
      "/* 127 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();\n",
      "/* 128 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();\n",
      "/* 129 */       agg_doAggregateWithKeys_0();\n",
      "/* 130 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);\n",
      "/* 131 */     }\n",
      "/* 132 */\n",
      "/* 133 */     // output the result\n",
      "/* 134 */\n",
      "/* 135 */     while (agg_mapIter_0.next()) {\n",
      "/* 136 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();\n",
      "/* 137 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();\n",
      "/* 138 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);\n",
      "/* 139 */\n",
      "/* 140 */       if (shouldStop()) return;\n",
      "/* 141 */     }\n",
      "/* 142 */\n",
      "/* 143 */     agg_mapIter_0.close();\n",
      "/* 144 */     if (agg_sorter_0 == null) {\n",
      "/* 145 */       agg_hashMap_0.free();\n",
      "/* 146 */     }\n",
      "/* 147 */   }\n",
      "/* 148 */\n",
      "/* 149 */ }\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grouped = [iso_country: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "printCodeGen: [_](ds: org.apache.spark.sql.Dataset[_])Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[iso_country: string, count: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.execution.command.ExplainCommand\n",
    "\n",
    "val grouped = airports.filter('type === \"small_airport\").groupBy('iso_country).count\n",
    "\n",
    "\n",
    "def printCodeGen[_](ds: Dataset[_]): Unit = {\n",
    "    val logicalPlan = ds.queryExecution.logical\n",
    "    val codeGen = ExplainCommand(logicalPlan, extended = true, codegen = true)\n",
    "    spark.sessionState.executePlan(codeGen).executedPlan.executeCollect().foreach {\n",
    "      r => println(r.getString(0))\n",
    "    }\n",
    "}\n",
    "\n",
    "printCodeGen(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://cs5.pikabu.ru/post_img/big/2015/12/11/7/1449830295198229367.jpg\">\n",
    "\n",
    "### Выводы:\n",
    "+ Spark составляет физический план выполнения запроса на основании написанного вами кода\n",
    "+ Изучив план запроса, можно понять, какие операторы будут применены в ходе обработки ваших данных\n",
    "+ План выполнения запроса - один из основных инструментов оптимизации запроса\n",
    "\n",
    "## Оптимизация соединений и группировок\n",
    "При выполнении `join` двух DF важно следовать рекомендациям:\n",
    "+ фильтровать данные до join'а\n",
    "+ использовать equ join \n",
    "+ если можно путем увеличения количества данных применить equ join вместо non-equ join'а, то делать именно так\n",
    "+ всеми силами избегать cross-join'ов\n",
    "+ если правый DF помещается в памяти worker'а, использовать broadcast()\n",
    "\n",
    "### Виды соединений\n",
    "+ **BroadcastHashJoin**\n",
    "  - equ join\n",
    "  - broadcast\n",
    "+ **SortMergeJoin**\n",
    "  - equ join\n",
    "  - sortable keys\n",
    "+ **BroadcastNestedLoopJoin**\n",
    "  - non-equ join\n",
    "  - using broadcast\n",
    "+ **CartesianProduct**\n",
    "  - non-equ join\n",
    "  \n",
    "[Optimizing Apache Spark SQL Joins: Spark Summit East talk by Vida Ha](https://youtu.be/fp53QhSfQcI)\n",
    "\n",
    "Подготовим два датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left = [type: string, ident: string ... 1 more field]\n",
       "right = [type: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, count: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val left = airports.select('type, 'ident, 'iso_country).localCheckpoint\n",
    "val right = airports.groupBy('type).count.localCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "airports.select('type, 'ident, 'iso_country).write.mode(\"overwrite\").parquet(\"/tmp/1.parquet\")\n",
    "airports.groupBy('type).count.write.mode(\"overwrite\").parquet(\"/tmp/2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left = [type: string, ident: string ... 1 more field]\n",
       "right = [type: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, count: bigint]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val left = spark.read.parquet(\"/tmp/1.parquet\")\n",
    "val right = spark.read.parquet(\"/tmp/2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadcastHashJoin\n",
    "+ работает, когда условие - равенство одного или нескольких ключей\n",
    "+ работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
    "+ оставляет левый датасет как есть\n",
    "+ копирует правый датасет на каждый воркер\n",
    "+ составляет hash map из правого датасета, где ключ - кортеж из колонок в условии соединения\n",
    "+ итерируется по левому датасета внутри каждой партиции и проверяет наличие ключей в HashMap\n",
    "+ может быть автоматически использован, либо явно через `broadcast(df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(2) Project [type#227, ident#228, iso_country#229, count#234L]\n",
      "+- *(2) BroadcastHashJoin [type#227], [type#233], Inner, BuildRight\n",
      "   :- *(2) Project [type#227, ident#228, iso_country#229]\n",
      "   :  +- *(2) Filter isnotnull(type#227)\n",
      "   :     +- *(2) FileScan parquet [type#227,ident#228,iso_country#229] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/1.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(type)], ReadSchema: struct<type:string,ident:string,iso_country:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "      +- *(1) Project [type#233, count#234L]\n",
      "         +- *(1) Filter isnotnull(type#233)\n",
      "            +- *(1) FileScan parquet [type#233,count#234L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/2.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(type)], ReadSchema: struct<type:string,count:bigint>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [type: string, ident: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, ident: string ... 2 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.broadcast // pyspark.sql.functions.broadcast\n",
    "\n",
    "// spark.sql.autobroadcastJoinThreshold\n",
    "\n",
    "val result = left.join(broadcast(right), Seq(\"type\"), \"inner\")\n",
    "\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_data = {}\n",
    "for i in right_rows:\n",
    "  broadcast_data[i[key]] = i\n",
    "\n",
    "for i in l_partition:\n",
    "  maybe_join = broadcast_data.get(i[key])\n",
    "  if maybe_join:\n",
    "    yield (i | maybe_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortMergeJoin\n",
    "+ работает, когда ключи соединения в обоих датасета являются сортируемыми\n",
    "+ репартиционирует оба датасета в 200 партиций по ключу (ключам) соединения\n",
    "+ сортирует партиции каждого из датасетов по ключу (ключам) соединения\n",
    "+ Используя сравнение левого и правого ключей, обходит каждую пару партиций и соединяет строки с одинаковыми ключами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26214400"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(5) Project [type#227, ident#228, iso_country#229, count#234L]\n",
      "+- *(5) SortMergeJoin [type#227], [type#233], Inner\n",
      "   :- *(2) Sort [type#227 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(type#227, 200)\n",
      "   :     +- *(1) Project [type#227, ident#228, iso_country#229]\n",
      "   :        +- *(1) Filter isnotnull(type#227)\n",
      "   :           +- *(1) FileScan parquet [type#227,ident#228,iso_country#229] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/1.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(type)], ReadSchema: struct<type:string,ident:string,iso_country:string>\n",
      "   +- *(4) Sort [type#233 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(type#233, 200)\n",
      "         +- *(3) Project [type#233, count#234L]\n",
      "            +- *(3) Filter isnotnull(type#233)\n",
      "               +- *(3) FileScan parquet [type#233,count#234L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/2.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(type)], ReadSchema: struct<type:string,count:bigint>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [type: string, ident: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, ident: string ... 2 more fields]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "val result = left.join(right, Seq(\"type\"), \"inner\")\n",
    "\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadcastNestedLoopJoin\n",
    "+ работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
    "+ оставляет левый датасет как есть\n",
    "+ копирует правый датасет на каждый воркер\n",
    "+ проходится вложенным циклом по каждой партиции левого датасета и копией правого датасета и проверяет условие\n",
    "+ может быть автоматически использован, либо явно через `broadcast(df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BroadcastNestedLoopJoin BuildRight, Inner, UDF(type#227, type#233)\n",
      ":- *(1) FileScan parquet [type#227,ident#228,iso_country#229] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/1.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<type:string,ident:string,iso_country:string>\n",
      "+- BroadcastExchange IdentityBroadcastMode\n",
      "   +- *(2) FileScan parquet [type#233,count#234L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/2.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<type:string,count:bigint>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "compare_udf = UserDefinedFunction(<function2>,BooleanType,Some(List(StringType, StringType)))\n",
       "joinExpr = UDF(left.type, right.type)\n",
       "result = [type: string, ident: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, ident: string ... 3 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{ expr, udf, col }\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "// Не смотря на то, что UDF сравнивает два ключа, Spark ничего про нее не знает\n",
    "// и не может применить BroadcastHashJoin или SortMergeJoin\n",
    "val compare_udf = udf { (leftVal: String, rightVal: String) => leftVal == rightVal }\n",
    "\n",
    "val joinExpr = compare_udf(col(\"left.type\"), col(\"right.type\"))\n",
    "\n",
    "val result = left.as(\"left\").join(broadcast(right).as(\"right\"), joinExpr, \"inner\")\n",
    "\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_list = []\n",
    "for i in broadcasted_rows:\n",
    "    broadcast_list.append(i)\n",
    "\n",
    "for i in left_partition:\n",
    "  for j in broadcast_list:\n",
    "    if join_function(i, j):\n",
    "       yield (i | j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartesianProduct\n",
    "+ Создает пары из каждой партиции левого датасета с каждой партицией правого датасета, релоцирует каждую пару на один воркер и проверяет условие соединения\n",
    "+ на выходе создает N*M партиций\n",
    "+ работает медленнее остальных и часто приводит к ООМ воркеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartesianProduct UDF(type#227, type#233)\n",
      ":- *(1) FileScan parquet [type#227,ident#228,iso_country#229] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/1.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<type:string,ident:string,iso_country:string>\n",
      "+- *(2) FileScan parquet [type#233,count#234L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/2.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<type:string,count:bigint>\n",
      "\n",
      "Partition summary: \n",
      "    left=2, \n",
      "    right=2, \n",
      "    result=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "compare_udf = UserDefinedFunction(<function2>,BooleanType,Some(List(StringType, StringType)))\n",
       "joinExpr = UDF(left.type, right.type)\n",
       "result = [type: string, ident: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[type: string, ident: string ... 3 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{ expr, udf, col }\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "// Не смотря на то, что UDF сравнивает два ключа, Spark ничего про нее не знает\n",
    "// и не может применить BroadcastHashJoin или SortMergeJoin\n",
    "val compare_udf = udf { (leftVal: String, rightVal: String) => leftVal == rightVal }\n",
    "\n",
    "val joinExpr = compare_udf(col(\"left.type\"), col(\"right.type\"))\n",
    "\n",
    "val result = left.as(\"left\").join(right.as(\"right\"), joinExpr, \"inner\")\n",
    "\n",
    "printPhysicalPlan(result)\n",
    "println(\n",
    "    s\"\"\"Partition summary: \n",
    "    left=${left.rdd.getNumPartitions}, \n",
    "    right=${right.rdd.getNumPartitions}, \n",
    "    result=${result.rdd.getNumPartitions}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CartesianProduct\n",
      ":- *(1) FileScan parquet [type#227,ident#228,iso_country#229] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/1.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<type:string,ident:string,iso_country:string>\n",
      "+- *(2) FileScan parquet [type#233,count#234L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/2.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<type:string,count:bigint>\n"
     ]
    }
   ],
   "source": [
    "left.crossJoin(right).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Снижение количества shuffle\n",
    "В ряде случаев можно уйти от лишних `shuffle` операций при выполнении соединения. Для этого оба DF должны иметь одинаковое партиционирование - одинаковое количество партиций и ключ партиционирования, совпадающий с ключом соединения.\n",
    "\n",
    "Разница между планами выполнения будет хорошо видна в Spark UI на графе выполнения в Jobs и плане выполнения в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "200\n",
      "*(4) Project [type#11, ident#10, name#12, elevation_ft#13, continent#14, iso_country#15, iso_region#16, municipality#17, gps_code#18, iata_code#19, local_code#20, coordinates#21, count#947L]\n",
      "+- *(4) SortMergeJoin [type#11], [type#946], Inner\n",
      "   :- *(2) Sort [type#11 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(type#11, 200)\n",
      "   :     +- *(1) Filter isnotnull(type#11)\n",
      "   :        +- Scan ExistingRDD[ident#10,type#11,name#12,elevation_ft#13,continent#14,iso_country#15,iso_region#16,municipality#17,gps_code#18,iata_code#19,local_code#20,coordinates#21]\n",
      "   +- *(3) Sort [type#946 ASC NULLS FIRST], false, 0\n",
      "      +- *(3) Filter isnotnull(type#946)\n",
      "         +- Scan ExistingRDD[type#946,count#947L]\n",
      "\n",
      "Time taken: 3647 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55113"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.time { \n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    val left = airports.localCheckpoint\n",
    "    val right = airports.groupBy('type).count.localCheckpoint\n",
    "\n",
    "    val joined = left.join(right, Seq(\"type\"))\n",
    "\n",
    "    println(left.rdd.getNumPartitions)\n",
    "    println(right.rdd.getNumPartitions)\n",
    "    printPhysicalPlan(joined)\n",
    "    \n",
    "    joined.count\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "*(5) Project [type#11, ident#10, name#12, elevation_ft#13, continent#14, iso_country#15, iso_region#16, municipality#17, gps_code#18, iata_code#19, local_code#20, coordinates#21, count#1151L]\n",
      "+- *(5) SortMergeJoin [type#11], [type#1155], Inner\n",
      "   :- *(2) Sort [type#11 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(type#11, 50)\n",
      "   :     +- *(1) Project [ident#10, type#11, name#12, elevation_ft#13, continent#14, iso_country#15, iso_region#16, municipality#17, gps_code#18, iata_code#19, local_code#20, coordinates#21]\n",
      "   :        +- *(1) Filter isnotnull(type#11)\n",
      "   :           +- *(1) FileScan csv [ident#10,type#11,name#12,elevation_ft#13,continent#14,iso_country#15,iso_region#16,municipality#17,gps_code#18,iata_code#19,local_code#20,coordinates#21] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(type)], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,...\n",
      "   +- *(4) Sort [type#1155 ASC NULLS FIRST], false, 0\n",
      "      +- *(4) HashAggregate(keys=[type#1155], functions=[count(1)], output=[type#1155, count#1151L])\n",
      "         +- *(4) HashAggregate(keys=[type#1155], functions=[partial_count(1)], output=[type#1155, count#1194L])\n",
      "            +- Exchange hashpartitioning(type#1155, 50)\n",
      "               +- *(3) Project [type#1155]\n",
      "                  +- *(3) Filter isnotnull(type#1155)\n",
      "                     +- *(3) FileScan csv [type#1155] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(type)], ReadSchema: struct<type:string>\n",
      "\n",
      "Time taken: 937 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55113"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.time { \n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    val airportsRep = airports.repartition(200, col(\"type\"))\n",
    "    val left = airportsRep.localCheckpoint\n",
    "    val right = airportsRep.groupBy('type).count.localCheckpoint\n",
    "\n",
    "    val joined = left.join(right, Seq(\"type\"))\n",
    "\n",
    "        println(left.rdd.getNumPartitions)\n",
    "    println(right.rdd.getNumPartitions)\n",
    "    printPhysicalPlan(joined)\n",
    "    \n",
    "    joined.count\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ В Spark используются 4 вида соединений: `BroadcastHashJoin`, `SortMergeJoin`, `BroadcastNestedLoopJoin`, `CartesianProduct`\n",
    "+ Выбор алгоритма основывается на условии соединения и размере датасетов\n",
    "+ `CartesianProduct` обладает самой низкой вычислительной эффективностью и его по возможности стоит избегать\n",
    "\n",
    "## Управление схемой данных\n",
    "В DF API каждая колонка имеет свой тип. Он может быть:\n",
    "+ скаляром - `StringType`, `IntegerType` и т. д.\n",
    "+ массивом - `ArrayType(T)`\n",
    "+ словарем `MapType(K, V)`\n",
    "+ структурой - `StructType()`\n",
    "\n",
    "DF целиком также имеет схему, описанную с помощью класса `StructType`\n",
    "\n",
    "Посмотреть список колонок можно с помощью атрибута `columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident\n",
       "type\n",
       "name\n",
       "elevation_ft\n",
       "continent\n",
       "iso_country\n",
       "iso_region\n",
       "municipality\n",
       "gps_code\n",
       "iata_code\n",
       "local_code\n",
       "coordinates\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "airports.columns.mkString(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема DF доступна через атрибут `schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,true), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,true), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates,StringType,true))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schema: StructType = airports.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply()` метод возвращает поле структуры по имени, как в словаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "field = StructField(ident,StringType,true)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructField(ident,StringType,true)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val field: StructField = schema(\"ident\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "field = StructField(ident,StringType,true)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructField(ident,StringType,true)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val field: StructField = schema.apply(\"ident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StructField` обладает атрибутами `name` и `dataType`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is string\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name = ident\n",
       "fieldType = StringType\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StringType"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val name: String = field.name\n",
    "\n",
    "val fieldType: DataType = field.dataType\n",
    "\n",
    "fieldType match {\n",
    "    case f: StringType => println(\"This is string\")\n",
    "    case _ => println(\"This is not string!\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column ident is string - StringType\n",
      "Column type is string - StringType\n",
      "Column name is string - StringType\n",
      "Column is not string! It's IntegerType\n",
      "Column continent is string - StringType\n",
      "Column iso_country is string - StringType\n",
      "Column iso_region is string - StringType\n",
      "Column municipality is string - StringType\n",
      "Column gps_code is string - StringType\n",
      "Column iata_code is string - StringType\n",
      "Column local_code is string - StringType\n",
      "Column coordinates is string - StringType\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "List((), (), (), (), (), (), (), (), (), (), (), ())"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.schema.map {fieldType =>\n",
    "    fieldType match {\n",
    "        case StructField(n, s: StringType, _, _) => println(s\"Column ${n} is string - ${s}\")\n",
    "        case StructField(_, st, _, _) => println(s\"Column is not string! It's ${st}\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `simpleString` можно использовать, чтобы получить DDL схемы в виде строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "string"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldType.simpleString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airportSchema = struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,iso_region:string,municipality:string,gps_code:string,iata_code:string,local_code:string,coordinates:string>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_country:string,iso_region:string,municipality:string,gps_code:string,iata_code:string,local_code:string,coordinates:string>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airportSchema = schema.simpleString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ddlString = `ident` STRING,`type` STRING,`name` STRING,`elevation_ft` INT,`continent` STRING,`iso_country` STRING,`iso_region` STRING,`municipality` STRING,`gps_code` STRING,`iata_code` STRING,`local_code` STRING,`coordinates` STRING\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,true), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordin..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ddlString = schema.toDDL\n",
    "\n",
    "import org.apache.spark.sql.types.DataType\n",
    "\n",
    "DataType.fromDDL(ddlString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,true), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates,StringType,true))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsonString = {\"type\":\"struct\",\"fields\":[{\"name\":\"ident\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"elevation_ft\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"continent\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"iso_country\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"iso_region\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"municipality\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"gps_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"iata_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"local_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"coordin...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\"type\":\"struct\",\"fields\":[{\"name\":\"ident\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"type\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"elevation_ft\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"continent\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"iso_country\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"iso_region\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"municipality\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"gps_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"iata_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"local_code\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"coordin..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonString = schema.json\n",
    "\n",
    "import org.apache.spark.sql.types.DataType\n",
    "\n",
    "println(DataType.fromJson(jsonString))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема может быть создана из `case class`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Airport\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Airport(\n",
    "    ident: String,\n",
    "    `type`: String,\n",
    "    name: String,\n",
    "    elevation_ft: Int,\n",
    "    continent: String,\n",
    "    iso_country: String,\n",
    "    iso_region: String,\n",
    "    municipality: String,\n",
    "    gps_code: String,\n",
    "    iata_code: String,\n",
    "    local_code: String,\n",
    "    coordinates: String\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schemaFromClass = StructType(StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,false), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,false), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates,StringType,true))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import org.apache.spark.sql.catalyst.ScalaReflection\n",
    "val schemaFromClass = ScalaReflection.schemaFor[Airport].dataType.asInstanceOf[StructType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schemaFromClass = StructType(StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,false), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,false), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates,StringType,true))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schemaFromClass = ScalaReflection.schemaFor[Airport].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(StructField(ident,StringType,true), StructField(type,StringType,true), StructField(name,StringType,true), StructField(elevation_ft,IntegerType,false), StructField(continent,StringType,true), StructField(iso_country,StringType,true), StructField(iso_region,StringType,true), StructField(municipality,StringType,true), StructField(gps_code,StringType,true), StructField(iata_code,StringType,true), StructField(local_code,StringType,true), StructField(coordinates,StringType,true))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.emptyDataset[Airport].schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема может быть использована:\n",
    "+ при чтении источника\n",
    "+ при работе с JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | 40.07080078125, -74.93360137939453 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "csvOptions = Map(header -> true, inferSchema -> false)\n",
       "airports = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"false\")\n",
    "val airports = spark.read.options(csvOptions).schema(schemaFromClass).csv(\"/tmp/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                                                                   |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"ident\":\"00A\",\"type\":\"heliport\",\"name\":\"Total Rf Heliport\",\"elevation_ft\":11,\"continent\":\"NA\",\"iso_country\":\"US\",\"iso_region\":\"US-PA\",\"municipality\":\"Bensalem\",\"gps_code\":\"00A\",\"local_code\":\"00A\",\"coordinates\":\"40.07080078125, -74.93360137939453\"}|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | 40.07080078125, -74.93360137939453 \n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parseJson = jsontostructs(value) AS `s`\n",
       "jsoned = [value: string]\n",
       "withColumns = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val parseJson = from_json(col(\"value\"), schemaFromClass).alias(\"s\")\n",
    "\n",
    "val jsoned = airports.toJSON\n",
    "jsoned.show(1, false)\n",
    "\n",
    "\n",
    "\n",
    "val withColumns = jsoned.select(parseJson).select(col(\"s.*\"))\n",
    "\n",
    "withColumns.show(1, 200, true)\n",
    "withColumns.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема может быть создана вручную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "withColumns: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val withColumns = jsoned.select(parseJson).select(\"s.*\").printSchema\n",
    "\n",
    "//.show(1, truncate=500, vertical=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- foo: string (nullable = true)\n",
      " |-- bar: string (nullable = true)\n",
      " |-- boo: struct (nullable = true)\n",
      " |    |-- x: integer (nullable = true)\n",
      " |    |-- y: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "someSchema = StructType(StructField(foo,StringType,true), StructField(bar,StringType,true), StructField(boo,StructType(StructField(x,IntegerType,true), StructField(y,BooleanType,true)),true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(foo,StringType,true), StructField(bar,StringType,true), StructField(boo,StructType(StructField(x,IntegerType,true), StructField(y,BooleanType,true)),true))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val someSchema = \n",
    "    StructType(\n",
    "        List(\n",
    "            StructField(\"foo\", StringType),\n",
    "            StructField(\"bar\", StringType),\n",
    "            StructField(\n",
    "                        \"boo\", \n",
    "                        StructType(\n",
    "                            List(\n",
    "                                StructField(\"x\", IntegerType),\n",
    "                                StructField(\"y\", BooleanType)\n",
    "                                )\n",
    "                            )\n",
    "                       )\n",
    "        \n",
    "        )\n",
    "    )\n",
    "\n",
    "someSchema.printTreeString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема также может быть получена из JSON строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsoned = [value: string]\n",
       "firstLine = {\"ident\":\"00A\",\"type\":\"heliport\",\"name\":\"Total Rf Heliport\",\"elevation_ft\":11,\"continent\":\"NA\",\"iso_country\":\"US\",\"iso_region\":\"US-PA\",\"municipality\":\"Bensalem\",\"gps_code\":\"00A\",\"local_code\":\"00A\",\"coordinates\":\"40.07080078125, -74.93360137939453\"}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[struct<continent:string,coordinates:string,elevation_ft:bigint,gps_code:string,ident:string,iso_country:string,iso_region:string,local_code:string,municipality:string,name:string,type:string>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsoned = airports.toJSON\n",
    "\n",
    "val firstLine = jsoned.head\n",
    "\n",
    "spark.range(1).select(schema_of_json(lit(firstLine))).head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы изменить тип колонки, следует использовать метод `cast`. Данная операция может как возвращать `null`, так и бросать исключение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- elevation_ft: string (nullable = true)\n",
      "\n",
      "+------------+\n",
      "|elevation_ft|\n",
      "+------------+\n",
      "|11          |\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.select('elevation_ft.cast(\"string\")).printSchema\n",
    "airports.select('elevation_ft.cast(\"string\")).show(1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: float (nullable = true)\n",
      "\n",
      "+----+\n",
      "|type|\n",
      "+----+\n",
      "|null|\n",
      "+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.select('type.cast(\"float\")).printSchema\n",
    "airports.select('type.cast(\"float\")).show(1, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "+ Spark использует схемы для описания типов колонок, схемы всего DF, чтения источников и для работы с JSON\n",
    "+ Схема представляет собой инстанс класса `StructType`\n",
    "+ Колонки в Spark могут иметь любой тип. При этом вложенность словарей, массивов и структур не ограничена\n",
    "\n",
    "## Оптимизатор запросов Catalyst\n",
    "Catalyst выполняет оптимизацию запросов с целью ускорения их выполнения и применяет следующие методы:\n",
    " + Column projection\n",
    " + Partition pruning\n",
    " + Predicate pushdown\n",
    " + Constant folding\n",
    " \n",
    " Подготовим датасет для демонстрации работы Catalyst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airportPq = [ident: string, type: string ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, type: string ... 10 more fields]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports\n",
    "    .repartition(2)\n",
    "    .write\n",
    "    .format(\"parquet\")\n",
    "    .partitionBy(\"iso_country\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"/tmp/airports_2.parquet\")\n",
    "\n",
    "val airportPq = spark.read.parquet(\"/tmp/airports_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/tmp/datasets/airports_2.parquet\n",
    "├── _SUCCESS\n",
    "├── iso_country=AD\n",
    "│   ├── part-00000-656e0232-1f3a-4077-a4dc-438d816b6e4d.c000.snappy.parquet\n",
    "│   └── part-00001-656e0232-1f3a-4077-a4dc-438d816b6e4d.c000.snappy.parquet\n",
    "├── iso_country=AE\n",
    "│   ├── part-00000-656e0232-1f3a-4077-a4dc-438d816b6e4d.c000.snappy.parquet\n",
    "│   └── part-00001-656e0232-1f3a-4077-a4dc-438d816b6e4d.c000.snappy.parquet\n",
    "├── iso_country=AF\n",
    "│   ├── part-00000-656e0232-1f3a-4077-a4dc-438d816b6e4d.c000.snappy.parquet\n",
    "│   └── part-00001-656e0232-1f3a-4077-a4dc-438d816b6e4d.c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column projection\n",
    "Данный механизм позволяет избегать вычитывания ненужных колонок при работе с источниками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(2) HashAggregate(keys=[ident#1673], functions=[count(1)], output=[ident#1673, count#1710L])\n",
      "+- Exchange hashpartitioning(ident#1673, 200)\n",
      "   +- *(1) HashAggregate(keys=[ident#1673], functions=[partial_count(1)], output=[ident#1673, count#1714L])\n",
      "      +- *(1) Project [ident#1673]\n",
      "         +- *(1) FileScan parquet [ident#1673,iso_country#1684] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airports_2.parquet], PartitionCount: 243, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string>\n",
      "\n",
      "Time taken: 10646 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time { \n",
    "    val selected = airportPq.groupBy('ident).count\n",
    "    selected.cache\n",
    "    selected.count\n",
    "    selected.unpersist\n",
    "    printPhysicalPlan(selected)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(1) FileScan parquet [ident#1673,type#1674,name#1675,elevation_ft#1676,continent#1677,iso_region#1678,municipality#1679,gps_code#1680,iata_code#1681,local_code#1682,coordinates#1683,iso_country#1684] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airports_2.parquet], PartitionCount: 243, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_region:string,m...\n",
      "\n",
      "Time taken: 5732 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time { \n",
    "    val selected = airportPq\n",
    "    selected.cache\n",
    "    selected.count\n",
    "    selected.unpersist\n",
    "    printPhysicalPlan(selected)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition pruning\n",
    "Данный механизм позволяет избежать чтения ненужных партиций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(1) FileScan parquet [ident#1673,type#1674,name#1675,elevation_ft#1676,continent#1677,iso_region#1678,municipality#1679,gps_code#1680,iata_code#1681,local_code#1682,coordinates#1683,iso_country#1684] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airports_2.parquet], PartitionCount: 1, PartitionFilters: [isnotnull(iso_country#1684), (iso_country#1684 = RU)], PushedFilters: [], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_region:string,m...\n",
      "\n",
      "Time taken: 3338 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time { \n",
    "    val filtered = airportPq.filter('iso_country === \"RU\")\n",
    "    filtered.count\n",
    "    printPhysicalPlan(filtered)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicate pushdown\n",
    "Данный механизм позволяет \"протолкнуть\" условия фильтрации данных на уровень datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(1) Project [ident#1673, type#1674, name#1675, elevation_ft#1676, continent#1677, iso_region#1678, municipality#1679, gps_code#1680, iata_code#1681, local_code#1682, coordinates#1683, iso_country#1684]\n",
      "+- *(1) Filter (isnotnull(iso_region#1678) && (iso_region#1678 = RU))\n",
      "   +- *(1) FileScan parquet [ident#1673,type#1674,name#1675,elevation_ft#1676,continent#1677,iso_region#1678,municipality#1679,gps_code#1680,iata_code#1681,local_code#1682,coordinates#1683,iso_country#1684] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airports_2.parquet], PartitionCount: 243, PartitionFilters: [], PushedFilters: [IsNotNull(iso_region), EqualTo(iso_region,RU)], ReadSchema: struct<ident:string,type:string,name:string,elevation_ft:int,continent:string,iso_region:string,m...\n",
      "\n",
      "Time taken: 5877 ms\n"
     ]
    }
   ],
   "source": [
    "spark.time { \n",
    "    val filtered = airportPq.filter('iso_region === \"RU\")\n",
    "    filtered.count\n",
    "    printPhysicalPlan(filtered)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify casts\n",
    "Данный механизм убирает ненужные `cast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = spark.range(0,10)\n",
    "result.show\n",
    "result.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias(cast('id as bigint), None)]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Project [cast(id#1935L as bigint) AS id#1940L]\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Range (0, 10, step=1, splits=2)\n"
     ]
    }
   ],
   "source": [
    "spark.range(0,10).select('id.cast(\"long\")).explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(1) Range (0, 10, step=1, splits=2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = spark.range(0,10).select('id.cast(\"long\"))\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(1) Project [cast(id#1941L as int) AS id#1943]\n",
      "+- *(1) Range (0, 10, step=1, splits=2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [id: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = spark.range(0,10).select('id.cast(\"int\"))\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant folding\n",
    "Данный механизм сокращает количество констант, используемых в физическом плане"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(1) Project [true AS foo#1947]\n",
      "+- *(1) Range (0, 10, step=1, splits=2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [foo: boolean]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[foo: boolean]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = spark.range(0,10).select((lit(3) >  lit(0)).alias(\"foo\"))\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(1) Project [(id#1949L > 0) AS foo#1951]\n",
      "+- *(1) Range (0, 10, step=1, splits=2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [foo: boolean]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[foo: boolean]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = spark.range(0,10).select(('id >  0).alias(\"foo\"))\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine filters\n",
    "Данный механизм объединяет фильтры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(1) Filter (((id#1953L > 0) && NOT (id#1953L = 5)) && (id#1953L < 10))\n",
      "+- *(1) Range (0, 10, step=1, splits=2)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result = [id: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = spark.range(0,10).filter('id > 0).filter('id !== 5).filter('id < 10)\n",
    "printPhysicalPlan(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('id < 10)\n",
      "+- Filter NOT (id#1958L = cast(5 as bigint))\n",
      "   +- Filter (id#1958L > cast(0 as bigint))\n",
      "      +- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint\n",
      "Filter (id#1958L < cast(10 as bigint))\n",
      "+- Filter NOT (id#1958L = cast(5 as bigint))\n",
      "   +- Filter (id#1958L > cast(0 as bigint))\n",
      "      +- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (((id#1958L > 0) && NOT (id#1958L = 5)) && (id#1958L < 10))\n",
      "+- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (((id#1958L > 0) && NOT (id#1958L = 5)) && (id#1958L < 10))\n",
      "+- *(1) Range (0, 10, step=1, splits=2)\n"
     ]
    }
   ],
   "source": [
    "spark.range(0,10).filter('id > 0).filter('id !== 5).filter('id < 10).explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Collapse Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('c1, None), unresolvedalias('c2, None), 3 AS c3#2010]\n",
      "+- Project [c1#2005, 2 AS c2#2007]\n",
      "   +- Project [1 AS c1#2005]\n",
      "      +- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "c1: int, c2: int, c3: int\n",
      "Project [c1#2005, c2#2007, 3 AS c3#2010]\n",
      "+- Project [c1#2005, 2 AS c2#2007]\n",
      "   +- Project [1 AS c1#2005]\n",
      "      +- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [1 AS c1#2005, 2 AS c2#2007, 3 AS c3#2010]\n",
      "+- Project\n",
      "   +- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [1 AS c1#2005, 2 AS c2#2007, 3 AS c3#2010]\n",
      "+- *(1) Project\n",
      "   +- *(1) Range (0, 10, step=1, splits=2)\n"
     ]
    }
   ],
   "source": [
    "spark.range(10)\n",
    ".select(lit(1) as \"c1\").select(col(\"c1\"), lit(2) as \"c2\").select(col(\"c1\"), col(\"c2\"), lit(3) as \"c3\").explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.optimizer.excludedRules\", \"org.apache.spark.sql.catalyst.optimizer.CollapseProject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('c1, None), unresolvedalias('c2, None), 3 AS c3#2022]\n",
      "+- Project [c1#2017, 2 AS c2#2019]\n",
      "   +- Project [1 AS c1#2017]\n",
      "      +- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "c1: int, c2: int, c3: int\n",
      "Project [c1#2017, c2#2019, 3 AS c3#2022]\n",
      "+- Project [c1#2017, 2 AS c2#2019]\n",
      "   +- Project [1 AS c1#2017]\n",
      "      +- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [1 AS c1#2017, 2 AS c2#2019, 3 AS c3#2022]\n",
      "+- Project\n",
      "   +- Range (0, 10, step=1, splits=Some(2))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [1 AS c1#2017, 2 AS c2#2019, 3 AS c3#2022]\n",
      "+- *(1) Project\n",
      "   +- *(1) Range (0, 10, step=1, splits=2)\n"
     ]
    }
   ],
   "source": [
    "spark.range(10)\n",
    ".select(lit(1) as \"c1\").select(col(\"c1\"), lit(2) as \"c2\").select(col(\"c1\"), col(\"c2\"), lit(3) as \"c3\").explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
