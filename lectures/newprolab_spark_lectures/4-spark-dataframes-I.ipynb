{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame I\n",
    "**Sergey Grishaev**  \n",
    "serg.grishaev@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Устройство Spark Dataframe API\n",
    "+ Очистка, проекции и срезы данных\n",
    "+ Чтение и запись данных\n",
    "+ Работа с данными\n",
    "  - Группировки\n",
    "  - Запись данных\n",
    "  - Соединения\n",
    "  - Оконные функции\n",
    "  - Встроенные функции\n",
    "+ Кеширование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe API\n",
    "\n",
    "**Dataframe:**\n",
    "+ структурированная колоночная структура данных\n",
    "+ может быть создана на основе:\n",
    "  - локальной коллекции\n",
    "  - файла (файлов)\n",
    "  - базы данных\n",
    "+ в python работает значительно быстрее, чем RDD, тк использует кодогенерацию (см. Tungsten, Janino)\n",
    "+ под капотом использует RDD\n",
    "+ позволяет выполнять произвольные SQL операции с данными\n",
    "+ аналогично RDD являются ленивыми и неизменяеыми\n",
    "\n",
    "## Из чего состоит Dataframe\n",
    "+ схема [pyspsark.sql.StructType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType)\n",
    "+ колонки [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column)\n",
    "+ данные [pyspark.sql.Row](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 3 --executor-memory 3g pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-3.newprolab.com:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sergey Grishaev Spark Dataframe app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=Sergey Grishaev Spark Dataframe app>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "# conf.set(\"spark.app.name\", \"Sergey Grishaev Spark Dataframe app\") \n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Sergey Grishaev Spark Dataframe app\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим тестовый набор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[continent: string, country: string, name: string, population: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "test_data = [\n",
    "{\"name\":\"Moscow\", \"country\":\"Rossiya\", \"continent\": \"Europe\", \"population\": 12380664},\n",
    "{ \"name\":\"Madrid\", \"country\":\"Spain\" },\n",
    "{ \"name\":\"Paris\", \"country\":\"France\", \"continent\": \"Europe\", \"population\" : 2196936},\n",
    "{ \"name\":\"Berlin\", \"country\":\"Germany\", \"continent\": \"Europe\", \"population\": 3490105},\n",
    "{ \"name\":\"Barselona\", \"country\":\"Spain\", \"continent\": \"Europe\" },\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 },\n",
    "{ \"name\":\"Cairo\", \"country\":\"Egypt\", \"continent\": \"Africa\", \"population\": 11922948 },\n",
    "{ }\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(test_data)\n",
    "df = spark.read.json(rdd).localCheckpoint()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `show` выводит часть датафрейма в консоль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------\n",
      " continent  | Europe    \n",
      " country    | Rossiya   \n",
      " name       | Moscow    \n",
      " population | 12380664  \n",
      "-RECORD 1---------------\n",
      " continent  | null      \n",
      " country    | Spain     \n",
      " name       | Madrid    \n",
      " population | null      \n",
      "-RECORD 2---------------\n",
      " continent  | Europe    \n",
      " country    | France    \n",
      " name       | Paris     \n",
      " population | 2196936   \n",
      "-RECORD 3---------------\n",
      " continent  | Europe    \n",
      " country    | Germany   \n",
      " name       | Berlin    \n",
      " population | 3490105   \n",
      "-RECORD 4---------------\n",
      " continent  | Europe    \n",
      " country    | Spain     \n",
      " name       | Barselona \n",
      " population | null      \n",
      "-RECORD 5---------------\n",
      " continent  | Africa    \n",
      " country    | Egypt     \n",
      " name       | Cairo     \n",
      " population | 11922948  \n",
      "-RECORD 6---------------\n",
      " continent  | Africa    \n",
      " country    | Egypt     \n",
      " name       | Cairo     \n",
      " population | 11922948  \n",
      "-RECORD 7---------------\n",
      " continent  | null      \n",
      " country    | null      \n",
      " name       | null      \n",
      " population | null      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, truncate=50, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `printSchema` выводит схему датафрейма в консоль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `select` позволяет выбрать существующие (а также создать новые) колонки из датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe|Rossiya|   Moscow|  12380664|\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|     null|   null|     null|      null|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|continent|country|\n",
      "+---------+-------+\n",
      "|Europe   |Rossiya|\n",
      "|null     |Spain  |\n",
      "|Europe   |France |\n",
      "|Europe   |Germany|\n",
      "|Europe   |Spain  |\n",
      "|Africa   |Egypt  |\n",
      "|Africa   |Egypt  |\n",
      "|null     |null   |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"table0\") #.createOrReplaceTempView(\"table0\")\n",
    "spark.sql(\"\"\"SELECT continent, country FROM table0\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|continent|country|\n",
      "+---------+-------+\n",
      "|Europe   |Rossiya|\n",
      "|null     |Spain  |\n",
      "|Europe   |France |\n",
      "|Europe   |Germany|\n",
      "|Europe   |Spain  |\n",
      "|Africa   |Egypt  |\n",
      "|Africa   |Egypt  |\n",
      "|null     |null   |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"table1\") \n",
    "spark.sql(\"\"\"SELECT continent, country FROM table1\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df = spark.sql(\"\"\"SELECT continent, country FROM table0\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[continent: string, country: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|continent|country|\n",
      "+---------+-------+\n",
      "|Europe   |Rossiya|\n",
      "|null     |Spain  |\n",
      "|Europe   |France |\n",
      "|Europe   |Germany|\n",
      "|Europe   |Spain  |\n",
      "|Africa   |Egypt  |\n",
      "|Africa   |Egypt  |\n",
      "|null     |null   |\n",
      "+---------+-------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('continent, None), unresolvedalias('country, None)]\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string\n",
      "Project [continent#5, country#6]\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [continent#5, country#6]\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [continent#5, country#6]\n",
      "+- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(col(\"continent\"), col(\"country\")).show(10, False)\n",
    "df.select(col(\"continent\"), col(\"country\")).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = col(\"continent\").alias(\"column\") # pyspark.sql.Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|column|\n",
      "+------+\n",
      "|Europe|\n",
      "|  null|\n",
      "|Europe|\n",
      "|Europe|\n",
      "|Europe|\n",
      "|Africa|\n",
      "|Africa|\n",
      "|  null|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(c).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|continent|continent|\n",
      "+---------+---------+\n",
      "|   Europe|   Europe|\n",
      "|     null|     null|\n",
      "|   Europe|   Europe|\n",
      "|   Europe|   Europe|\n",
      "|   Europe|   Europe|\n",
      "|   Africa|   Africa|\n",
      "|   Africa|   Africa|\n",
      "|     null|     null|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"continent\", col(\"continent\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [continent#5, continent#5, continent#5 AS column#131]\n",
      "+- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "df.select(\"continent\", col(\"continent\"), c).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'foo'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "lit(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|FOO|\n",
      "+---+\n",
      "|foo|\n",
      "|foo|\n",
      "|foo|\n",
      "|foo|\n",
      "|foo|\n",
      "|foo|\n",
      "|foo|\n",
      "|foo|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit(\"foo\").alias(\"FOO\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'2 AS `foo`'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(2).alias(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [('foo + 'population) AS foo#175]\n",
      "+- Project [2 AS foo#172, population#8L]\n",
      "   +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "foo: bigint\n",
      "Project [(cast(foo#172 as bigint) + population#8L) AS foo#175L]\n",
      "+- Project [2 AS foo#172, population#8L]\n",
      "   +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [(2 + population#8L) AS foo#175L]\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [(2 + population#8L) AS foo#175L]\n",
      "+- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "df.select(lit(2).alias(\"foo\"), col(\"population\")) \\\n",
    "    .select((col(\"foo\") + col(\"population\")).alias(\"foo\")).explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_sum = (col(\"foo\") / col(\"population\")).alias(\"sum\") # pyspark.sql.Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(foo + population)'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---+\n",
      "|                 sum|population|foo|\n",
      "+--------------------+----------+---+\n",
      "|1.615422242296536E-7|  12380664|  2|\n",
      "|                null|      null|  2|\n",
      "|9.103587906065538E-7|   2196936|  2|\n",
      "|5.730486618597435E-7|   3490105|  2|\n",
      "|                null|      null|  2|\n",
      "|1.677437492807987E-7|  11922948|  2|\n",
      "|1.677437492807987E-7|  11922948|  2|\n",
      "|                null|      null|  2|\n",
      "+--------------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit(2).alias(\"foo\"), col(\"population\")).select(op_sum, col(\"population\"), col(\"foo\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+----------+\n",
      "|continent|country|name  |population|\n",
      "+---------+-------+------+----------+\n",
      "|Europe   |Rossiya|Moscow|12380664  |\n",
      "|Europe   |France |Paris |2196936   |\n",
      "|Europe   |Germany|Berlin|3490105   |\n",
      "|Africa   |Egypt  |Cairo |11922948  |\n",
      "|Africa   |Egypt  |Cairo |11922948  |\n",
      "+---------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"population\") > 10000).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+----------+\n",
      "|continent|country|name  |population|\n",
      "+---------+-------+------+----------+\n",
      "|Europe   |Rossiya|Moscow|12380664  |\n",
      "|Europe   |France |Paris |2196936   |\n",
      "|Europe   |Germany|Berlin|3490105   |\n",
      "|Africa   |Egypt  |Cairo |11922948  |\n",
      "|Africa   |Egypt  |Cairo |11922948  |\n",
      "+---------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"population\") > 10000).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(population > 10000)'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"population\") > 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+----------+\n",
      "|continent|country|name  |population|\n",
      "+---------+-------+------+----------+\n",
      "|Europe   |Rossiya|Moscow|12380664  |\n",
      "|Europe   |France |Paris |2196936   |\n",
      "|Europe   |Germany|Berlin|3490105   |\n",
      "|Africa   |Egypt  |Cairo |11922948  |\n",
      "|Africa   |Egypt  |Cairo |11922948  |\n",
      "+---------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM table0 WHERE population > 10000\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter ('population > 10000)\n",
      "   +- 'UnresolvedRelation `table0`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Project [continent#5, country#6, name#7, population#8L]\n",
      "+- Filter (population#8L > cast(10000 as bigint))\n",
      "   +- SubqueryAlias `table0`\n",
      "      +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(population#8L) && (population#8L > 10000))\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(population#8L) && (population#8L > 10000))\n",
      "+- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM table0 WHERE population > 10000\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('population > 10000)\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Filter (population#8L > cast(10000 as bigint))\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(population#8L) && (population#8L > 10000))\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(population#8L) && (population#8L > 10000))\n",
      "+- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"population\") > 10000).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+----------+\n",
      "|continent|country|name  |population|\n",
      "+---------+-------+------+----------+\n",
      "|Europe   |Rossiya|Moscow|12380664  |\n",
      "|Europe   |France |Paris |2196936   |\n",
      "|Europe   |Germany|Berlin|3490105   |\n",
      "+---------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((col(\"population\") > 10000) & (col(\"continent\") == \"Europe\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter (('population > 10000) && ('continent = Europe))\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Filter ((population#8L > cast(10000 as bigint)) && (continent#5 = Europe))\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (((isnotnull(population#8L) && isnotnull(continent#5)) && (population#8L > 10000)) && (continent#5 = Europe))\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (((isnotnull(population#8L) && isnotnull(continent#5)) && (population#8L > 10000)) && (continent#5 = Europe))\n",
      "+- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "df.filter((col(\"population\") > 10000) & (col(\"continent\") == \"Europe\")).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('continent = Europe)\n",
      "+- Filter (population#8L > cast(10000 as bigint))\n",
      "   +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Filter (continent#5 = Europe)\n",
      "+- Filter (population#8L > cast(10000 as bigint))\n",
      "   +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (((isnotnull(population#8L) && isnotnull(continent#5)) && (population#8L > 10000)) && (continent#5 = Europe))\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (((isnotnull(population#8L) && isnotnull(continent#5)) && (population#8L > 10000)) && (continent#5 = Europe))\n",
      "+- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"population\") > 10000).filter(col(\"continent\") == \"Europe\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+\n",
      "|name  |t   |continent|\n",
      "+------+----+---------+\n",
      "|Moscow|true|Europe   |\n",
      "|Paris |true|Europe   |\n",
      "|Berlin|true|Europe   |\n",
      "+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\n",
    "        col(\"population\") > 10000\n",
    "        ) \\\n",
    "        .select(\n",
    "        col(\"name\"), \n",
    "        lit(True).alias(\"t\"),\n",
    "        col(\"continent\")\n",
    "        ) \\\n",
    "        .filter(col(\"continent\") == \"Europe\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('continent = Europe)\n",
      "+- Project [name#7, true AS t#313, continent#5]\n",
      "   +- Filter (population#8L > cast(10000 as bigint))\n",
      "      +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, t: boolean, continent: string\n",
      "Filter (continent#5 = Europe)\n",
      "+- Project [name#7, true AS t#313, continent#5]\n",
      "   +- Filter (population#8L > cast(10000 as bigint))\n",
      "      +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [name#7, true AS t#313, continent#5]\n",
      "+- Filter (((isnotnull(population#8L) && isnotnull(continent#5)) && (population#8L > 10000)) && (continent#5 = Europe))\n",
      "   +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [name#7, true AS t#313, continent#5]\n",
      "+- *(1) Filter (((isnotnull(population#8L) && isnotnull(continent#5)) && (population#8L > 10000)) && (continent#5 = Europe))\n",
      "   +- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "df.filter(\n",
    "        col(\"population\") > 10000\n",
    "        ) \\\n",
    "        .select(\n",
    "        col(\"name\"), \n",
    "        lit(True).alias(\"t\"),\n",
    "        col(\"continent\")\n",
    "        ) \\\n",
    "        .filter(col(\"continent\") == \"Europe\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = df.filter(\n",
    "        col(\"population\") > 10000\n",
    "        ) \\\n",
    "        .select(\n",
    "        col(\"name\"), \n",
    "        lit(True).alias(\"t\"),\n",
    "        col(\"continent\")\n",
    "        ) \\\n",
    "        .filter(col(\"continent\") == \"Europe\").limit(2).collect()\n",
    "\n",
    "type(rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Moscow', t=True, continent='Europe'),\n",
       " Row(name='Paris', t=True, continent='Europe')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очистка данных\n",
    "Удалим дубликаты. По умолчанию метод `dropDuplicates` удаляет дубликаты строк, у которых ВСЕ колонки совпадают"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|name     |population|\n",
      "+---------+-------+---------+----------+\n",
      "|Europe   |Rossiya|Moscow   |12380664  |\n",
      "|null     |null   |null     |null      |\n",
      "|null     |Spain  |Madrid   |null      |\n",
      "|Europe   |Spain  |Barselona|null      |\n",
      "|Europe   |France |Paris    |2196936   |\n",
      "|Europe   |Germany|Berlin   |3490105   |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Deduplicate [continent#5, country#6, name#7, population#8L]\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Deduplicate [continent#5, country#6, name#7, population#8L]\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [continent#5, country#6, name#7, population#8L], [continent#5, country#6, name#7, population#8L]\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[continent#5, country#6, name#7, population#8L], functions=[], output=[continent#5, country#6, name#7, population#8L])\n",
      "+- Exchange hashpartitioning(continent#5, country#6, name#7, population#8L, 200)\n",
      "   +- *(1) HashAggregate(keys=[continent#5, country#6, name#7, population#8L], functions=[], output=[continent#5, country#6, name#7, population#8L])\n",
      "      +- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show(10, False)\n",
    "df.dropDuplicates().explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|name     |population|\n",
      "+---------+-------+---------+----------+\n",
      "|null     |Spain  |Madrid   |null      |\n",
      "|Europe   |France |Paris    |2196936   |\n",
      "|null     |null   |null     |null      |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "|Europe   |Germany|Berlin   |3490105   |\n",
      "|Europe   |Rossiya|Moscow   |12380664  |\n",
      "|Europe   |Spain  |Barselona|null      |\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Deduplicate [continent#5, name#7]\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Deduplicate [continent#5, name#7]\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [continent#5, name#7], [continent#5, first(country#6, false) AS country#6, name#7, first(population#8L, false) AS population#8L]\n",
      "+- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "SortAggregate(key=[continent#5, name#7], functions=[first(country#6, false), first(population#8L, false)], output=[continent#5, country#6, name#7, population#8L])\n",
      "+- *(2) Sort [continent#5 ASC NULLS FIRST, name#7 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(continent#5, name#7, 200)\n",
      "      +- SortAggregate(key=[continent#5, name#7], functions=[partial_first(country#6, false), partial_first(population#8L, false)], output=[continent#5, name#7, first#371, valueSet#372, first#373L, valueSet#374])\n",
      "         +- *(1) Sort [continent#5 ASC NULLS FIRST, name#7 ASC NULLS FIRST], false, 0\n",
      "            +- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(subset=[\"continent\", \"name\"]).show(10, False)\n",
    "df.dropDuplicates(subset=[\"continent\", \"name\"]).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `.na.drop` удаляет СТРОКИ, в которых отсутствует часть данных. Параметр `how=\"all\"` означает, что будут удалены строки, у которых ВСЕ колонки `null`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   Europe|Rossiya|   Moscow|  12380664|\n",
      "|     null|  Spain|   Madrid|      null|\n",
      "|   Europe| France|    Paris|   2196936|\n",
      "|   Europe|Germany|   Berlin|   3490105|\n",
      "|   Europe|  Spain|Barselona|      null|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|\n",
      "|     null|   null|     null|      null|\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|name     |population|\n",
      "+---------+-------+---------+----------+\n",
      "|Europe   |Rossiya|Moscow   |12380664  |\n",
      "|null     |Spain  |Madrid   |null      |\n",
      "|Europe   |Spain  |Barselona|null      |\n",
      "|Europe   |France |Paris    |2196936   |\n",
      "|Europe   |Germany|Berlin   |3490105   |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[continent#5, country#6, name#7, population#8L], functions=[])\n",
      "+- Exchange hashpartitioning(continent#5, country#6, name#7, population#8L, 200)\n",
      "   +- *(1) HashAggregate(keys=[continent#5, country#6, name#7, population#8L], functions=[])\n",
      "      +- *(1) Filter AtLeastNNulls(n, continent#5,country#6,name#7,population#8L)\n",
      "         +- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().na.drop(how=\"all\").show(10, False)\n",
    "df.dropDuplicates().na.drop(how=\"all\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------+----------+\n",
      "|continent|country|name  |population|\n",
      "+---------+-------+------+----------+\n",
      "|Europe   |Rossiya|Moscow|12380664  |\n",
      "|Europe   |France |Paris |2196936   |\n",
      "|Europe   |Germany|Berlin|3490105   |\n",
      "|Africa   |Egypt  |Cairo |11922948  |\n",
      "+---------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().na.drop(how=\"any\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|name     |population|\n",
      "+---------+-------+---------+----------+\n",
      "|Europe   |Rossiya|Moscow   |12380664  |\n",
      "|Europe   |Spain  |Barselona|null      |\n",
      "|Europe   |France |Paris    |2196936   |\n",
      "|Europe   |Germany|Berlin   |3490105   |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().na.drop(how=\"any\", subset=[\"continent\", \"name\"]).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `.na.fill` заполняет `null`. Для работы этого метода требуется словарь с изменениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|name     |population|\n",
      "+---------+-------+---------+----------+\n",
      "|Europe   |Rossiya|Moscow   |12380664  |\n",
      "|n/a      |Spain  |Madrid   |0         |\n",
      "|Europe   |Spain  |Barselona|0         |\n",
      "|Europe   |France |Paris    |2196936   |\n",
      "|Europe   |Germany|Berlin   |3490105   |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Project [coalesce(continent#5, cast(n/a as string)) AS continent#478, country#6, name#7, coalesce(population#8L, cast(0 as bigint)) AS population#479L]\n",
      "+- Filter AtLeastNNulls(n, continent#5,country#6,name#7,population#8L)\n",
      "   +- Deduplicate [continent#5, country#6, name#7, population#8L]\n",
      "      +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Project [coalesce(continent#5, cast(n/a as string)) AS continent#478, country#6, name#7, coalesce(population#8L, cast(0 as bigint)) AS population#479L]\n",
      "+- Filter AtLeastNNulls(n, continent#5,country#6,name#7,population#8L)\n",
      "   +- Deduplicate [continent#5, country#6, name#7, population#8L]\n",
      "      +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [continent#5, country#6, name#7, population#8L], [coalesce(continent#5, n/a) AS continent#478, country#6, name#7, coalesce(population#8L, 0) AS population#479L]\n",
      "+- Filter AtLeastNNulls(n, continent#5,country#6,name#7,population#8L)\n",
      "   +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[continent#5, country#6, name#7, population#8L], functions=[], output=[continent#478, country#6, name#7, population#479L])\n",
      "+- Exchange hashpartitioning(continent#5, country#6, name#7, population#8L, 200)\n",
      "   +- *(1) HashAggregate(keys=[continent#5, country#6, name#7, population#8L], functions=[], output=[continent#5, country#6, name#7, population#8L])\n",
      "      +- *(1) Filter AtLeastNNulls(n, continent#5,country#6,name#7,population#8L)\n",
      "         +- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "fill_dict = {'continent': 'n/a', 'population': 0 }\n",
    "\n",
    "df.dropDuplicates().na.drop(how=\"all\").na.fill(fill_dict).show(10, False)\n",
    "df.dropDuplicates().na.drop(how=\"all\").na.fill(fill_dict).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `.na.replace` заменяет данные в колонках. Для его работы требуется словарь с заменами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|name     |population|\n",
      "+---------+-------+---------+----------+\n",
      "|Europe   |Russia |Moscow   |12380664  |\n",
      "|n/a      |Spain  |Madrid   |0         |\n",
      "|Europe   |Spain  |Barselona|0         |\n",
      "|Europe   |France |Paris    |2196936   |\n",
      "|Europe   |Germany|Berlin   |3490105   |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Project [continent#528, CASE WHEN (country#6 = Rossiya) THEN cast(Russia as string) ELSE country#6 END AS country#538, name#7, population#529L]\n",
      "+- Project [coalesce(continent#5, cast(n/a as string)) AS continent#528, country#6, name#7, coalesce(population#8L, cast(0 as bigint)) AS population#529L]\n",
      "   +- Filter AtLeastNNulls(n, continent#5,country#6,name#7,population#8L)\n",
      "      +- Deduplicate [continent#5, country#6, name#7, population#8L]\n",
      "         +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "continent: string, country: string, name: string, population: bigint\n",
      "Project [continent#528, CASE WHEN (country#6 = Rossiya) THEN cast(Russia as string) ELSE country#6 END AS country#538, name#7, population#529L]\n",
      "+- Project [coalesce(continent#5, cast(n/a as string)) AS continent#528, country#6, name#7, coalesce(population#8L, cast(0 as bigint)) AS population#529L]\n",
      "   +- Filter AtLeastNNulls(n, continent#5,country#6,name#7,population#8L)\n",
      "      +- Deduplicate [continent#5, country#6, name#7, population#8L]\n",
      "         +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [continent#5, country#6, name#7, population#8L], [coalesce(continent#5, n/a) AS continent#528, CASE WHEN (country#6 = Rossiya) THEN Russia ELSE country#6 END AS country#538, name#7, coalesce(population#8L, 0) AS population#529L]\n",
      "+- Filter AtLeastNNulls(n, continent#5,country#6,name#7,population#8L)\n",
      "   +- LogicalRDD [continent#5, country#6, name#7, population#8L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[continent#5, country#6, name#7, population#8L], functions=[], output=[continent#528, country#538, name#7, population#529L])\n",
      "+- Exchange hashpartitioning(continent#5, country#6, name#7, population#8L, 200)\n",
      "   +- *(1) HashAggregate(keys=[continent#5, country#6, name#7, population#8L], functions=[], output=[continent#5, country#6, name#7, population#8L])\n",
      "      +- *(1) Filter AtLeastNNulls(n, continent#5,country#6,name#7,population#8L)\n",
      "         +- Scan ExistingRDD[continent#5,country#6,name#7,population#8L]\n"
     ]
    }
   ],
   "source": [
    "replace_dict = {\"Rossiya\": \"Russia\"}\n",
    "\n",
    "df.dropDuplicates().na.drop(\"all\").na.fill(fill_dict).na.replace(replace_dict, subset=[\"country\"]).show(10, False)\n",
    "df.dropDuplicates().na.drop(\"all\").na.fill(fill_dict).na.replace(replace_dict, subset=[\"country\"]).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":2},{\"class\":\"org.apache.spark.sql.execution.aggregate.HashAggregateExec\",\"num-children\":1,\"requiredChildDistributionExpressions\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"continent\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":5,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":6,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":7,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"population\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":8,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}]],\"groupingExpressions\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"continent\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":5,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":6,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":7,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"population\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":8,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}]],\"aggregateExpressions\":[],\"aggregateAttributes\":[],\"initialInputBufferOffset\":4,\"resultExpressions\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Alias\",\"num-children\":1,\"child\":0,\"name\":\"continent\",\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":597,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[],\"explicitMetadata\":{}},{\"class\":\"org.apache.spark.sql.catalyst.expressions.Coalesce\",\"num-children\":2,\"children\":[0,1]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"continent\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":5,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"n/a\",\"dataType\":\"string\"}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Alias\",\"num-children\":1,\"child\":0,\"name\":\"country\",\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":607,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[],\"explicitMetadata\":{}},{\"class\":\"org.apache.spark.sql.catalyst.expressions.CaseWhen\",\"num-children\":3,\"branches\":null,\"elseValue\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":6,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.EqualTo\",\"num-children\":2,\"left\":0,\"right\":1},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":6,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"Rossiya\",\"dataType\":\"string\"},{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"Russia\",\"dataType\":\"string\"},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":6,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":7,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Alias\",\"num-children\":1,\"child\":0,\"name\":\"population\",\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":598,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[],\"explicitMetadata\":{}},{\"class\":\"org.apache.spark.sql.catalyst.expressions.Coalesce\",\"num-children\":2,\"children\":[0,1]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"population\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":8,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"0\",\"dataType\":\"long\"}]],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.InputAdapter\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.exchange.ShuffleExchangeExec\",\"num-children\":1,\"newPartitioning\":[{\"class\":\"org.apache.spark.sql.catalyst.plans.physical.HashPartitioning\",\"num-children\":4,\"expressions\":[0,1,2,3],\"numPartitions\":200},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"continent\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":5,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":6,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":7,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"population\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":8,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.aggregate.HashAggregateExec\",\"num-children\":1,\"groupingExpressions\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"continent\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":5,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":6,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":7,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"population\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":8,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}]],\"aggregateExpressions\":[],\"aggregateAttributes\":[],\"initialInputBufferOffset\":0,\"resultExpressions\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"continent\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":5,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":6,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":7,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"population\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":8,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}]],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.FilterExec\",\"num-children\":1,\"condition\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AtLeastNNonNulls\",\"num-children\":4,\"n\":1,\"children\":[0,1,2,3]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"continent\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":5,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":6,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":7,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]},{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"population\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":8,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.InputAdapter\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"continent\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":5,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"country\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":6,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":7,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"population\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":8,\"jvmId\":\"c6b9b368-4ef7-4732-8e07-da4fa9b238b1\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .dropDuplicates() \\\n",
    "    .na.drop(\"all\") \\\n",
    "    .na.fill(fill_dict) \\\n",
    "    .na.replace(replace_dict, subset=[\"country\"]) \\\n",
    "    ._jdf \\\n",
    "    .queryExecution() \\\n",
    "    .executedPlan() \\\n",
    "    .toJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrameNaFunctions"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1687282146874"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._jvm.System.currentTimeMillis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготровим датафрейм с очищенными данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "clean_data = df \\\n",
    "                .dropDuplicates() \\\n",
    "                .na.drop(\"all\") \\\n",
    "                .na.fill(fill_dict) \\\n",
    "                .na.replace(replace_dict) \\\n",
    "                .filter(col(\"population\") >= 0) \\\n",
    "                .select(col(\"continent\"), col(\"country\"), col(\"name\"), col(\"population\")) \\\n",
    "                .localCheckpoint()\n",
    "\n",
    "clean_data.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+\n",
      "|continent|country|name     |population|\n",
      "+---------+-------+---------+----------+\n",
      "|Europe   |Russia |Moscow   |12380664  |\n",
      "|n/a      |Spain  |Madrid   |0         |\n",
      "|Europe   |Spain  |Barselona|0         |\n",
      "|Europe   |France |Paris    |2196936   |\n",
      "|Europe   |Germany|Berlin   |3490105   |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |\n",
      "+---------+-------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим базовый агрегат. По умолчанию имена колонок принимают неудобные названия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------------+\n",
      "|continent|count(1)|sum(population)|\n",
      "+---------+--------+---------------+\n",
      "|Europe   |4       |18067705       |\n",
      "|Africa   |1       |11922948       |\n",
      "|n/a      |1       |0              |\n",
      "+---------+--------+---------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[continent#630], functions=[count(1), sum(population#621L)])\n",
      "+- Exchange hashpartitioning(continent#630, 200)\n",
      "   +- *(1) HashAggregate(keys=[continent#630], functions=[partial_count(1), partial_sum(population#621L)])\n",
      "      +- *(1) Project [continent#630, population#621L]\n",
      "         +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n",
      "+---------------+\n",
      "|sum(population)|\n",
      "+---------------+\n",
      "|       18067705|\n",
      "|       11922948|\n",
      "|              0|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import count, sum\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "agg = clean_data.groupBy(col(\"continent\")).agg(F.count(\"*\"), F.sum(col(\"population\")))\n",
    "\n",
    "agg.show(10, False)\n",
    "agg.explain()\n",
    "\n",
    "agg.select(col(\"`sum(population)`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `alias` позволяет переименовывать колонки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|continent|city_count|population_sum|\n",
      "+---------+----------+--------------+\n",
      "|europe   |4         |18067705      |\n",
      "|africa   |1         |11922948      |\n",
      "|n/a      |1         |0             |\n",
      "+---------+----------+--------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[continent#630], functions=[count(1), sum(population#621L)])\n",
      "+- Exchange hashpartitioning(continent#630, 200)\n",
      "   +- *(1) HashAggregate(keys=[continent#630], functions=[partial_count(1), partial_sum(population#621L)])\n",
      "      +- *(1) Project [continent#630, population#621L]\n",
      "         +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, sum, lower\n",
    "\n",
    "pop_count = count(\"*\").alias(\"city_count\")\n",
    "pop_sum = F.sum(col(\"population\")).alias(\"population_sum\")\n",
    "\n",
    "agg = clean_data \\\n",
    "            .groupBy(\"continent\") \\\n",
    "            .agg(pop_count, pop_sum) \\\n",
    "            .withColumn(\"continent\", lower(col(\"continent\")))\n",
    "\n",
    "agg.show(10, False)\n",
    "agg.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `orderBy` позволяет сортировать Dataframe. Это удобно, если мы хотим вывести его содержимое на экран с помощью `show`, однако не подходит при записи данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'population_sum ASC NULLS FIRST'>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"population_sum\").desc()\n",
    "col(\"population_sum\").asc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|continent|city_count|population_sum|\n",
      "+---------+----------+--------------+\n",
      "|   europe|         4|      18067705|\n",
      "|   africa|         1|      11922948|\n",
      "|      n/a|         1|             0|\n",
      "+---------+----------+--------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [population_sum#693L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(population_sum#693L DESC NULLS LAST, 1)\n",
      "   +- Exchange RoundRobinPartitioning(1)\n",
      "      +- *(2) HashAggregate(keys=[continent#630], functions=[count(1), sum(population#621L)])\n",
      "         +- Exchange hashpartitioning(continent#630, 1)\n",
      "            +- *(1) HashAggregate(keys=[continent#630], functions=[partial_count(1), partial_sum(population#621L)])\n",
      "               +- *(1) Project [continent#630, population#621L]\n",
      "                  +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n"
     ]
    }
   ],
   "source": [
    "agg.repartition(1).orderBy(col(\"population_sum\").desc()).show()\n",
    "agg.repartition(1).orderBy(col(\"population_sum\").desc()).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|continent|city_count|population_sum|\n",
      "+---------+----------+--------------+\n",
      "|   europe|         4|      18067705|\n",
      "|   africa|         1|      11922948|\n",
      "|      n/a|         1|             0|\n",
      "+---------+----------+--------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [population_sum#693L DESC NULLS LAST], false, 0\n",
      "+- Exchange RoundRobinPartitioning(1)\n",
      "   +- *(2) HashAggregate(keys=[continent#630], functions=[count(1), sum(population#621L)])\n",
      "      +- Exchange hashpartitioning(continent#630, 200)\n",
      "         +- *(1) HashAggregate(keys=[continent#630], functions=[partial_count(1), partial_sum(population#621L)])\n",
      "            +- *(1) Project [continent#630, population#621L]\n",
      "               +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n"
     ]
    }
   ],
   "source": [
    "agg.repartition(1).sortWithinPartitions(col(\"population_sum\").desc()).show()\n",
    "agg.repartition(1).sortWithinPartitions(col(\"population_sum\").desc()).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтение данных из источника\n",
    "Основной метод чтения любых источников\n",
    "\n",
    "```df = spark.read.format(datasource_type).option(datasource_options).load(object_name)```\n",
    "\n",
    "+ ```datasource_type``` - тип источника (\"parquet\", \"json\", \"cassandra\") и т. д.\n",
    "+ ```datasource_options``` - опции для работы с источником (логины, пароли, адреса для подключения и т. д.)\n",
    "+ ```object_name``` - имя таблицы/файла/топика/индекса\n",
    "\n",
    "[DataframeReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader):\n",
    "+ по умолчанию выводит схему данных\n",
    "+ является трансформацией (ленивый)\n",
    "+ возвращает [Dataframe](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "\n",
    "### Список (неполный) поддерживаемых источников данных\n",
    "+ Файлы:\n",
    "  - json\n",
    "  - text\n",
    "  - csv\n",
    "  - orc\n",
    "  - parquet\n",
    "  - delta\n",
    "+ Базы данных\n",
    "  - elasticsearch\n",
    "  - cassandra\n",
    "  - jdbc\n",
    "  - hive\n",
    "  - redis\n",
    "  - mongo\n",
    "+ Брокеры сообщений\n",
    "  - kafka\n",
    "  \n",
    "\n",
    "**Библиотеки для работы с источниками должны быть доступны в JAVA CLASSPATH на драйвере и воркерах!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"/tmp/airport-codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | 40.07080078125, -74.93360137939453 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запись данных\n",
    "Основной метод записи в любые системы\n",
    "\n",
    "```df.write.format(datasource_type).options(datasource_options).mode(savemode).save(object_name)```\n",
    "\n",
    "+ ```datasource_type``` - тип источника (\"parquet\", \"json\", \"cassandra\") и т. д.\n",
    "+ ```datasource_options``` - опции для работы с источником (логины, пароли, адреса для подключения и т. д.)\n",
    "+ ```savemode``` - режим записи данных (добавление, перезапись и т. д.)\n",
    "+ ```object_name``` - имя таблицы/файла/топика/индекса\n",
    "\n",
    "[DataFrameWriter](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter):\n",
    "+ метод ```save``` является действием\n",
    "+ позволяет работать с партиционированными данными (parquet, orc)\n",
    "+ не всегда валидирует схему и формат данных\n",
    "\n",
    "\n",
    "### Список (неполный) поддерживаемых источников данных\n",
    "+ Файлы:\n",
    "  - json\n",
    "  - text\n",
    "  - csv\n",
    "  - orc\n",
    "  - parquet\n",
    "  - delta\n",
    "+ Базы данных\n",
    "  - elasticsearch\n",
    "  - cassandra\n",
    "  - jdbc\n",
    "  - hive\n",
    "  - redis\n",
    "  - mongo\n",
    "+ Брокеры сообщений\n",
    "  - kafka\n",
    "  \n",
    "\n",
    "**Библиотеки для работы с источниками должны быть доступны в JAVA CLASSPATH на драйвере и воркерах!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok! Data is written to /tmp/teacher/agg0.parquet\n"
     ]
    }
   ],
   "source": [
    "condition = col(\"continent\") != \"n/a\"\n",
    "\n",
    "agg \\\n",
    "    .filter(condition) \\\n",
    "    .repartition(2) \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/{u}/agg0.parquet\".format(u=sc.sparkUser()))\n",
    "\n",
    "print(\"Ok! Data is written to /tmp/{u}/agg0.parquet\".format(u=sc.sparkUser()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok! Data is written to /tmp/teacher/agg0.parquet\n"
     ]
    }
   ],
   "source": [
    "condition = col(\"continent\") != \"n/a\"\n",
    "\n",
    "agg \\\n",
    "    .filter(condition) \\\n",
    "    .repartition(2) \\\n",
    "    .write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"codec\", \"gzip\") \\\n",
    "    .save(\"/tmp/{u}/agg0.csv\".format(u=sc.sparkUser()))\n",
    "\n",
    "print(\"Ok! Data is written to /tmp/{u}/agg0.csv\".format(u=sc.sparkUser()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 teacher hdfs          0 2023-06-20 21:08 /tmp/teacher/agg0.parquet/_SUCCESS\r\n",
      "-rw-r--r--   3 teacher hdfs        954 2023-06-20 21:08 /tmp/teacher/agg0.parquet/part-00000-5e90b924-aca5-41ed-8092-9eb7ae8313df-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /tmp/teacher/agg0.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 teacher hdfs          0 2023-06-20 21:07 /tmp/teacher/agg0.csv/_SUCCESS\r\n",
      "-rw-r--r--   3 teacher hdfs         56 2023-06-20 21:07 /tmp/teacher/agg0.csv/part-00000-56029a5f-1de2-4a9e-bd3e-5029ba01c8b4-c000.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /tmp/teacher/agg0.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[continent: string, city_count: bigint, population_sum: bigint]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/tmp/teacher/agg0.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соединения\n",
    "\n",
    "Join'ы позволяют соединять два DF в один по заданным условиям.\n",
    "\n",
    "По типу условия join'ы делятся на:\n",
    "+ equ-join - соединение по равенству одного или более ключей\n",
    "+ non-equ join - соединение по условию, отличному от равенства одного или более ключей\n",
    "\n",
    "При выполнении Join используется один из алгоритмов:\n",
    "- BroadcastHashJoin\n",
    "- SortMergeJoin\n",
    "- BroadcastNestedLoopJoin\n",
    "- CartesianProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n",
      "+---------+-------+---------+----------+\n",
      "|continent|country|     name|population|\n",
      "+---------+-------+---------+----------+\n",
      "|   europe| Russia|   Moscow|  12380664|\n",
      "|      n/a|  Spain|   Madrid|         0|\n",
      "|   europe|  Spain|Barselona|         0|\n",
      "|   europe| France|    Paris|   2196936|\n",
      "|   europe|Germany|   Berlin|   3490105|\n",
      "|   africa|  Egypt|    Cairo|  11922948|\n",
      "+---------+-------+---------+----------+\n",
      "\n",
      "+---------+----------+--------------+\n",
      "|continent|city_count|population_sum|\n",
      "+---------+----------+--------------+\n",
      "|   europe|         4|      18067705|\n",
      "|   africa|         1|      11922948|\n",
      "+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left = clean_data.withColumn(\"continent\", lower(col(\"continent\")))\n",
    "left.printSchema()\n",
    "\n",
    "right = spark.read.parquet(\"/tmp/{u}/agg0.parquet\".format(u=sc.sparkUser()))\n",
    "right.printSchema()\n",
    "\n",
    "left.show()\n",
    "right.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве условия соединения можно использовать:\n",
    "- имя колонки, по которой делается соединение\n",
    "- массив имен колонок, по которым делается соединение\n",
    "- выражение `pyspark.sql.Column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|continent|country|name     |population|city_count|population_sum|\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|europe   |Russia |Moscow   |12380664  |4         |18067705      |\n",
      "|europe   |Spain  |Barselona|0         |4         |18067705      |\n",
      "|europe   |France |Paris    |2196936   |4         |18067705      |\n",
      "|europe   |Germany|Berlin   |3490105   |4         |18067705      |\n",
      "|africa   |Egypt  |Cairo    |11922948  |1         |11922948      |\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [continent#898, country#631, name#632, population#621L, city_count#904L, population_sum#905L]\n",
      "+- *(2) BroadcastHashJoin [continent#898], [continent#903], Inner, BuildRight\n",
      "   :- *(2) Project [lower(continent#630) AS continent#898, country#631, name#632, population#621L]\n",
      "   :  +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "      +- *(1) Project [continent#903, city_count#904L, population_sum#905L]\n",
      "         +- *(1) Filter isnotnull(continent#903)\n",
      "            +- *(1) FileScan parquet [continent#903,city_count#904L,population_sum#905L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/teacher/agg0.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(continent)], ReadSchema: struct<continent:string,city_count:bigint,population_sum:bigint>\n"
     ]
    }
   ],
   "source": [
    "joined = left.join(right, 'continent', 'inner')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show(10, False)\n",
    "joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|continent|country|name     |population|city_count|population_sum|\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|europe   |Russia |Moscow   |12380664  |4         |18067705      |\n",
      "|europe   |Spain  |Barselona|0         |4         |18067705      |\n",
      "|europe   |France |Paris    |2196936   |4         |18067705      |\n",
      "|europe   |Germany|Berlin   |3490105   |4         |18067705      |\n",
      "|africa   |Egypt  |Cairo    |11922948  |1         |11922948      |\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [continent#898, country#631, name#632, population#621L, city_count#904L, population_sum#905L]\n",
      "+- *(2) BroadcastHashJoin [continent#898], [continent#903], Inner, BuildRight\n",
      "   :- *(2) Project [lower(continent#630) AS continent#898, country#631, name#632, population#621L]\n",
      "   :  +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "      +- *(1) Project [continent#903, city_count#904L, population_sum#905L]\n",
      "         +- *(1) Filter isnotnull(continent#903)\n",
      "            +- *(1) FileScan parquet [continent#903,city_count#904L,population_sum#905L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/teacher/agg0.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(continent)], ReadSchema: struct<continent:string,city_count:bigint,population_sum:bigint>\n"
     ]
    }
   ],
   "source": [
    "joined = left.join(right, ['continent'], 'inner')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show(10, False)\n",
    "joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n",
      "+---------+-------+---------+----------+---------+----------+--------------+\n",
      "|continent|country|name     |population|continent|city_count|population_sum|\n",
      "+---------+-------+---------+----------+---------+----------+--------------+\n",
      "|europe   |Russia |Moscow   |12380664  |europe   |4         |18067705      |\n",
      "|europe   |Spain  |Barselona|0         |europe   |4         |18067705      |\n",
      "|europe   |France |Paris    |2196936   |europe   |4         |18067705      |\n",
      "|europe   |Germany|Berlin   |3490105   |europe   |4         |18067705      |\n",
      "|africa   |Egypt  |Cairo    |11922948  |africa   |1         |11922948      |\n",
      "+---------+-------+---------+----------+---------+----------+--------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [continent#898], [continent#903], Inner, BuildRight\n",
      ":- *(2) Project [lower(continent#630) AS continent#898, country#631, name#632, population#621L]\n",
      ":  +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "   +- *(1) Project [continent#903, city_count#904L, population_sum#905L]\n",
      "      +- *(1) Filter isnotnull(continent#903)\n",
      "         +- *(1) FileScan parquet [continent#903,city_count#904L,population_sum#905L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/teacher/agg0.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(continent)], ReadSchema: struct<continent:string,city_count:bigint,population_sum:bigint>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "joined = left.alias(\"first\") \\\n",
    "                .join(right.alias(\"second\"), \n",
    "                      col(\"first.continent\") == col(\"second.continent\"), 'inner')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show(10, False)\n",
    "joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Reference 'continent' is ambiguous, could be: first.continent, second.continent.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o852.select.\n: org.apache.spark.sql.AnalysisException: Reference 'continent' is ambiguous, could be: first.continent, second.continent.;\n\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:259)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$42.apply(Analyzer.scala:1001)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$42.apply(Analyzer.scala:1003)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:1000)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:1009)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:1009)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:1009)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$37.apply(Analyzer.scala:1072)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$37.apply(Analyzer.scala:1072)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:1072)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:1012)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:1012)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:823)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-72fb78b5c824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"continent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"l_continent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \"\"\"\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Reference 'continent' is ambiguous, could be: first.continent, second.continent.;\""
     ]
    }
   ],
   "source": [
    "joined.select(col(\"continent\").alias(\"l_continent\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- population: long (nullable = false)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- city_count: long (nullable = true)\n",
      " |-- population_sum: long (nullable = true)\n",
      "\n",
      "+---------+-------+---------+----------+---------+----------+--------------+\n",
      "|continent|country|name     |population|continent|city_count|population_sum|\n",
      "+---------+-------+---------+----------+---------+----------+--------------+\n",
      "|europe   |Russia |Moscow   |12380664  |europe   |4         |18067705      |\n",
      "|europe   |Spain  |Barselona|0         |europe   |4         |18067705      |\n",
      "|europe   |France |Paris    |2196936   |europe   |4         |18067705      |\n",
      "|europe   |Germany|Berlin   |3490105   |europe   |4         |18067705      |\n",
      "|africa   |Egypt  |Cairo    |11922948  |africa   |1         |11922948      |\n",
      "+---------+-------+---------+----------+---------+----------+--------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [continent#898], [continent#903], Inner, BuildRight\n",
      ":- *(2) Project [lower(continent#630) AS continent#898, country#631, name#632, population#621L]\n",
      ":  +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      "   +- *(1) Project [continent#903, city_count#904L, population_sum#905L]\n",
      "      +- *(1) Filter isnotnull(continent#903)\n",
      "         +- *(1) FileScan parquet [continent#903,city_count#904L,population_sum#905L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/teacher/agg0.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(continent)], ReadSchema: struct<continent:string,city_count:bigint,population_sum:bigint>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "joined = left.alias(\"left\") \\\n",
    "                .join(broadcast(right).alias(\"right\"), \n",
    "                      expr(\"\"\"left.continent = right.continent\"\"\"), 'inner')\n",
    "\n",
    "joined.printSchema()\n",
    "\n",
    "joined.show(10, False)\n",
    "joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(left.continent = right.continent)'>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr(\"\"\"left.continent = right.continent\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оконные функции\n",
    "\n",
    "Оконные функции позволяют делать функции над \"окнами\" (кто бы мог подумать) данных\n",
    "\n",
    "Окно создается из класса [pyspark.sql.Window](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Window) с указанием полей, определяющих границы окон и полей, определяющих порядок сортировки внутри окна:\n",
    "\n",
    "```window = Window.partitionBy(\"a\", \"b\").orderBy(\"a\")```\n",
    "\n",
    "Применяя окна, можно использовать такие полезные функции из [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions), как ```lag()``` и ```lead()```, а также эффективно работать с данными time-series, вычисляя такие параметры, как, например, среднее значение заданного поля за 3-х часовой интервал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|continent|country|     name|population|city_count|population_sum|\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "|   Europe| France|    Paris|   2196936|         4|      18067705|\n",
      "|   Europe|Germany|   Berlin|   3490105|         4|      18067705|\n",
      "|   Europe| Russia|   Moscow|  12380664|         4|      18067705|\n",
      "|   Europe|  Spain|Barselona|         0|         4|      18067705|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|         1|      11922948|\n",
      "|      n/a|  Spain|   Madrid|         0|         1|             0|\n",
      "+---------+-------+---------+----------+----------+--------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "Window [count(1) windowspecdefinition(continent#630, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS city_count#1138L, sum(population#621L) windowspecdefinition(continent#630, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS population_sum#1145L], [continent#630]\n",
      "+- *(1) Sort [continent#630 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(continent#630, 200)\n",
      "      +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n"
     ]
    }
   ],
   "source": [
    "# В нашем случае, используя оконные функции, мы можем построить DF из предыдущих примеров c join, \n",
    "# но без использования соединения\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "window = Window.partitionBy(\"continent\")\n",
    "\n",
    "agg = clean_data \\\n",
    "    .withColumn(\"city_count\", F.count(\"*\").over(window)) \\\n",
    "    .withColumn(\"population_sum\", F.sum(\"population\").over(window)) \\\n",
    "\n",
    "agg.show()\n",
    "agg.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+---+\n",
      "|continent|country|     name|population| rn|\n",
      "+---------+-------+---------+----------+---+\n",
      "|   Europe| Russia|   Moscow|  12380664|  1|\n",
      "|   Europe|Germany|   Berlin|   3490105|  2|\n",
      "|   Europe| France|    Paris|   2196936|  3|\n",
      "|   Europe|  Spain|Barselona|         0|  4|\n",
      "|   Africa|  Egypt|    Cairo|  11922948|  1|\n",
      "|      n/a|  Spain|   Madrid|         0|  1|\n",
      "+---------+-------+---------+----------+---+\n",
      "\n",
      "== Physical Plan ==\n",
      "Window [row_number() windowspecdefinition(continent#630, population#621L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#1112], [continent#630], [population#621L DESC NULLS LAST]\n",
      "+- *(1) Sort [continent#630 ASC NULLS FIRST, population#621L DESC NULLS LAST], false, 0\n",
      "   +- Exchange hashpartitioning(continent#630, 200)\n",
      "      +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n"
     ]
    }
   ],
   "source": [
    "# В нашем случае, используя оконные функции, мы можем построить DF из предыдущих примеров c join, \n",
    "# но без использования соединения\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "window = Window.partitionBy(\"continent\").orderBy(col(\"population\").desc())\n",
    "\n",
    "agg = clean_data.withColumn(\"rn\", row_number().over(window))\n",
    "agg.show()\n",
    "agg.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В нашем случае, используя оконные функции, мы можем построить DF из предыдущих примеров c join, \n",
    "# но без использования соединения\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "window = \\\n",
    "    Window.partitionBy(\"continent\") \\\n",
    "            .orderBy(col(\"population\").desc()) \\\n",
    "            .rowsBetween(-1, 1)\n",
    "\n",
    "window = \\\n",
    "    Window.partitionBy(\"continent\") \\\n",
    "            .orderBy(col(\"population\").desc()) \\\n",
    "            .rangeBetween(-100, 100)\n",
    "\n",
    "# Window.unboundedPreceding\n",
    "# Window.unboundedFollowing\n",
    "# Window.currentRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции pyspark.sql.functions\n",
    "\n",
    "Spark обладает достаточно большим набором встроенных функций, доступных в [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions), поэтому перед тем, как писать свою UDF, стоит обязательно поискать нужную функцию в данном пакете.\n",
    "\n",
    "К тому же, все функции Spark принимают на вход и возвращают [pyspark.sql.Column](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column), а это значит, что вы можете совмещать функции вместе\n",
    "\n",
    "**Также важно помнить, что функции и колонки в Spark могут быть созданы без привязки к конкретным данным и DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+----------+----------+--------------+---------------------+\n",
      "|continent|country|name     |population|city_count|population_sum|avg_pop              |\n",
      "+---------+-------+---------+----------+----------+--------------+---------------------+\n",
      "|Europe   |France |Paris    |2196936   |4         |18067705      |{\"value\":4516926.25} |\n",
      "|Europe   |Germany|Berlin   |3490105   |4         |18067705      |{\"value\":4516926.25} |\n",
      "|Europe   |Russia |Moscow   |12380664  |4         |18067705      |{\"value\":4516926.25} |\n",
      "|Europe   |Spain  |Barselona|0         |4         |18067705      |{\"value\":4516926.25} |\n",
      "|Africa   |Egypt  |Cairo    |11922948  |1         |11922948      |{\"value\":1.1922948E7}|\n",
      "|n/a      |Spain  |Madrid   |0         |1         |0             |{\"value\":0.0}        |\n",
      "+---------+-------+---------+----------+----------+--------------+---------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "Project [continent#630, country#631, name#632, population#621L, city_count#1138L, population_sum#1145L, structstojson(named_struct(value, (cast(population_sum#1145L as double) / cast(city_count#1138L as double))), Some(Europe/Moscow)) AS avg_pop#1172]\n",
      "+- Window [count(1) windowspecdefinition(continent#630, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS city_count#1138L, sum(population#621L) windowspecdefinition(continent#630, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS population_sum#1145L], [continent#630]\n",
      "   +- *(1) Sort [continent#630 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(continent#630, 200)\n",
      "         +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json, col, struct\n",
    "\n",
    "avg_pop = \\\n",
    "    to_json(\n",
    "        struct(\n",
    "            (col(\"population_sum\") / col(\"city_count\")).alias(\"value\")\n",
    "        )\n",
    "    ).alias(\"avg_pop\")\n",
    "\n",
    "agg.select(col(\"*\"), avg_pop).show(truncate=False)\n",
    "agg.select(col(\"*\"), avg_pop).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большим преимуществом Spark по сравнению с большинством SQL ориентированных БД является наличие встроенных функций работы со списками, словарями и структурами данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- allinone: struct (nullable = false)\n",
      " |    |-- continent: string (nullable = false)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- population: long (nullable = false)\n",
      " |    |-- city_count: long (nullable = false)\n",
      " |    |-- population_sum: long (nullable = true)\n",
      "\n",
      "+-----------------------------------------------+\n",
      "|allinone                                       |\n",
      "+-----------------------------------------------+\n",
      "|[Europe, Spain, Barselona, 0, 4, 18067705]     |\n",
      "|[Europe, Russia, Moscow, 12380664, 4, 18067705]|\n",
      "|[Europe, France, Paris, 2196936, 4, 18067705]  |\n",
      "|[Europe, Germany, Berlin, 3490105, 4, 18067705]|\n",
      "|[Africa, Egypt, Cairo, 11922948, 1, 11922948]  |\n",
      "|[n/a, Spain, Madrid, 0, 1, 0]                  |\n",
      "+-----------------------------------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [named_struct(continent, continent#630, country, country#631, name, name#632, population, population#621L, city_count, city_count#1138L, population_sum, population_sum#1145L) AS allinone#1209]\n",
      "+- Window [count(1) windowspecdefinition(continent#630, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS city_count#1138L, sum(population#621L) windowspecdefinition(continent#630, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS population_sum#1145L], [continent#630]\n",
      "   +- *(1) Sort [continent#630 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(continent#630, 200)\n",
      "         +- Scan ExistingRDD[continent#630,country#631,name#632,population#621L]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "all_in_one = agg.select(struct(*agg.columns).alias(\"allinone\"))\n",
    "\n",
    "all_in_one.printSchema()\n",
    "all_in_one.show(20, False)\n",
    "all_in_one.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, можно создавать массивы и объединять их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|c                 |\n",
      "+------------------+\n",
      "|[1, 2, 3, 4, 5, 6]|\n",
      "+------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [array_union(a#1217, b#1220) AS c#1227]\n",
      "+- Scan ExistingRDD[id#1215L,a#1217,b#1220]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "arrays = \\\n",
    "    spark.range(0,1) \\\n",
    "    .withColumn(\"a\", array(lit(1), lit(2), lit(3))) \\\n",
    "    .withColumn(\"b\", array(lit(4),lit(5),lit(6))) \\\n",
    "    .localCheckpoint() \\\n",
    "    .select(array_union(col(\"a\"), col(\"b\")).alias(\"c\"))\n",
    "\n",
    "\n",
    "arrays.show(1, False)\n",
    "arrays.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Списки не очень удобны для анализа, поэтому обычно их \"взрывают\" с помощью `explode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  c|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "+---+\n",
      "\n",
      "== Physical Plan ==\n",
      "Generate explode(c#1227), false, [c#1241]\n",
      "+- *(1) Project [array_union(a#1217, b#1220) AS c#1227]\n",
      "   +- Scan ExistingRDD[id#1215L,a#1217,b#1220]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "arrays.select(explode(col(\"c\")).alias(\"c\")).show()\n",
    "arrays.select(explode(col(\"c\")).alias(\"c\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|pmod(id, CAST(2 AS BIGINT))|\n",
      "+---------------------------+\n",
      "|                          0|\n",
      "|                          1|\n",
      "|                          0|\n",
      "|                          1|\n",
      "|                          0|\n",
      "|                          1|\n",
      "|                          0|\n",
      "|                          1|\n",
      "|                          0|\n",
      "|                          1|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).select(F.expr(\"\"\"pmod(id, 2)\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.functions' has no attribute 'pmod'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-6c3a99d049c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.sql.functions' has no attribute 'pmod'"
     ]
    }
   ],
   "source": [
    "spark.range(10).select(F.pmod(col(\"id\"), 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кеширование\n",
    "По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения. Для демонстрации возьмем датасет [Airport Codes](https://datahub.io/core/airport-codes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | 40.07080078125, -74.93360137939453 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"/tmp/airport-codes.csv\")\n",
    "df.printSchema()\n",
    "df.show(1, 200, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем несколько агрегатов. Несмотря на то, что `only_ru` является общим для всех действий, он пересчитывается при вызове каждого действия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "only_ru = df.filter((col(\"iso_country\") == \"RU\") & (col(\"elevation_ft\") > 1000))\n",
    "only_ru.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|iso_region|count|\n",
      "+----------+-----+\n",
      "|    RU-CHI|   18|\n",
      "|    RU-IRK|   14|\n",
      "|     RU-SA|   13|\n",
      "|     RU-BU|    9|\n",
      "|    RU-KYA|    7|\n",
      "|    RU-AMU|    7|\n",
      "|     RU-BA|    4|\n",
      "|    RU-STA|    3|\n",
      "|    RU-MAG|    2|\n",
      "|     RU-AL|    1|\n",
      "|     RU-TY|    1|\n",
      "|     RU-DA|    1|\n",
      "|    RU-KEM|    1|\n",
      "|    RU-PRI|    1|\n",
      "|     RU-KB|    1|\n",
      "|     RU-KC|    1|\n",
      "|     RU-IN|    1|\n",
      "|     RU-SE|    1|\n",
      "+----------+-----+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [count#1377L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(count#1377L DESC NULLS LAST, 200)\n",
      "   +- *(2) HashAggregate(keys=[iso_region#1269], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(iso_region#1269, 200)\n",
      "         +- *(1) HashAggregate(keys=[iso_region#1269], functions=[partial_count(1)])\n",
      "            +- *(1) Project [iso_region#1269]\n",
      "               +- *(1) Filter (((isnotnull(iso_country#1268) && isnotnull(elevation_ft#1266)) && (iso_country#1268 = RU)) && (elevation_ft#1266 > 1000))\n",
      "                  +- *(1) FileScan csv [elevation_ft#1266,iso_country#1268,iso_region#1269] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country), IsNotNull(elevation_ft), EqualTo(iso_country,RU), GreaterThan(elevation_..., ReadSchema: struct<elevation_ft:int,iso_country:string,iso_region:string>\n"
     ]
    }
   ],
   "source": [
    "only_ru.groupBy(col(\"iso_region\")).count().orderBy(col(\"count\").desc()).show()\n",
    "only_ru.groupBy(col(\"iso_region\")).count().orderBy(col(\"count\").desc()).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|          type|count|\n",
      "+--------------+-----+\n",
      "|      heliport|    6|\n",
      "|        closed|    9|\n",
      "|medium_airport|   34|\n",
      "| small_airport|   37|\n",
      "+--------------+-----+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[type#1264], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(type#1264, 200)\n",
      "   +- *(1) HashAggregate(keys=[type#1264], functions=[partial_count(1)])\n",
      "      +- *(1) Project [type#1264]\n",
      "         +- *(1) Filter (((isnotnull(iso_country#1268) && isnotnull(elevation_ft#1266)) && (iso_country#1268 = RU)) && (elevation_ft#1266 > 1000))\n",
      "            +- *(1) FileScan csv [type#1264,elevation_ft#1266,iso_country#1268] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country), IsNotNull(elevation_ft), EqualTo(iso_country,RU), GreaterThan(elevation_..., ReadSchema: struct<type:string,elevation_ft:int,iso_country:string>\n"
     ]
    }
   ],
   "source": [
    "only_ru.groupBy(col(\"type\")).count().show()\n",
    "only_ru.groupBy(col(\"type\")).count().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой проблемы следует использовать методы `cache`, либо `persist`. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что `persist` позволяет выбрать, куда сохранить данные, а `cache` использует значение по умолчанию. В текущей версии Spark это [StorageLevel.MEMORY_AND_DISK](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence). Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить `unpersist` для очистки памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_ru.cache()\n",
    "only_ru.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|iso_region|count|\n",
      "+----------+-----+\n",
      "|    RU-CHI|   18|\n",
      "|    RU-IRK|   14|\n",
      "|     RU-SA|   13|\n",
      "|     RU-BU|    9|\n",
      "|    RU-KYA|    7|\n",
      "|    RU-AMU|    7|\n",
      "|     RU-BA|    4|\n",
      "|    RU-STA|    3|\n",
      "|    RU-MAG|    2|\n",
      "|     RU-AL|    1|\n",
      "|     RU-TY|    1|\n",
      "|     RU-DA|    1|\n",
      "|    RU-KEM|    1|\n",
      "|    RU-PRI|    1|\n",
      "|     RU-KB|    1|\n",
      "|     RU-KC|    1|\n",
      "|     RU-IN|    1|\n",
      "|     RU-SE|    1|\n",
      "+----------+-----+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Sort [count#2004L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(count#2004L DESC NULLS LAST, 200)\n",
      "   +- *(2) HashAggregate(keys=[iso_region#1269], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(iso_region#1269, 200)\n",
      "         +- *(1) HashAggregate(keys=[iso_region#1269], functions=[partial_count(1)])\n",
      "            +- *(1) Project [iso_region#1269]\n",
      "               +- *(1) Filter (((isnotnull(iso_country#1268) && isnotnull(elevation_ft#1266)) && (iso_country#1268 = RU)) && (elevation_ft#1266 > 1000))\n",
      "                  +- *(1) FileScan csv [elevation_ft#1266,iso_country#1268,iso_region#1269] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country), IsNotNull(elevation_ft), EqualTo(iso_country,RU), GreaterThan(elevation_..., ReadSchema: struct<elevation_ft:int,iso_country:string,iso_region:string>\n",
      "+--------------+-----+\n",
      "|          type|count|\n",
      "+--------------+-----+\n",
      "|      heliport|    6|\n",
      "|        closed|    9|\n",
      "|medium_airport|   34|\n",
      "| small_airport|   37|\n",
      "+--------------+-----+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[type#1264], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(type#1264, 200)\n",
      "   +- *(1) HashAggregate(keys=[type#1264], functions=[partial_count(1)])\n",
      "      +- *(1) Project [type#1264]\n",
      "         +- *(1) Filter (((isnotnull(iso_country#1268) && isnotnull(elevation_ft#1266)) && (iso_country#1268 = RU)) && (elevation_ft#1266 > 1000))\n",
      "            +- *(1) FileScan csv [type#1264,elevation_ft#1266,iso_country#1268] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://spark-master-1.newprolab.com:8020/tmp/airport-codes.csv], PartitionFilters: [], PushedFilters: [IsNotNull(iso_country), IsNotNull(elevation_ft), EqualTo(iso_country,RU), GreaterThan(elevation_..., ReadSchema: struct<type:string,elevation_ft:int,iso_country:string>\n"
     ]
    }
   ],
   "source": [
    "only_ru.groupBy(col(\"iso_region\")).count().orderBy(col(\"count\").desc()).show()\n",
    "only_ru.groupBy(col(\"iso_region\")).count().orderBy(col(\"count\").desc()).explain()\n",
    "only_ru.groupBy(col(\"type\")).count().show()\n",
    "only_ru.groupBy(col(\"type\")).count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ident: string, type: string, name: string, elevation_ft: int, continent: string, iso_country: string, iso_region: string, municipality: string, gps_code: string, iata_code: string, local_code: string, coordinates: string]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_ru.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "**Dataframe API**:\n",
    "+ мощный инструмент для работы с данными\n",
    "+ в отличие от RDD, Dataframe API устроен так, что все вычисления происходят в JVM\n",
    "+ обладает единым API для работы с различными источниками данных\n",
    "+ имеет большой набор встроенных функций работы с данными\n",
    "\n",
    "# Спасибо!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
