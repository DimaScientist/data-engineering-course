{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--driver-memory 4g --executor-memory 4g --num-executors 5 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Sergey Grishaev Link Prediction app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir /tmp/trainGraph/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put prediction.csv /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphPath = \"/tmp/trainGraph/\"\n",
    "usersToPredictPath = \"/tmp/prediction.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType(fields=[\n",
    "    StructField(\"user\", IntegerType()),\n",
    "    StructField(\"friendsString\", StringType())\n",
    "])\n",
    "\n",
    "data = spark.read.format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"delimiter\", \"\\t\") \\\n",
    "        .load(graphPath).repartition(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, collect_list, sort_array, size, split, lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "def cutStartEndBrackets(series):\n",
    "    return series.str[2:-2]\n",
    "\n",
    "cutStartEndBracketsUDF = pandas_udf(cutStartEndBrackets, StringType())\n",
    "\n",
    "userFriend = \\\n",
    "    data.select(col(\"user\"), split(cutStartEndBracketsUDF(col(\"friendsString\")), \"\\),\\(\").alias(\"friendsMasks\"))\\\n",
    "    .withColumn(\"friendMask\", explode('friendsMasks'))\\\n",
    "    .withColumn(\"friend\", split(col(\"friendMask\"), \",\")[0])\\\n",
    "    .select(col(\"user\").cast(\"integer\"), col(\"friend\").cast(\"integer\")).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/step1.jpg\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userFriend.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userFriend.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersWithCommonFriend = userFriend\\\n",
    "    .groupBy(\"friend\")\\\n",
    "    .agg(collect_list(\"user\").alias(\"usersWithCommonFriend\")) \\\n",
    "    .select(\"usersWithCommonFriend\")\\\n",
    "    .where(size(col(\"usersWithCommonFriend\")) >= 2)\\\n",
    "    .select(sort_array(\"usersWithCommonFriend\").alias(\"sortedUsersWithCommonFriend\"))\\\n",
    "    .drop(\"usersWithCommonFriend\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/step2.jpg\" width=700/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersWithCommonFriend.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersWithCommonFriend.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_users_to_pred = StructType(fields=[\n",
    "    StructField(\"user\", IntegerType()),\n",
    "])\n",
    "\n",
    "usersToPredict = spark.read.format(\"csv\") \\\n",
    "    .schema(schema_users_to_pred) \\\n",
    "    .load(usersToPredictPath) \\\n",
    "    .select(col(\"user\").cast(\"integer\")) \\\n",
    "    .rdd.map(lambda t : t.user).collect()\n",
    "\n",
    "usersToPredictBC = spark.sparkContext.broadcast(set(usersToPredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "def pairsWithCommonFriend(usersWithCommonFriend):\n",
    "    pairs = []\n",
    "    for user1Index in range(0, len(usersWithCommonFriend)):\n",
    "        for user2Index in range(user1Index + 1, len(usersWithCommonFriend)):\n",
    "            if user1Index != user2Index:\n",
    "                if (usersWithCommonFriend[user1Index] in usersToPredictBC.value or \\\n",
    "                usersWithCommonFriend[user2Index] in usersToPredictBC.value):\n",
    "                    pairs.append((usersWithCommonFriend[user1Index], usersWithCommonFriend[user2Index]))\n",
    "    return pairs\n",
    "\n",
    "schema = ArrayType(ArrayType(IntegerType()))\n",
    "\n",
    "pairsWithCommonFriendUdf = udf(pairsWithCommonFriend, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/step4_2.jpg\" width=700/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "def pairsWithCommonFriend(series):\n",
    "    pairs_lists = []\n",
    "    for usersWithCommonFriend in series:\n",
    "        pairs = []\n",
    "        for user1Index in range(0, len(usersWithCommonFriend)):\n",
    "            for user2Index in range(user1Index + 1, len(usersWithCommonFriend)):\n",
    "                if user1Index != user2Index:\n",
    "                    if usersWithCommonFriend[user1Index] in usersToPredictBC.value or \\\n",
    "                    usersWithCommonFriend[user2Index] in usersToPredictBC.value:\n",
    "                        pairs.append((usersWithCommonFriend[user1Index], usersWithCommonFriend[user2Index]))\n",
    "        pairs_lists.append(pairs)\n",
    "    return pd.Series(pairs_lists)\n",
    "        \n",
    "pairsWithCommonFriendUdf = pandas_udf(pairsWithCommonFriend, schema)\n",
    "\n",
    "commonFriendsCounts = usersWithCommonFriend\\\n",
    "            .select(pairsWithCommonFriendUdf(\"sortedUsersWithCommonFriend\").alias(\"pairsWithCommonFriend\"))\\\n",
    "            .where(size(col(\"pairsWithCommonFriend\")) > 0).repartition(24).cache()    \n",
    "\n",
    "commonFriendsCounts\\\n",
    "    .withColumn(\"pairWithCommonFriend\", explode(\"pairsWithCommonFriend\"))\\\n",
    "    .drop(col(\"pairsWithCommonFriend\"))\\\n",
    "    .groupBy(col(\"pairWithCommonFriend\"))\\\n",
    "    .count()\\\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairsWithCommonFriend(series):\n",
    "    pairs_lists = []\n",
    "    for usersWithCommonFriend in series:\n",
    "        pairs = []\n",
    "        for user1Index in range(0, len(usersWithCommonFriend)):\n",
    "            for user2Index in range(user1Index + 1, len(usersWithCommonFriend)):\n",
    "                if user1Index != user2Index:\n",
    "                    pairs.append((usersWithCommonFriend[user1Index], usersWithCommonFriend[user2Index]))\n",
    "        pairs_lists.append(pairs)\n",
    "    return pd.Series(pairs_lists)\n",
    "         \n",
    "pairsWithCommonFriendUdf = pandas_udf(pairsWithCommonFriend, schema_pandas)\n",
    "\n",
    "commonFriendsCounts = usersWithCommonFriend\\\n",
    "            .select(pairsWithCommonFriendUdf(\"sortedUsersWithCommonFriend\").alias(\"pairsWithCommonFriend\"))\\\n",
    "            .where(size(col(\"pairsWithCommonFriend\")) > 0)    \n",
    "\n",
    "commonFriendsCounts\\\n",
    "    .withColumn(\"pairWithCommonFriend\", explode(\"pairsWithCommonFriend\"))\\\n",
    "    .drop(col(\"pairsWithCommonFriend\"))\\\n",
    "    .groupBy(col(\"pairWithCommonFriend\"))\\\n",
    "    .count()\\\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def pairsWithCommonFriendUpgraded(series, modulo):\n",
    "    pairs_lists = []\n",
    "\n",
    "    for usersWithCommonFriend in series:\n",
    "        pairs = []\n",
    "        for user1Index in range(0, len(usersWithCommonFriend)):\n",
    "             for user2Index in range(user1Index + 1, len(usersWithCommonFriend)):\n",
    "                    if user1Index != user2Index and user1Index % 13 == modulo:\n",
    "                        pairs.append((usersWithCommonFriend[user1Index], usersWithCommonFriend[user2Index]))\n",
    "        pairs_lists.append(pairs)\n",
    "    return pd.Series(pairs_lists)\n",
    "\n",
    "\n",
    "for i in range(13):\n",
    "    pairsWithCommonFriendUdfUpgraded = pandas_udf(partial(pairsWithCommonFriendUpgraded, modulo=i), schema)\n",
    "\n",
    "    commonFriendsCounts = usersWithCommonFriend\\\n",
    "            .select(pairsWithCommonFriendUdfUpgraded(\"sortedUsersWithCommonFriend\").alias(\"pairsWithCommonFriend\"))\\\n",
    "            .where(size(col(\"pairsWithCommonFriend\")) > 0)\\\n",
    "            .write.parquet(\"pairs/\" + str(i), mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"pairs/*\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"pairs/0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"pairs/*\")\\\n",
    "    .withColumn(\"pairWithCommonFriend\", explode(\"pairsWithCommonFriend\"))\\\n",
    "    .drop(col(\"pairsWithCommonFriend\"))\\\n",
    "    .groupBy(col(\"pairWithCommonFriend\"))\\\n",
    "    .count()\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
