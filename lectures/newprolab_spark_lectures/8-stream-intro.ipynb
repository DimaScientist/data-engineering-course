{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"200\" height=\"200\" src=\"https://static.tildacdn.com/tild6236-6337-4339-b337-313363643735/new_logo.png\">\n",
    "\n",
    "# Spark Structured Streaming I\n",
    "**Сергей Гришаев**  \n",
    "serg.grishaev@gmail.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На этом занятии\n",
    "+ Общие сведения\n",
    "+ Rate streaming\n",
    "+ File streaming\n",
    "+ Kafka streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие сведения\n",
    "\n",
    "Системы поточной обработки данных:\n",
    "- работают с непрерывным потоком данных\n",
    "- нужно хранить состояние стрима\n",
    "- результат обработки быстро появляется в целевой системе\n",
    "- должны проектироваться с учетом требований к высокой доступности\n",
    "- важная скорость обработки данных и время зажержки (лаг)\n",
    "\n",
    "### Примеры систем поточной обработки данных\n",
    "\n",
    "#### Карточный процессинг\n",
    "- нельзя терять платежи\n",
    "- нельзя дублировать платежи\n",
    "- простой сервиса недопустим\n",
    "- максимальное время задержки ~ 1 сек\n",
    "- небольшой поток событий\n",
    "- OLTP\n",
    "\n",
    "#### Обработка логов \n",
    "- потеря единичных событий допустима\n",
    "- дублирование единичных событий допустимо\n",
    "- простой сервиса допустим\n",
    "- максимальное время задержки ~ 1 час\n",
    "- большой поток событий\n",
    "- OLAP\n",
    "\n",
    "### Виды стриминг систем\n",
    "\n",
    "#### Real-time streaming\n",
    "- низкие задержки на обработку\n",
    "- низкая пропускная способность\n",
    "- подходят для критичных систем\n",
    "- пособытийная обработка\n",
    "- OLTP\n",
    "- exactly once consistency (нет потери данных и нет дубликатов)\n",
    "\n",
    "#### Micro batch streaming\n",
    "- высокие задержки\n",
    "- высокая пропускная способность\n",
    "- не подходят для критичных систем\n",
    "- обработка батчами\n",
    "- OLAP\n",
    "- at least once consistency (во время сбоев могут возникать дубликаты)\n",
    "\n",
    "### Выводы:\n",
    "+ Существуют два типа систем поточной обработки данных - real-time и micro-batch\n",
    "+ Spark Structured Streaming является micro-batch системой\n",
    "+ При работе с большими данными обычно пропускная способность важнее, чем время задержки\n",
    "\n",
    "\n",
    "## Rate streaming\n",
    "\n",
    "Самый простой способ создать стрим - использовать `rate` источник. Созданный DF является streaming, о чем нам говорит метод создания `readStream` и атрибут `isStreaming`. `rate` хорошо подходит для тестирования приложений, когда нет возможности подключится к потоку реальных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sdf = spark.readStream.format(\"rate\").load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У `sdf`, как и у любого DF, есть схема и план выполнения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema\n",
    "sdf.explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличии от обычных DF, у `sdf` нет таких методов, как `show`, `collect`, `take`. Для них также недоступен Dataset API. Поэтому для того, чтобы посмотреть их содержимое, мы должны использовать `console` синк и создать `StreamingQuery`. Процессинг начинается только после вызова метода `start`. `trigger` позволяет настроить, как часто стрим будет читать новые данные и обрабатывать их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSink: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createConsoleSink(df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sink = createConsoleSink(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sq = sink.start // StreamingQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы остановить DF, можно вызвать метод `stop` к `sdf`, либо получить список всех streming DF и остановить их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "killAll: ()Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "def killAll() = {\n",
    "    SparkSession\n",
    "        .active\n",
    "        .streams\n",
    "        .active\n",
    "        .foreach { x =>\n",
    "                    val desc = x.lastProgress.sources.head.description\n",
    "                    x.stop\n",
    "                    println(s\"Stopped ${desc}\")\n",
    "        }               \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим стрим, выполняющий запись в `parquet` файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createParquetSink(df: DataFrame, \n",
    "                      fileName: String) = {\n",
    "    df\n",
    "    .repartition(1)\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", s\"/tmp/spark-intro-stream/$fileName\")\n",
    "    .option(\"checkpointLocation\", s\"/tmp/chk_01/$fileName\")\n",
    "    .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hadoop fs -mkdir /tmp/spark-intro-stream \"\"\".!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"hadoop fs -ls / \"\"\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sink = createParquetSink(sdf, \"s1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sq = sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Убедимся, что стрим пишется в файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\"hadoop fs -ls  /tmp/spark-intro-stream/s1.parquet/*\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем файл с помощью Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rates = spark.read.parquet(\"/tmp/spark-intro-stream/s1.parquet\")\n",
    "println(rates.count)\n",
    "rates.printSchema\n",
    "rates.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параллельно внутри одного Spark приложения может работать несколько стримов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val consoleSink = createConsoleSink(sdf)\n",
    "val consoleSq = consoleSink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая добавляет к нашей колонке случайный `ident` аэропорта из датасета [Airport Codes](https://datahub.io/core/airport-codes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csvOptions = Map(\"header\" -> \"true\", \"inferSchema\" -> \"true\")\n",
    "val airports = spark.read.options(csvOptions).csv(\"/tmp/airport-codes.csv\")\n",
    "airports.printSchema\n",
    "airports.show(numRows = 1, truncate = 100, vertical = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val idents = airports.select('ident).limit(200).distinct.as[String].collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val identSdf = sdf.withColumn(\"ident\", shuffle(array(idents.map(lit(_)):_*))(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val identPqSink = createParquetSink(identSdf, \"s2.parquet\")\n",
    "val identPqSq = identPqSink.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что данные записываются в `parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val identPq = spark.read.parquet(\"/tmp/spark-intro-stream/s2.parquet\")\n",
    "println(identPq.count)\n",
    "identPq.printSchema\n",
    "identPq.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Временно остановим стрим, он понадобится нам для следующих экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- `rate` - самый простой способ создать стрим для тестирования приложений\n",
    "- стрим начинает работу после вызова метода `start` и не блокирует основной поток программы\n",
    "- в одном Spark приложении может работать несколько стримов одновременно\n",
    "\n",
    "## File Streaming\n",
    "Spark позволяет запустить стрим, который будет \"слушать\" директорию и читать из нее новые файлы. При этом за раз будет прочитано количество файлов, установленное в параметре `maxFilesPerTrigger` [ссылка](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources). В этом кроется одна из основных проблем данного источника. Поскольку стрим, сконфигурированный под чтение небольших файлов, может \"упасть\", если в директорию начнут попадать файлы большого объема. Создадим стрим из директории `datasets/s2.parquet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sdfFromParquet = spark\n",
    "        .readStream\n",
    "        .format(\"parquet\")\n",
    "        .option(\"maxFilesPerTrigger\", \"1\")\n",
    "        .option(\"path\", \"/tmp/spark-intro-stream/s2.parquet\")\n",
    "        .load\n",
    "\n",
    "sdfFromParquet.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку в директорию могут попасть любые данные, а df должен иметь фиксированную схему, то Spark не позволяет нам создавать SDF на основе файлов без указания схемы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val letters = List(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"i\")\n",
    "val condition = letters.map { x => col(\"ident\").startsWith(x) }.reduce { (x,y) => x or y }\n",
    "\n",
    "val sdfFromParquet = spark\n",
    "        .readStream\n",
    "        .format(\"parquet\")\n",
    "        .schema(identPq.schema)\n",
    "        .option(\"maxFilesPerTrigger\", \"1\")\n",
    "        .option(\"path\", \"/tmp/spark-intro-stream/s2.parquet\")\n",
    "        .load\n",
    "        .withColumn(\"ident\", lower('ident))\n",
    "\n",
    "sdfFromParquet.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val opts = Map(\n",
    "    \"maxFilesPerTrigger\" -> \"100\", \n",
    "    \"path\" -> \"/tmp/spark-intro-stream/s2.parquet\"\n",
    ")\n",
    "\n",
    "val sdfFromParquet = spark\n",
    "        .readStream\n",
    "        .format(\"parquet\")\n",
    "        .schema(identPq.schema)\n",
    "        .options(opts)\n",
    "        .load\n",
    "        .withColumn(\"ident\", lower('ident))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val consoleSink = createConsoleSink(sdfFromParquet)\n",
    "consoleSink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File source позволяет со всеми типами файлов, с которыми умеет работать Spark: `parquet`, `orc`, `csv`, `json`, `text`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Spark позволяет создавать SDF на базе всех поддерживаемых типов файлов\n",
    "- При создании SDF вы должны указать схему данных\n",
    "- File streaming имеет несколько серьезных недостатков:\n",
    "  + Входной поток можно ограничить только макисмальным количество файлов, попадающих в батч\n",
    "  + Если стрим упадает посередине файла, то при перезапуске эти данные будут обработаны еще раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"100\" height=\"100\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Apache_kafka.svg/1200px-Apache_kafka.svg.png\">\n",
    "\n",
    "## Kafka streaming\n",
    "\n",
    "https://kafka.apache.org\n",
    "\n",
    "**Apache Kafka** - самая распространенная в мире система, на основе которой строятся приложения для поточной обработки данных. Она имеет несколько преимуществ:\n",
    "- высокая пропускная способность\n",
    "- высокая доступность за счет распределенной архитектуры и репликации\n",
    "- у каждого сообщения есть свой номер, который называется offset, что позволяет гранулярно сохранять состояние стрима\n",
    "\n",
    "### Архитектура системы\n",
    "\n",
    "#### Topic\n",
    "Топик - это таблицы в Kafka. Мы пишем данные в топик и читаем данные из топика. Топик как правило распределен по нескольким узлам кластера для обеспечения высокой доступности и скорости работы с данными\n",
    "\n",
    "<img align=\"center\" width=\"500\" height=\"500\" src=\"https://kafka.apache.org/25/images/log_anatomy.png\">\n",
    "\n",
    "#### Partition\n",
    "Партиции - это блоки, из которых состоят топики. Партиция представляет собой неделимый блок, который хранится на одном из узлов. Топик может иметь произвольное количество партиций. Чем больше партиций - тем выше параллелзим при чтении и записи, однако слишком большое число партиций в топике может привести к замедлению работы всей системы.\n",
    "\n",
    "#### Replica\n",
    "Каждая партиция имеет (может иметь) несколько реплик. Внешние приложения всегда работают (читают и пишут) с основной репликой. Остальные реплики являются дочерними и не используются во внешнем IO. Если узел, на котором расположена основная реплика, падает, то одна из дочерних реплик становится основной и работа с данными продолжается\n",
    "\n",
    "#### Message\n",
    "Сообщения - это данные, которые мы пишем и читаем в Kafka. Они представлены кортежем (Key, Value), но ключ может быть иметь значение `null` (используется не всегда). Сереализация и десереализация данных всегда происходит на уровне клиентов Kafka. Сама Kafka ничего о типах данных не знает и хранит ключи и значения в виде массива байт\n",
    "\n",
    "#### Offset\n",
    "Оффсет - это порядковый номер сообщения в партиции. Когда мы пишем сообщение (сообщение всегда пишется в одну из партиций топика), Kafka помещает его в топик с номер `n+1`, где `n` - номер последнего сообщения в этом топике\n",
    "\n",
    "<img align=\"center\" width=\"400\" height=\"400\" src=\"https://kafka.apache.org/25/images/log_consumer.png\">\n",
    "\n",
    "#### Producer\n",
    "Producer - это приложение, которое пишет в топик. Producer'ов может быть много. Параллельная запись достигается за счет того, что каждое новое сообщение попадает в случайную партицию топика (если не указан `key`)\n",
    "\n",
    "#### Consumer\n",
    "Consumer - это приложение, читающее данные из топика. Consumer'ов может быть много, в этом случае они называются `consumer group`. Параллельное чтение достигается за счет распределения партиций топика между consumer'ами в рамках одной группы. Каждый consumer читает данные из \"своих\" партиций и ничего про другие не знает. Если consumer падает, то \"его\" партиции переходят другим consumer'ам.\n",
    "\n",
    "#### Commit\n",
    "Коммитом в Kafka называют сохранение информации о факте обработки сообщения с определенным оффсетом. Поскольку оффсеты для каждой партиции топика свои, то и информация о последнем обработанном оффсете хранится по каждой партиции отдельно. Обычные приложения пишут коммиты в специальный топик Kafka, который имеет название `__consumer_offsets`. Spark хранит обработанные оффсеты по каждому батчу в ФС (например, в HDFS).\n",
    "\n",
    "#### Retention\n",
    "Поскольку кластер Kafka не может хранить данные вечно, то в ее конфигурации задаются пороговые значение по **объему** и **времени хранения** для каждого топика, при превышении которых данные удаляются. Например, если у топика A установлен renention по времени 1 месяц, то данные будут хранится в системе не менее одного месяца (и затем будут удалены одной из внутренних подсистем)\n",
    "\n",
    "### Spark connector\n",
    "https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10  \n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html  \n",
    "\n",
    "### Запуск Kafka в docker\n",
    "```shell\n",
    "docker run --rm \\\n",
    "   -p 2181:2181 \\\n",
    "   --name=test_zoo \\\n",
    "   -e ZOOKEEPER_CLIENT_PORT=2181 \\\n",
    "   confluentinc/cp-zookeeper\n",
    "```\n",
    "\n",
    "```shell\n",
    "docker run --rm \\\n",
    "    -p 9092:9092 \\\n",
    "    --name=test_kafka \\\n",
    "    -e KAFKA_ZOOKEEPER_CONNECT=host.docker.internal:2181 \\\n",
    "    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://host.docker.internal:9092 \\\n",
    "    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\n",
    "    confluentinc/cp-kafka\n",
    "```\n",
    "\n",
    "### Работа с Kafka с помощь Static Dataframe\n",
    "\n",
    "Spark позволяет работать с кафкой как с обычной базой данных. Запишем данные в топик `test_topic0`. Для этого нам необходимо подготовить DF, в котором будет две колонки:\n",
    "- `value: String` - данные, которые мы хотим записать\n",
    "- `topic: String` - топик, куда писать каждую строку DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1678\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      "\n",
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-07-04 19:59:39.598|38   |02OI |\n",
      "|2023-07-04 19:59:41.598|40   |01PA |\n",
      "|2023-07-04 19:59:43.598|42   |02KY |\n",
      "|2023-07-04 19:59:45.598|44   |01FA |\n",
      "|2023-07-04 19:59:47.598|46   |01PN |\n",
      "+-----------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "identPq = [timestamp: timestamp, value: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[timestamp: timestamp, value: bigint ... 1 more field]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val identPq = spark.read.parquet(\"/tmp/spark-intro-stream/s2.parquet\")\n",
    "println(identPq.count)\n",
    "identPq.printSchema\n",
    "identPq.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "def writeKafka[T](topic: String, data: Dataset[T]): Unit = {\n",
    "    val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\"\n",
    "    )\n",
    "    \n",
    "    val output = data.select(to_json(struct(col(\"*\"))).alias(\"value\"))\n",
    "    output.printSchema\n",
    "    output.show(2, false)\n",
    "    output.withColumn(\"key\", lit(\"hello world\"))\n",
    "            .withColumn(\"topic\", lit(topic)).write.format(\"kafka\").options(kafkaParams).save\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeKafka(\"test_topic3\", identPq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем данные из Kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+------------------------------+------------------------------+-----------+---------+------+-----------------------+-------------+\n",
      "|                           key|                         value|      topic|partition|offset|              timestamp|timestampType|\n",
      "+------------------------------+------------------------------+-----------+---------+------+-----------------------+-------------+\n",
      "|[68 65 6C 6C 6F 20 77 6F 72...|[7B 22 74 69 6D 65 73 74 61...|test_topic3|        0|     0|2023-07-04 21:07:37.149|            0|\n",
      "|[68 65 6C 6C 6F 20 77 6F 72...|[7B 22 74 69 6D 65 73 74 61...|test_topic3|        0|     1|2023-07-04 21:07:37.161|            0|\n",
      "|[68 65 6C 6C 6F 20 77 6F 72...|[7B 22 74 69 6D 65 73 74 61...|test_topic3|        0|     2|2023-07-04 21:07:37.161|            0|\n",
      "+------------------------------+------------------------------+-----------+---------+------+-----------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-----------------------------------------------------------------------+\n",
      "|key        |value                                                                  |\n",
      "+-----------+-----------------------------------------------------------------------+\n",
      "|hello world|{\"timestamp\":\"2023-07-04T19:59:39.598+03:00\",\"value\":38,\"ident\":\"02OI\"}|\n",
      "|hello world|{\"timestamp\":\"2023-07-04T19:59:41.598+03:00\",\"value\":40,\"ident\":\"01PA\"}|\n",
      "|hello world|{\"timestamp\":\"2023-07-04T19:59:43.598+03:00\",\"value\":42,\"ident\":\"02KY\"}|\n",
      "+-----------+-----------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667, subscribe -> test_topic3)\n",
       "df = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\",\n",
    "        \"subscribe\" -> \"test_topic3\"\n",
    "    )\n",
    "\n",
    "\n",
    "val df = spark.read.format(\"kafka\").options(kafkaParams).load\n",
    "\n",
    "df.printSchema\n",
    "df.show(3, 30)\n",
    "df.select(col(\"key\").cast(\"string\"), col(\"value\").cast(\"string\")).show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KafkaSourceRDDPartition(0,KafkaSourceRDDOffsetRange(test_topic3-0,-2,-1,None))\n"
     ]
    }
   ],
   "source": [
    "df.rdd.partitions.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+\n",
      "|value                                                                  |\n",
      "+-----------------------------------------------------------------------+\n",
      "|{\"timestamp\":\"2023-07-04T19:59:39.598+03:00\",\"value\":38,\"ident\":\"02OI\"}|\n",
      "|{\"timestamp\":\"2023-07-04T19:59:41.598+03:00\",\"value\":40,\"ident\":\"01PA\"}|\n",
      "|{\"timestamp\":\"2023-07-04T19:59:43.598+03:00\",\"value\":42,\"ident\":\"02KY\"}|\n",
      "+-----------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"value\").cast(\"string\")).show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|value                              |\n",
      "+-----------------------------------+\n",
      "|[2023-07-04 19:59:39.598, 38, 02OI]|\n",
      "|[2023-07-04 19:59:41.598, 40, 01PA]|\n",
      "|[2023-07-04 19:59:43.598, 42, 02KY]|\n",
      "+-----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    ".select(from_json(col(\"value\").cast(\"string\"), identPq.schema).alias(\"value\"))\n",
    ".show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+-----+\n",
      "|timestamp              |value|ident|\n",
      "+-----------------------+-----+-----+\n",
      "|2023-07-04 19:59:39.598|38   |02OI |\n",
      "|2023-07-04 19:59:41.598|40   |01PA |\n",
      "|2023-07-04 19:59:43.598|42   |02KY |\n",
      "+-----------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    ".select(from_json(col(\"value\").cast(\"string\"), identPq.schema).alias(\"value\"))\n",
    ".select(col(\"value.*\")).show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение из Kafka имеет несколько особенностей:\n",
    "- по умолчанию читается все содержимое топика. Поскольку обычно в нем много данных, эта операция может создать большую нагрузку на кластер Kafka и Spark приложение\n",
    "- колонки `value` и `key` имеют тип `binary`, который необходимо десереализовать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы прочитать только определенную часть топика, нам необходимо задать минимальный и максимальный оффсет для чтения с помощью параметров `startingOffsets` , `endingOffsets`. Возьмем два случайных события:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------+\n",
      "|      topic|partition|offset|\n",
      "+-----------+---------+------+\n",
      "|test_topic3|        0|     0|\n",
      "|test_topic3|        0|    16|\n",
      "+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(0.1).limit(2).select('topic, 'partition, 'offset).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основании этих событий подготовим параметры `startingOffsets` и `endingOffsets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|                 key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+--------------------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|     2|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|     3|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|     4|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|     5|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|     6|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|     7|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|     8|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|     9|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    10|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    11|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    12|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    13|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    14|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    15|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    16|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    17|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    18|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    19|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    20|2023-07-04 21:07:...|            0|\n",
      "|[68 65 6C 6C 6F 2...|[7B 22 74 69 6D 6...|test_topic3|        0|    21|2023-07-04 21:07:...|            0|\n",
      "+--------------------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kafkaParams = Map(kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667, subscribe -> test_topic3, startingOffsets -> {\"test_topic3\":{\"0\": 2}}, endingOffsets -> {\"test_topic3\":{\"0\": 500}})\n",
       "df = [key: binary, value: binary ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\",\n",
    "        \"subscribe\" -> \"test_topic3\",\n",
    "        \"startingOffsets\" -> \"\"\"{\"test_topic3\":{\"0\": 2}}\"\"\",\n",
    "        \"endingOffsets\" -> \"\"\"{\"test_topic3\":{\"0\": 500}}\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "val df = spark.read.format(\"kafka\").options(kafkaParams).load\n",
    "\n",
    "df.printSchema\n",
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// latest/earliest - offset options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию параметр `startingOffsets` имеет значение `earliest`, а `endingOffsets` - `latest`. Поэтому, когда мы не указывали эти параметры, Spark прочитал содержимое всего топика\n",
    "\n",
    "Чтобы получить наши данные, которые мы записали в топик, нам необходимо их десереализовать. В нашем случае достаточно использовать `.cast(\"string\")`, однако это работает не всегда, т.к. формат данных может быть произвольным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+\n",
      "|value                                                                  |\n",
      "+-----------------------------------------------------------------------+\n",
      "|{\"timestamp\":\"2023-07-04T19:59:43.598+03:00\",\"value\":42,\"ident\":\"02KY\"}|\n",
      "|{\"timestamp\":\"2023-07-04T19:59:45.598+03:00\",\"value\":44,\"ident\":\"01FA\"}|\n",
      "|{\"timestamp\":\"2023-07-04T19:59:47.598+03:00\",\"value\":46,\"ident\":\"01PN\"}|\n",
      "|{\"timestamp\":\"2023-07-04T19:59:49.598+03:00\",\"value\":48,\"ident\":\"02PS\"}|\n",
      "|{\"timestamp\":\"2023-07-04T19:59:51.598+03:00\",\"value\":50,\"ident\":\"02IN\"}|\n",
      "|{\"timestamp\":\"2023-07-04T19:59:53.598+03:00\",\"value\":52,\"ident\":\"02FL\"}|\n",
      "|{\"timestamp\":\"2023-07-04T19:59:55.598+03:00\",\"value\":54,\"ident\":\"02PS\"}|\n",
      "|{\"timestamp\":\"2023-07-04T19:59:57.598+03:00\",\"value\":56,\"ident\":\"03CO\"}|\n",
      "|{\"timestamp\":\"2023-07-04T19:59:59.598+03:00\",\"value\":58,\"ident\":\"02MI\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:01.598+03:00\",\"value\":60,\"ident\":\"02MD\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:03.598+03:00\",\"value\":62,\"ident\":\"03AR\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:05.598+03:00\",\"value\":64,\"ident\":\"00SD\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:07.598+03:00\",\"value\":66,\"ident\":\"02WA\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:09.598+03:00\",\"value\":68,\"ident\":\"01TA\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:11.598+03:00\",\"value\":70,\"ident\":\"02OR\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:13.598+03:00\",\"value\":72,\"ident\":\"00HI\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:15.598+03:00\",\"value\":74,\"ident\":\"01IA\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:17.598+03:00\",\"value\":76,\"ident\":\"00PA\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:19.598+03:00\",\"value\":78,\"ident\":\"02WA\"}|\n",
      "|{\"timestamp\":\"2023-07-04T20:00:21.598+03:00\",\"value\":80,\"ident\":\"02MS\"}|\n",
      "+-----------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n",
      "+-----+-----------------------------+-----+\n",
      "|ident|timestamp                    |value|\n",
      "+-----+-----------------------------+-----+\n",
      "|02KY |2023-07-04T19:59:43.598+03:00|42   |\n",
      "|01FA |2023-07-04T19:59:45.598+03:00|44   |\n",
      "|01PN |2023-07-04T19:59:47.598+03:00|46   |\n",
      "|02PS |2023-07-04T19:59:49.598+03:00|48   |\n",
      "|02IN |2023-07-04T19:59:51.598+03:00|50   |\n",
      "|02FL |2023-07-04T19:59:53.598+03:00|52   |\n",
      "|02PS |2023-07-04T19:59:55.598+03:00|54   |\n",
      "|03CO |2023-07-04T19:59:57.598+03:00|56   |\n",
      "|02MI |2023-07-04T19:59:59.598+03:00|58   |\n",
      "|02MD |2023-07-04T20:00:01.598+03:00|60   |\n",
      "|03AR |2023-07-04T20:00:03.598+03:00|62   |\n",
      "|00SD |2023-07-04T20:00:05.598+03:00|64   |\n",
      "|02WA |2023-07-04T20:00:07.598+03:00|66   |\n",
      "|01TA |2023-07-04T20:00:09.598+03:00|68   |\n",
      "|02OR |2023-07-04T20:00:11.598+03:00|70   |\n",
      "|00HI |2023-07-04T20:00:13.598+03:00|72   |\n",
      "|01IA |2023-07-04T20:00:15.598+03:00|74   |\n",
      "|00PA |2023-07-04T20:00:17.598+03:00|76   |\n",
      "|02WA |2023-07-04T20:00:19.598+03:00|78   |\n",
      "|02MS |2023-07-04T20:00:21.598+03:00|80   |\n",
      "+-----+-----------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jsonString = [value: string]\n",
       "parsed = [ident: string, timestamp: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ident: string, timestamp: string ... 1 more field]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonString = df.select('value.cast(\"string\")).as[String]\n",
    "\n",
    "jsonString.show(20, false)\n",
    "\n",
    "val parsed = spark.read.json(jsonString)\n",
    "parsed.printSchema\n",
    "parsed.show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с Kafka с помощью Streaming DF\n",
    "При создании SDF из Kafka необходимо помнить, что:\n",
    "- `startingOffsets` по умолчанию имеет значение `latest`\n",
    "- `endingOffsets` использовать нельзя\n",
    "- количество сообщений за батч можно (и нужно) ограничить параметром `maxOffsetPerTrigger` (по умолчанию он не задан и первый батч будет содержать данные всего топика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\n",
       "Aggregate [ident#1522], [ident#1522, avg(cast(value#1523 as double)) AS avg(value)#1529]\n",
       "+- Project [get_json_object(value#1517, $.ident) AS ident#1522, get_json_object(value#1517, $.value) AS value#1523]\n",
       "   +- Project [cast(value#1504 as string) AS value#1517, topic#1505, partition#1506, offset#1507L]\n",
       "      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2cb51d17, kafka, Map(maxOffsetsPerTrigger -> 5, startingOffsets -> earliest, subscribe -> test_topic3, kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667), [key#1503, value#1504, topic#1505, partition#1506, offset#1507L, timestamp#1508, timestampType#1509], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@42b753fa,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -> 5, startingOffsets -> earliest, subscribe -> test_topic3, kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667),None), kafka, [key#1496, value#1497, topic#1498, partition#1499, offset#1500L, timestamp#1501, timestampType#1502]\n",
       "\n",
       "StackTrace: Aggregate [ident#1522], [ident#1522, avg(cast(value#1523 as double)) AS avg(value)#1529]\n",
       "+- Project [get_json_object(value#1517, $.ident) AS ident#1522, get_json_object(value#1517, $.value) AS value#1523]\n",
       "   +- Project [cast(value#1504 as string) AS value#1517, topic#1505, partition#1506, offset#1507L]\n",
       "      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@2cb51d17, kafka, Map(maxOffsetsPerTrigger -> 5, startingOffsets -> earliest, subscribe -> test_topic3, kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667), [key#1503, value#1504, topic#1505, partition#1506, offset#1507L, timestamp#1508, timestampType#1509], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@42b753fa,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -> 5, startingOffsets -> earliest, subscribe -> test_topic3, kafka.bootstrap.servers -> spark-master-1.newprolab.com:6667),None), kafka, [key#1496, value#1497, topic#1498, partition#1499, offset#1500L, timestamp#1501, timestampType#1502]\n",
       "  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:389)\n",
       "  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:111)\n",
       "  at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:256)\n",
       "  at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)\n",
       "  at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kafkaParams = Map(\n",
    "        \"kafka.bootstrap.servers\" -> \"spark-master-1.newprolab.com:6667\",\n",
    "        \"subscribe\" -> \"test_topic3\",\n",
    "        \"startingOffsets\" -> \"\"\"earliest\"\"\",\n",
    "        \"maxOffsetsPerTrigger\" -> \"50\",\n",
    "        \"failOnDataLoss\" -> \"false\"\n",
    "    )\n",
    "\n",
    "val sdf = spark.readStream.format(\"kafka\").options(kafkaParams).load\n",
    "val parsedSdf = sdf.select('value.cast(\"string\"), 'topic, 'partition, 'offset)\n",
    "\n",
    "val outputDf = parsedSdf.select(\n",
    "                            get_json_object(col(\"value\"), \"$.ident\").alias(\"ident\"),\n",
    "                            get_json_object(col(\"value\"), \"$.value\").alias(\"value\")\n",
    "                        )\n",
    "                        .groupBy(col(\"ident\")).agg(avg(col(\"value\")))\n",
    "\n",
    "outputDf\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"numRows\", \"30\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    "\n",
    "//val sink = createConsoleSink(parsedSdf)\n",
    "\n",
    "//val sq = sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы перезапустим этот стрим, он повторно прочитает все данные. Чтобы обеспечить сохранение состояния стрима после обработки каждого батча, нам необходимо добавить параметр `checkpointLocation` в опции `writeStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createConsoleSinkWithCheckpoint: (chkName: String, df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def createConsoleSinkWithCheckpoint(chkName: String, df: DataFrame) = {\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .trigger(Trigger.ProcessingTime(\"3 seconds\"))\n",
    "    .option(\"checkpointLocation\", s\"/tmp/chk_01/$chkName\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", \"20\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sink = org.apache.spark.sql.streaming.DataStreamWriter@46206936\n",
       "sq = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@77e5e6a7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@77e5e6a7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2023-07-04T20:01:09.598+03:00\",\"value\":128,\"ident\":\"01MI\"}|test_topic3|0        |45    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:11.598+03:00\",\"value\":130,\"ident\":\"00UT\"}|test_topic3|0        |46    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:13.598+03:00\",\"value\":132,\"ident\":\"02CO\"}|test_topic3|0        |47    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:15.598+03:00\",\"value\":134,\"ident\":\"01II\"}|test_topic3|0        |48    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:17.598+03:00\",\"value\":136,\"ident\":\"00XS\"}|test_topic3|0        |49    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2023-07-04T20:01:19.598+03:00\",\"value\":138,\"ident\":\"01PA\"}|test_topic3|0        |50    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:21.598+03:00\",\"value\":140,\"ident\":\"01WA\"}|test_topic3|0        |51    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:23.598+03:00\",\"value\":142,\"ident\":\"00VI\"}|test_topic3|0        |52    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:25.598+03:00\",\"value\":144,\"ident\":\"00FL\"}|test_topic3|0        |53    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:27.598+03:00\",\"value\":146,\"ident\":\"02P\"} |test_topic3|0        |54    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2023-07-04T20:01:29.598+03:00\",\"value\":148,\"ident\":\"00IL\"}|test_topic3|0        |55    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:31.598+03:00\",\"value\":150,\"ident\":\"00KY\"}|test_topic3|0        |56    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:33.598+03:00\",\"value\":152,\"ident\":\"01LS\"}|test_topic3|0        |57    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:35.598+03:00\",\"value\":154,\"ident\":\"02KY\"}|test_topic3|0        |58    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:37.598+03:00\",\"value\":156,\"ident\":\"00AS\"}|test_topic3|0        |59    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2023-07-04T20:01:39.598+03:00\",\"value\":158,\"ident\":\"02WI\"}|test_topic3|0        |60    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:41.598+03:00\",\"value\":160,\"ident\":\"02MN\"}|test_topic3|0        |61    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:43.598+03:00\",\"value\":162,\"ident\":\"00TA\"}|test_topic3|0        |62    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:45.598+03:00\",\"value\":164,\"ident\":\"00WI\"}|test_topic3|0        |63    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:47.598+03:00\",\"value\":166,\"ident\":\"03AK\"}|test_topic3|0        |64    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2023-07-04T20:01:49.598+03:00\",\"value\":168,\"ident\":\"01NM\"}|test_topic3|0        |65    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:51.598+03:00\",\"value\":170,\"ident\":\"01UT\"}|test_topic3|0        |66    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:53.598+03:00\",\"value\":172,\"ident\":\"02WA\"}|test_topic3|0        |67    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:55.598+03:00\",\"value\":174,\"ident\":\"00TN\"}|test_topic3|0        |68    |\n",
      "|{\"timestamp\":\"2023-07-04T20:01:57.598+03:00\",\"value\":176,\"ident\":\"00IL\"}|test_topic3|0        |69    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|value                                                                   |topic      |partition|offset|\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "|{\"timestamp\":\"2023-07-04T20:01:59.598+03:00\",\"value\":178,\"ident\":\"02WA\"}|test_topic3|0        |70    |\n",
      "|{\"timestamp\":\"2023-07-04T20:02:01.598+03:00\",\"value\":180,\"ident\":\"00FL\"}|test_topic3|0        |71    |\n",
      "|{\"timestamp\":\"2023-07-04T20:02:03.598+03:00\",\"value\":182,\"ident\":\"00CL\"}|test_topic3|0        |72    |\n",
      "|{\"timestamp\":\"2023-07-04T20:02:05.598+03:00\",\"value\":184,\"ident\":\"00MO\"}|test_topic3|0        |73    |\n",
      "|{\"timestamp\":\"2023-07-04T20:02:07.598+03:00\",\"value\":186,\"ident\":\"00KY\"}|test_topic3|0        |74    |\n",
      "+------------------------------------------------------------------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val sink = createConsoleSinkWithCheckpoint(\"test1\", parsedSdf)\n",
    "val sq = sink.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped KafkaV2[Subscribe[test_topic3]]\n"
     ]
    }
   ],
   "source": [
    "killAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Found 4 items\n",
       "drwxr-xr-x   - teacher hdfs          0 2023-07-04 21:39 /tmp/chk_01/test1/commits\n",
       "-rw-r--r--   3 teacher hdfs         45 2023-07-04 21:38 /tmp/chk_01/test1/metadata\n",
       "drwxr-xr-x   - teacher hdfs          0 2023-07-04 21:39 /tmp/chk_01/test1/offsets\n",
       "drwxr-xr-x   - teacher hdfs          0 2023-07-04 21:38 /tmp/chk_01/test1/sources\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"hadoop fs -ls /tmp/chk_01/test1\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 teacher hdfs        430 2023-07-04 21:39 /tmp/chk_01/test1/offsets/0\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/1\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/10\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/11\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/12\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/13\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/14\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/2\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/3\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/4\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/5\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/6\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/7\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/8\n",
      "-rw-r--r--   3 teacher hdfs        431 2023-07-04 21:39 /tmp/chk_01/test1/offsets/9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hadoop fs -ls /tmp/chk_01/test1/offsets/*\".!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"v1\n",
       "{\"batchWatermarkMs\":0,\"batchTimestampMs\":1688495979001,\"conf\":{\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\",\"spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion\":\"2\",\"spark.sql.streaming.multipleWatermarkPolicy\":\"min\",\"spark.sql.streaming.aggregation.stateFormatVersion\":\"2\",\"spark.sql.shuffle.partitions\":\"200\"}}\n",
       "{\"test_topic3\":{\"0\":65}}\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"hadoop fs -cat /tmp/chk_01/test1/offsets/12\".!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- Apache Kafka - распределенная система, обеспечивающая передачу потока данных в слабосвязанных системах\n",
    "- Работать с Kafka можно как с использованием Static DF, так и с помощью Streaming DF\n",
    "- Чтобы стрим запоминал свое состояние после остановки, необходимо использовать checkpoint - директорию на HDFS (или локальной ФС), в которую будет сохранятся состояние стрима после каждого батча"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце работы не забудьте остановить Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(sq.lastProgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.awaitTermination(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
